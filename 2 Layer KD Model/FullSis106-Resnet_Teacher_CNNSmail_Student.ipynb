{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7962c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import time\n",
    "from thop import profile\n",
    "\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import pdb\n",
    "import math\n",
    "import h5py as h5\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8715560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3233, 2393, 3)\n",
      "(3233, 39, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remember to install CUDA and cuDNN\n",
    "# Determine if your system supports CUDA.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  \n",
    "#%% Read FallAllD data\n",
    "data=pd.read_hdf(\"D:/conda/FallAllD.h5\") #\n",
    "data = data.drop(columns = ['Gyr','Mag','Bar'])\n",
    "\n",
    "#%% Read SisFull\n",
    "data_Sis_pash=\"D:/conda/111_1117/SisFall_DL-master/SisFall_dataset/\"\n",
    "df=pd.read_csv(data_Sis_pash+\"SisFall_DataNameList.csv\",header=0)\n",
    "df_r=df[1:]\n",
    "\n",
    "df_v=df['DataName'].values[:]\n",
    "\n",
    "#%% Label\n",
    "testmin=[]\n",
    "test_waist = []\n",
    "data_waist = []\n",
    "label_waist = []\n",
    "subject_waist = []\n",
    "# for i in range(len(data)):\n",
    "for i in range (len(df_r)):\n",
    "\n",
    "#     if(data['Device'][i] == 'Waist'):\n",
    "#         a = data['Acc'][i]*0.000244        \n",
    "        C_df = pd.read_csv(data_Sis_pash+df['DataName'][i])\n",
    "        testmin.append(len(C_df))\n",
    "        C_df = C_df[:].values[:2393].astype('float64')\n",
    "        a= C_df\n",
    "        # Normalize each data between 0 and 1\n",
    "        b = b = (a-np.amin(a))/(np.amax(a)-np.amin(a))\n",
    "        data_waist.append(b)        \n",
    "        subject_waist.append(df['Subject'][i])\n",
    "        # Class: fall = 0 ,ADL = 1 \n",
    "        if (df['FALL(1)_ADL(0)'][i]): \n",
    "                label_waist.append(1)\n",
    "        else:\n",
    "                label_waist.append(0)\n",
    "\n",
    "#extend\n",
    "\n",
    "# Change list to array\n",
    "data_waist = np.array(data_waist, dtype=object)            \n",
    "label_waist = np.array(label_waist, dtype=object)\n",
    "subject_waist = np.array(subject_waist, dtype=object)\n",
    "print(data_waist.shape)\n",
    "# Downsampling 1/128\n",
    "down_data = []\n",
    "# (start:size:step)\n",
    "# def down_data_SisADD(data_waist):\n",
    "#down_data = data_waist[::1,::128,::1]\n",
    "# data_waist = data_waist.reshape((len(data_waist), 1, 1))\n",
    "down_data = data_waist[::1, ::62, ::1]\n",
    "down_data = np.array(down_data, dtype=object)\n",
    "print(down_data.shape)\n",
    "#%% Sliding_window\n",
    "sliding_waist_data =[]\n",
    "sliding_waist_label =[]\n",
    "sliding_waist_subject =[]  \n",
    "# The signal of each second \n",
    "per_sec = 38/20\n",
    "sec = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba63d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12932, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "# for num in range(len(down_data)):\n",
    "\n",
    "#     if label_waist[num] ==1:\n",
    "#         # Data[index][start:end,axis]\n",
    "#         sliding_waist_data = np.append(sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*0.5)):int(np.uint(sec*per_sec*1.5))+1,:])\n",
    "#         # Label each window with class and subject\n",
    "#         sliding_waist_label = np.append(sliding_waist_label, 1)\n",
    "#         sliding_waist_subject = np.append(sliding_waist_subject, subject_waist[num])\n",
    "        \n",
    "#         sliding_waist_data = np.append(sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*0.75)):int(np.uint(sec*per_sec*1.75-1))+1,:])\n",
    "#         sliding_waist_label = np.append(sliding_waist_label, 1)\n",
    "#         sliding_waist_subject = np.append(sliding_waist_subject, subject_waist[num])\n",
    "        \n",
    "#         sliding_waist_data = np.append(sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*1)):int(np.uint(sec*per_sec*2))+1,:])\n",
    "#         sliding_waist_label = np.append(sliding_waist_label, 1)\n",
    "#         sliding_waist_subject = np.append(sliding_waist_subject, subject_waist[num])\n",
    "        \n",
    "#         sliding_waist_data = np.append(sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*1.25)):int(np.uint(sec*per_sec*2.25))+1,:])\n",
    "#         sliding_waist_label = np.append(sliding_waist_label, 1)\n",
    "#         sliding_waist_subject = np.append(sliding_waist_subject, subject_waist[num])\n",
    "        \n",
    "#     else:\n",
    "#         #print(down_data[num])\n",
    "#         sliding_waist_data = np.concatenate([sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*0.5)):int(np.uint(sec*per_sec*1.5))+1,:]])\n",
    "#         #print(sliding_waist_data)\n",
    "#         sliding_waist_label = np.concatenate([sliding_waist_label, 0])\n",
    "#         sliding_waist_subject = np.concatenate([sliding_waist_subject, subject_waist[num]])\n",
    "        \n",
    "#         sliding_waist_data = np.concatenate([sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*0.75)):int(np.uint(sec*per_sec*1.75-1))+1,:]])\n",
    "#         sliding_waist_label = np.concatenate([sliding_waist_label, 0])\n",
    "#         sliding_waist_subject = np.concatenate([sliding_waist_subject, subject_waist[num]])\n",
    "        \n",
    "#         sliding_waist_data = np.concatenate([sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*1)):int(np.uint(sec*per_sec*2))+1,:]])\n",
    "#         sliding_waist_label = np.concatenate([sliding_waist_label, 0])\n",
    "#         sliding_waist_subject = np.concatenate([sliding_waist_subject, subject_waist[num]])\n",
    "        \n",
    "#         sliding_waist_data = np.concatenate([sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*1.25)):int(np.uint(sec*per_sec*2.25))+1,:]])\n",
    "#         sliding_waist_label = np.concatenate([sliding_waist_label, 0])\n",
    "#         sliding_waist_subject = np.concatenate([sliding_waist_subject, subject_waist[num]])\n",
    "\n",
    "\n",
    "for num in range(len(down_data)):\n",
    "    if label_waist[num] ==1:\n",
    "        # Data[index][start:end,axis]\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.5)):int(np.uint(sec*per_sec*1.5)+1),:])\n",
    "        # Label each window with class and subject\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.75)):int(np.uint(sec*per_sec*1.75-1))+1,:])\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1)):int(np.uint(sec*per_sec*2))+1,:])\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1.25)):int(np.uint(sec*per_sec*2.25))+1,:])\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        \n",
    "    else:\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.5)):int(np.uint(sec*per_sec*1.5))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.75)):int(np.uint(sec*per_sec*1.75-1))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1)):int(np.uint(sec*per_sec*2))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1.25)):int(np.uint(sec*per_sec*2.25))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "\n",
    "sliding_waist_data = np.array(sliding_waist_data)\n",
    "sliding_waist_label = np.array(sliding_waist_label)\n",
    "sliding_waist_subject = np.array(sliding_waist_subject) \n",
    "print(sliding_waist_data.shape)\n",
    "np.save('FallALLD_SW_data',sliding_waist_data)\n",
    "np.save('FallALLD_SW_label',sliding_waist_label)\n",
    "np.save('FallALLD_SW_subject',sliding_waist_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2388cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Show Teacher confusion matrix\n",
    "def show_CM_teacher(validation,prediction):     \n",
    "    matrix = metrics.confusion_matrix(validation,prediction)\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(matrix,cmap = 'coolwarm',linecolor= 'white',\n",
    "                linewidths= 1,annot= True,fmt = 'd')\n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.show()\n",
    "    plt.savefig('Prediction (Teacher) SubjectID' + str(sub) + '.png')\n",
    "\n",
    "#%% Show Student confusion matrix\n",
    "def show_CM_student(validation,prediction):     \n",
    "    matrix = metrics.confusion_matrix(validation,prediction)\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(matrix,cmap = 'coolwarm',linecolor= 'white',\n",
    "                linewidths= 1,annot= True,fmt = 'd')\n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.show()\n",
    "    plt.savefig('Prediction (Student) SubjectID' + str(sub) + '.png')\n",
    "    \n",
    "#%% Show Teacher-Student confusion matrix\n",
    "def show_CM_teacher_student(validation,prediction):     \n",
    "    matrix = metrics.confusion_matrix(validation,prediction)\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(matrix,cmap = 'coolwarm',linecolor= 'white',\n",
    "                linewidths= 1,annot= True,fmt = 'd')\n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.show()\n",
    "    plt.savefig('Prediction (Teacher-Student) SubjectID' + str(sub) + '.png')\n",
    "def show_loss_teacher(loss_train,loss_test):     \n",
    "    plt.plot(loss_train, label = 'trian_loss')\n",
    "    plt.plot(loss_test, label = 'test_loss')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend([\"Train Loss\",\"Test Loss\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Loss (teacher) SubjectID' + str(sub) + '.png')\n",
    "def show_Acc_teacher(train_acc, test_acc):     \n",
    "    plt.plot(train_acc, label = 'trian_Acc')\n",
    "    plt.plot(test_acc, label = 'test_Acc')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend([\"Train Acc\",\"Test Acc\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Acc (teacher) SubjectID' + str(sub) + '.png')\n",
    "#Loss_student\n",
    "def show_loss_student(loss_train,loss_test):     \n",
    "    plt.plot(loss_train, label = 'trian_loss')\n",
    "    plt.plot(loss_test, label = 'test_loss')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend([\"Train Loss\",\"Test Loss\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Loss (Student) SubjectID' + str(sub) + '.png')   \n",
    "def show_Acc_student(train_acc, test_acc):     \n",
    "    plt.plot(train_acc, label = 'trian_Acc')\n",
    "    plt.plot(test_acc, label = 'test_Acc')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend([\"Train Acc\",\"Test Acc\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Acc (student) SubjectID' + str(sub) + '.png')\n",
    "def show_loss_teacher_student(loss_train,loss_test):     \n",
    "    plt.plot(loss_train, label = 'trian_loss')\n",
    "    plt.plot(loss_test, label = 'test_loss')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend([\"Train Loss\",\"Test Loss\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Loss (teacher_Student) SubjectID' + str(sub) + '.png')   \n",
    "def show_Acc_teacher_student(train_acc, test_acc):     \n",
    "    plt.plot(train_acc, label = 'trian_Acc')\n",
    "    plt.plot(test_acc, label = 'test_Acc')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend([\"Train Acc\",\"Test Acc\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Acc (teacher_student) SubjectID' + str(sub) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f7e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):\n",
    "    # 一般的Cross Entropy\n",
    "    labels=labels.type(torch.cuda.LongTensor)\n",
    "    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    # 讓logits的log_softmax對目標機率(teacher的logits/T後softmax)做KL Divergence。\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs/T, dim=1),\n",
    "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)\n",
    "    return hard_loss + soft_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1446d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% This is the config object which contains all relevant settings.    \n",
    "config = {\n",
    "    'nb_filters': 64,\n",
    "    'filter_width': 1,\n",
    "    'drop_prob': 0.5,\n",
    "    'epochs': 200,#200\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-3,#1e-3\n",
    "    'weight_decay': 2e-6,\n",
    "    'gpu_name': 'cuda:0',\n",
    "    'print_counts': False,\n",
    "    'lr_factor' : 2,\n",
    "    'lr_warmup' : 40\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35895351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define teacher neural network\n",
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "      super(block, self).__init__()\n",
    "      self.expansion = 4\n",
    "      self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "      self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "      self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1,bias=False)\n",
    "      self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "      self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "      self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "      self.relu = nn.ReLU()\n",
    "      self.identity_downsample = identity_downsample\n",
    "      self.stride = stride\n",
    "\n",
    "  def forward(self, x):\n",
    "      identity = x.clone()\n",
    "      x = self.conv1(x)\n",
    "      x = self.bn1(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.bn2(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.conv3(x)\n",
    "      x = self.bn3(x)\n",
    "\n",
    "      if self.identity_downsample is not None:\n",
    "          identity = self.identity_downsample(identity)\n",
    "\n",
    "      x += identity\n",
    "      x = self.relu(x)\n",
    "      return x\n",
    "\n",
    "class Net_Teacher(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes): \n",
    "        # Return a proxy object that delegates method calls to a parent or sibling class of type, which is nn.Module in this case\n",
    "        super(Net_Teacher, self).__init__()\n",
    "        self.in_channels = 64        \n",
    "        # This way can print out every layers output in forward propagation, which is more convenient to adjust. nn.Sequential(nn.Conv2d(1, self.nb_filters, 3),nn.MaxPool2d([2,1]),...) is more visibility \n",
    "        # Output shape after convolution layer = (input shape – filter shape + 1) + (2*padding/stride)\n",
    "        # nn.Conv2d(input channel, output channel, kernel size(x,x),stride,padding)\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=3, stride=2, padding=3, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
    "        self.layer1 = self._make_layer(block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self._make_layer(block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], intermediate_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x            \n",
    "        # The loss function, nn.CrossEntropyLoss(), combines LogSoftmax and NLLLoss, so we should use softmax after loss calculation\n",
    "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
    "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
    "        # to the layer that's ahead\n",
    "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    intermediate_channels * 4,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(intermediate_channels * 4),\n",
    "            )\n",
    "\n",
    "        layers.append(\n",
    "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
    "        )\n",
    "\n",
    "        # The expansion size is always 4 for ResNet 50,101,152\n",
    "        self.in_channels = intermediate_channels * 4\n",
    "\n",
    "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
    "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
    "        # and also same amount of channels.\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, intermediate_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "#%% Define student neural network\n",
    "class Net_Student(nn.Module):\n",
    "    def __init__(self,config): \n",
    "        # Return a proxy object that delegates method calls to a parent or sibling class of type, which is nn.Module in this case\n",
    "        super(Net_Student, self).__init__()\n",
    "        # Input hyperparameter \n",
    "        self.window_size = config['window_size']\n",
    "        self.drop_prob = config['drop_prob']\n",
    "        self.nb_channels = config['nb_channels']\n",
    "        self.nb_classes = config['nb_classes']\n",
    "        self.nb_filters = config['nb_filters']\n",
    "        self.filter_width = config['filter_width']\n",
    "        \n",
    "        # This way can print out every layers output in forward propagation, which is more convenient to adjust. nn.Sequential(nn.Conv2d(1, self.nb_filters, 3),nn.MaxPool2d([2,1]),...) is more visibility \n",
    "        # Output shape after convolution layer = (input shape – filter shape + 1) + (2*padding/stride)\n",
    "        # nn.Conv2d(input channel, output channel, kernel size(x,x),stride,padding)\n",
    "        self.conv1 = nn.Conv2d(1, self.nb_filters//32, 3) \n",
    "        self.pool = nn.MaxPool2d([2,1])\n",
    "        self.conv2 = nn.Conv2d(2, self.nb_filters//32,(3,1))\n",
    "        \n",
    "        print('Conv1:{}'.format(self.conv1)) \n",
    "        print('pool:{}'.format(self.pool)) \n",
    "        print('Conv2:{}'.format(self.conv2)) \n",
    "        \n",
    "        # nn.Linear(input size,output size)\n",
    "        self.fc1 = nn.Linear(2*4,2)  \n",
    "        #self.fc2 = nn.Linear(8,4)\n",
    "        #self.fc3 = nn.Linear(8, 4)\n",
    "        # In this case, we need to classify 2 classes, fall and ADL, so the last output size need to be 2\n",
    "        self.fc4 = nn.Linear(2, 2)\n",
    "         \n",
    "    # Define forward propagation    \n",
    "    def forward(self, x):  \n",
    "        # Reshape input size (batch, input channels, window size, window channels)\n",
    "        x = x.view(-1, 1, 14, 3) \n",
    "        x = F.relu(self.conv1(x)) \n",
    "        # output(64,64,12,1)\n",
    "        x = self.pool(x)\n",
    "        #(64,64,6,1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #(64,64,4,1)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = F.relu(self.fc3(x)) \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x      \n",
    "        # The loss function, nn.CrossEntropyLoss(), combines LogSoftmax and NLLLoss, so we should use softmax after loss calculation\n",
    "\n",
    "#%% Define student neural network\n",
    "class Net_Teacher_Student(nn.Module):\n",
    "    def __init__(self,config): \n",
    "        # Return a proxy object that delegates method calls to a parent or sibling class of type, which is nn.Module in this case\n",
    "        super(Net_Teacher_Student, self).__init__()\n",
    "        # Input hyperparameter \n",
    "        self.window_size = config['window_size']\n",
    "        self.drop_prob = config['drop_prob']\n",
    "        self.nb_channels = config['nb_channels']\n",
    "        self.nb_classes = config['nb_classes']\n",
    "        self.nb_filters = config['nb_filters']\n",
    "        self.filter_width = config['filter_width']\n",
    "        \n",
    "        # This way can print out every layers output in forward propagation, which is more convenient to adjust. nn.Sequential(nn.Conv2d(1, self.nb_filters, 3),nn.MaxPool2d([2,1]),...) is more visibility \n",
    "        # Output shape after convolution layer = (input shape – filter shape + 1) + (2*padding/stride)\n",
    "        # nn.Conv2d(input channel, output channel, kernel size(x,x),stride,padding)\n",
    "        self.conv1 = nn.Conv2d(1, self.nb_filters//32, 3) \n",
    "        self.pool = nn.MaxPool2d([2,1])\n",
    "        self.conv2 = nn.Conv2d(2, self.nb_filters//32,(3,1))\n",
    "        \n",
    "        print('Conv1:{}'.format(self.conv1)) \n",
    "        print('pool:{}'.format(self.pool)) \n",
    "        print('Conv2:{}'.format(self.conv2)) \n",
    "        \n",
    "        # nn.Linear(input size,output size)\n",
    "        self.fc1 = nn.Linear(2*4,2)  \n",
    "        #self.fc2 = nn.Linear(8,4)\n",
    "        #self.fc3 = nn.Linear(8, 4)\n",
    "        # In this case, we need to classify 2 classes, fall and ADL, so the last output size need to be 2\n",
    "        self.fc4 = nn.Linear(2, 2)\n",
    "         \n",
    "    # Define forward propagation    \n",
    "    def forward(self, x):  \n",
    "        # Reshape input size (batch, input channels, window size, window channels)\n",
    "        x = x.view(-1, 1, 14, 3) \n",
    "        x = F.relu(self.conv1(x)) \n",
    "        # output(64,64,12,1)\n",
    "        x = self.pool(x)\n",
    "        #(64,64,6,1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #(64,64,4,1)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = F.relu(self.fc3(x)) \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x      \n",
    "        # The loss function, nn.CrossEntropyLoss(), combines LogSoftmax and NLLLoss, so we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2659401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define teacher training loop\n",
    "def training_teacher(trainloader,optimizer,criterion,model,testloader):\n",
    "    loss_train =[]\n",
    "    train_acc = []\n",
    "    total_train=0\n",
    "    correct_train=0\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_gt = []\n",
    "\n",
    "        # Set start time to count the calaulated time of each epoch\n",
    "        start_time = time.time()\n",
    "        # Iterate the trainloader \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            # Send data to GPU\n",
    "            inputs,labels = x.to(device), y.to(device)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass (compute output)\n",
    "            pred = model(inputs) \n",
    "            # Compute loss\n",
    "            loss = criterion(pred, labels.long())\n",
    "            # Backpropagaton\n",
    "            loss.backward() \n",
    "            # Update parameter\n",
    "            optimizer.step() \n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum()\n",
    "            \n",
    "            # Use softmax to normalize the output to a probability distribution over predicted output classes\n",
    "            train_output = F.softmax(pred, dim =1)\n",
    "            y_preds = np.argmax(train_output.cpu().detach().numpy(), axis=-1)\n",
    "            y_true = labels.cpu().numpy().flatten()\n",
    "            train_preds = np.concatenate((np.array(train_preds, int), np.array(y_preds, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "        \n",
    "        train_acc.append(100 * (correct_train / total_train).cpu())\n",
    "        cur_loss = np.mean(train_losses)\n",
    "        loss_train.append(cur_loss)\n",
    "        # Get the time this epoch used\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5.4f} s/epoch | train loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))\n",
    "    \n",
    "    eval_table = evaluation(train_preds, train_gt)\n",
    "\n",
    "    print('Train Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "    print(' Train Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "    print(' Train Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "    print(' Train F1-score:{:.5f}'.format(eval_table[3]), end='')\n",
    "\n",
    "#%% Define teacher testing loop\n",
    "#def testing_teacher(testloader,optimizer,criterion,model):\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_test=0\n",
    "    correct_test=0\n",
    "    # Sets network to eval mod\n",
    "    model.eval() \n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):\n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                inputs ,labels = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                labels = labels.to(torch.float32)\n",
    "\n",
    "                test_output = model(inputs)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(test_output, labels.long())\n",
    "                # Backpropagaton\n",
    "                #loss.backward()\n",
    "                # Update parameter\n",
    "                optimizer.step() \n",
    "                test_losses.append(loss.item())        \n",
    "                _, predicted = torch.max(test_output.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum()\n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)                \n",
    "\n",
    "                y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_true = labels.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))            \n",
    "            \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            test_acc.append(100 * (correct_test / total_test).cpu())\n",
    "#         eval_table = evaluation(test_preds,test_gt)\n",
    "#         print('\\nVal Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_teacher(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_teacher.append(eval_table[0])\n",
    "#         Recall_teacher.append(eval_table[1])\n",
    "#         Precision_teacher.append(eval_table[2])\n",
    "#         F1score_teacher.append(eval_table[3])\n",
    "    #Plot train_loss\n",
    "        teacher_train_preds.extend(test_preds)\n",
    "        teacher_train_gt.extend(test_gt)\n",
    "\n",
    "    show_loss_teacher(loss_train, loss_test)\n",
    "    show_Acc_teacher(train_acc, test_acc)\n",
    "    #return Accuracy_teacher, Recall_teacher, Precision_teacher, F1score_teacher\n",
    "    return teacher_train_preds, teacher_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7945a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define student training loop\n",
    "def training_student(trainloader,optimizer,criterion,model,testloader):\n",
    "    loss_train =[]\n",
    "    train_acc = []\n",
    "    total_train=0\n",
    "    correct_train=0\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_gt = []\n",
    "        # Set start time to count the calaulated time of each epoch\n",
    "        start_time = time.time()\n",
    "        # Iterate the trainloader \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            # Send data to GPU\n",
    "            inputs,labels = x.to(device), y.to(device)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass (compute output)\n",
    "            pred = model(inputs) \n",
    "            # Compute loss\n",
    "            loss = criterion(pred, labels.long())\n",
    "            # Backpropagaton\n",
    "            loss.backward() \n",
    "            # Update parameter\n",
    "            optimizer.step() \n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum()\n",
    "            # Use softmax to normalize the output to a probability distribution over predicted output classes\n",
    "            train_output = F.softmax(pred, dim =1)\n",
    "            y_preds = np.argmax(train_output.cpu().detach().numpy(), axis=-1)\n",
    "            y_true = labels.cpu().numpy().flatten()\n",
    "            train_preds = np.concatenate((np.array(train_preds, int), np.array(y_preds, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "        \n",
    "        cur_loss = np.mean(train_losses)\n",
    "        loss_train.append(cur_loss)\n",
    "        train_acc.append(100 * (correct_train / total_train).cpu())\n",
    "        # Get the time this epoch used\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5.4f} s/epoch | train loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))\n",
    "    \n",
    "    eval_table = evaluation(train_preds, train_gt)\n",
    "    print('Train Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "    print(' Train Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "    print(' Train Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "    print(' Train F1-score:{:.5f}'.format(eval_table[3]), end='')\n",
    "\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_test=0\n",
    "    correct_test=0\n",
    "    # Sets network to eval mod\n",
    "    model.eval()\n",
    "    #start_time = time.time()\n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):\n",
    "            for i, (x, y) in enumerate(testloader):            \n",
    "                inputs ,labels = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                labels = labels.to(torch.float32)\n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer.zero_grad()                \n",
    "                # Compute loss                \n",
    "\n",
    "                test_output = model(inputs)\n",
    "                \n",
    "                loss = criterion(test_output, labels.long())\n",
    "                # Backpropagaton\n",
    "                #loss.backward()\n",
    "                # Update parameter\n",
    "                optimizer.step()\n",
    "                test_losses.append(loss.item())                \n",
    "                \n",
    "                _, predicted = torch.max(test_output.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum()\n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)\n",
    "\n",
    "                y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_true = labels.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))\n",
    "                \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            test_acc.append(100 * (correct_test / total_test).cpu())\n",
    "            elapsed = time.time() - start_time\n",
    "            #print('| epoch {:3d} | {:5.4f} s/epoch | test loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))    \n",
    "#         eval_table = evaluation(test_preds,test_gt)\n",
    "#         print('\\nTest Val Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_student(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_student.append(eval_table[0])\n",
    "#         Recall_student.append(eval_table[1])\n",
    "#         Precision_student.append(eval_table[2])\n",
    "#         F1score_student.append(eval_table[3])\n",
    "        \n",
    "        student_train_preds.extend(test_preds)\n",
    "        student_train_gt.extend(test_gt)\n",
    "    #Plot train_loss\n",
    "    show_loss_student(loss_train, loss_test)\n",
    "    show_Acc_student(train_acc, test_acc)\n",
    "    \n",
    "    #return Accuracy_student, Recall_student, Precision_student, F1score_student\n",
    "    return student_train_preds, student_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94fcb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_teacher_student(trainloader,optimizer,criterion,model_teacher,model_student,alpha,testloader):\n",
    "    loss_train =[]\n",
    "    train_acc = []\n",
    "    total_train=0\n",
    "    correct_train=0\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Freeze the Teacher model\n",
    "    model_teacher.eval()\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_losses = []\n",
    "        train_preds_student = []\n",
    "        train_preds_teacher = []\n",
    "        train_gt = []\n",
    "        # Set start time to count the calaulated time of each epoch\n",
    "        start_time = time.time()\n",
    "        # Iterate the trainloader \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            \n",
    "            # Send data to GPU\n",
    "            inputs,labels = x.to(device), y.to(device)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (compute output)\n",
    "            output_student = model_student(inputs)\n",
    "            output_teacher = model_teacher(inputs)\n",
    "\n",
    "            # Use softmax to normalize the output to a probability distribution over predicted output classes\n",
    "            output_student_soft = F.softmax(output_student, dim =1)\n",
    "            y_preds_student = np.argmax(output_student_soft.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "            output_teacher_soft = F.softmax(output_teacher, dim =1)\n",
    "            y_preds_teacher = np.argmax(output_teacher_soft.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn_kd(output_student, labels, output_teacher, 20, alpha)\n",
    "            # Backpropagaton\n",
    "            loss.backward() \n",
    "            # Update parameter\n",
    "            optimizer.step() \n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            total_hit += torch.sum(torch.argmax(output_student, dim=1) == labels).item()\n",
    "            total_num += len(inputs)\n",
    "            y_true = labels.cpu().numpy().flatten()\n",
    "            train_preds_student = np.concatenate((np.array(train_preds_student, int), np.array(y_preds_student, int)))\n",
    "            train_preds_teacher = np.concatenate((np.array(train_preds_teacher, int), np.array(y_preds_teacher, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "        \n",
    "        cur_loss = np.mean(train_losses)\n",
    "        loss_train.append(cur_loss)\n",
    "        train_acc.append(total_hit/total_num)\n",
    "        # Get the time this epoch used\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5.4f} s/epoch | train loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))\n",
    "    \n",
    "    eval_table = evaluation(train_preds_student, train_gt)\n",
    "    print('Train Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "    print(' Train Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "    print(' Train Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "    print(' Train F1-score:{:.5f}'.format(eval_table[3]), end='')\n",
    "#%% Define Teacher-Student testing loop\n",
    "#def testing_teacher_student(testloader,optimizer,criterion,model_teacher,model_student):\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Sets network to eval mod\n",
    "    model_teacher.eval()\n",
    "    model_student.eval() \n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):            \n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                inputs ,targets = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                targets = targets.to(torch.float32)\n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                test_output = model_student(inputs)\n",
    "                test_teacher = model_teacher(inputs)\n",
    "                # Compute loss\n",
    "                #loss = loss_fn_kd(test_output, labels, output_teacher, 20, alpha)                \n",
    "                loss = loss_fn_kd(test_output, targets, test_teacher, 20, alpha)                \n",
    "                # Backpropagaton\n",
    "                #loss.backward() \n",
    "                # Update parameter\n",
    "                #optimizer.step() \n",
    "                test_losses.append(loss.item())     \n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)                \n",
    "                total_hit += torch.sum(torch.argmax(test_output, dim=1) == targets).item()\n",
    "                total_num += len(inputs)\n",
    "                \n",
    "                y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_true = targets.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))            \n",
    "            \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            #eval_table = evaluation(test_preds,test_gt)\n",
    "            test_acc.append(total_hit/total_num)\n",
    "#         print('\\nVal Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_teacher_student(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_teacher_student.append(eval_table[0])\n",
    "#         Recall_teacher_student.append(eval_table[1])\n",
    "#         Precision_teacher_student.append(eval_table[2])\n",
    "#         F1score_teacher_student.append(eval_table[3])      \n",
    "        teacher_student_train_preds.extend(test_preds)\n",
    "        teacher_student_train_gt.extend(test_gt)\n",
    "    show_loss_teacher_student(loss_train, loss_test)\n",
    "    show_Acc_teacher_student(train_acc,test_acc)\n",
    "    #return Accuracy_teacher_student, Recall_teacher_student, Precision_teacher_student, F1score_teacher_student\n",
    "    return teacher_student_train_preds, teacher_student_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939dc90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subject 1\n",
      "Conv1:Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "pool:MaxPool2d(kernel_size=[2, 1], stride=[2, 1], padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2:Conv2d(2, 2, kernel_size=(3, 1), stride=(1, 1))\n",
      "| epoch   0 | 0.2704 s/epoch | train loss 0.3767\n",
      "| epoch   1 | 0.2272 s/epoch | train loss 0.2760\n",
      "| epoch   2 | 0.2280 s/epoch | train loss 0.2967\n",
      "| epoch   3 | 0.2120 s/epoch | train loss 0.3289\n",
      "| epoch   4 | 0.1994 s/epoch | train loss 0.2845\n",
      "| epoch   5 | 0.1978 s/epoch | train loss 0.2366\n",
      "| epoch   6 | 0.1948 s/epoch | train loss 0.2145\n",
      "| epoch   7 | 0.2265 s/epoch | train loss 0.2058\n",
      "| epoch   8 | 0.2485 s/epoch | train loss 0.1907\n",
      "| epoch   9 | 0.2491 s/epoch | train loss 0.1842\n",
      "| epoch  10 | 0.2421 s/epoch | train loss 0.1798\n",
      "| epoch  11 | 0.2466 s/epoch | train loss 0.1712\n",
      "| epoch  12 | 0.2432 s/epoch | train loss 0.1694\n",
      "| epoch  13 | 0.2467 s/epoch | train loss 0.1615\n",
      "| epoch  14 | 0.2464 s/epoch | train loss 0.1484\n",
      "| epoch  15 | 0.2436 s/epoch | train loss 0.1562\n",
      "| epoch  16 | 0.2832 s/epoch | train loss 0.1489\n",
      "| epoch  17 | 0.2453 s/epoch | train loss 0.1380\n",
      "| epoch  18 | 0.2474 s/epoch | train loss 0.1879\n",
      "| epoch  19 | 0.2457 s/epoch | train loss 0.1505\n",
      "| epoch  20 | 0.2465 s/epoch | train loss 0.1381\n",
      "| epoch  21 | 0.2494 s/epoch | train loss 0.1335\n",
      "| epoch  22 | 0.2472 s/epoch | train loss 0.1401\n",
      "| epoch  23 | 0.2488 s/epoch | train loss 0.1287\n",
      "| epoch  24 | 0.2453 s/epoch | train loss 0.1256\n",
      "| epoch  25 | 0.2449 s/epoch | train loss 0.1275\n",
      "| epoch  26 | 0.2495 s/epoch | train loss 0.1171\n",
      "| epoch  27 | 0.2465 s/epoch | train loss 0.1136\n",
      "| epoch  28 | 0.2490 s/epoch | train loss 0.1030\n",
      "| epoch  29 | 0.2483 s/epoch | train loss 0.1117\n",
      "| epoch  30 | 0.2461 s/epoch | train loss 0.1088\n",
      "| epoch  31 | 0.2467 s/epoch | train loss 0.1111\n",
      "| epoch  32 | 0.2490 s/epoch | train loss 0.1085\n",
      "| epoch  33 | 0.2496 s/epoch | train loss 0.1003\n",
      "| epoch  34 | 0.2502 s/epoch | train loss 0.1099\n",
      "| epoch  35 | 0.2476 s/epoch | train loss 0.0951\n",
      "| epoch  36 | 0.2506 s/epoch | train loss 0.0935\n",
      "| epoch  37 | 0.2466 s/epoch | train loss 0.0985\n",
      "| epoch  38 | 0.2489 s/epoch | train loss 0.0948\n",
      "| epoch  39 | 0.2498 s/epoch | train loss 0.0897\n",
      "| epoch  40 | 0.2458 s/epoch | train loss 0.0905\n",
      "| epoch  41 | 0.2492 s/epoch | train loss 0.2596\n",
      "| epoch  42 | 0.2477 s/epoch | train loss 0.1850\n",
      "| epoch  43 | 0.2467 s/epoch | train loss 0.1523\n",
      "| epoch  44 | 0.2496 s/epoch | train loss 0.1226\n",
      "| epoch  45 | 0.2512 s/epoch | train loss 0.1100\n",
      "| epoch  46 | 0.2508 s/epoch | train loss 0.1080\n",
      "| epoch  47 | 0.2482 s/epoch | train loss 0.0997\n",
      "| epoch  48 | 0.2499 s/epoch | train loss 0.0890\n",
      "| epoch  49 | 0.2485 s/epoch | train loss 0.0887\n",
      "| epoch  50 | 0.2491 s/epoch | train loss 0.0807\n",
      "| epoch  51 | 0.2498 s/epoch | train loss 0.0820\n",
      "| epoch  52 | 0.2482 s/epoch | train loss 0.0812\n",
      "| epoch  53 | 0.2479 s/epoch | train loss 0.0739\n",
      "| epoch  54 | 0.2507 s/epoch | train loss 0.0774\n",
      "| epoch  55 | 0.3925 s/epoch | train loss 0.0963\n",
      "| epoch  56 | 0.6690 s/epoch | train loss 0.1502\n",
      "| epoch  57 | 0.6737 s/epoch | train loss 0.0965\n",
      "| epoch  58 | 0.3001 s/epoch | train loss 0.0840\n",
      "| epoch  59 | 0.2442 s/epoch | train loss 0.0711\n",
      "| epoch  60 | 0.2486 s/epoch | train loss 0.0681\n",
      "| epoch  61 | 0.2514 s/epoch | train loss 0.0713\n",
      "| epoch  62 | 0.2438 s/epoch | train loss 0.0665\n",
      "| epoch  63 | 0.2459 s/epoch | train loss 0.0690\n",
      "| epoch  64 | 0.2448 s/epoch | train loss 0.0626\n",
      "| epoch  65 | 0.2435 s/epoch | train loss 0.0621\n",
      "| epoch  66 | 0.2467 s/epoch | train loss 0.0567\n",
      "| epoch  67 | 0.2464 s/epoch | train loss 0.0618\n",
      "| epoch  68 | 0.2653 s/epoch | train loss 0.0618\n",
      "| epoch  69 | 0.2960 s/epoch | train loss 0.0572\n",
      "| epoch  70 | 0.2918 s/epoch | train loss 0.0516\n",
      "| epoch  71 | 0.2939 s/epoch | train loss 0.0531\n",
      "| epoch  72 | 0.2952 s/epoch | train loss 0.0614\n",
      "| epoch  73 | 0.2957 s/epoch | train loss 0.0508\n",
      "| epoch  74 | 0.3778 s/epoch | train loss 0.0634\n",
      "| epoch  75 | 0.7706 s/epoch | train loss 0.0465\n"
     ]
    }
   ],
   "source": [
    "#%% Evaluation\n",
    "def evaluation(pred,target):\n",
    "    acu = accuracy_score(pred, target)\n",
    "    rec = recall_score(pred, target)\n",
    "    pre = precision_score(pred, target)\n",
    "    f1 = f1_score(pred, target)  \n",
    "    return np.array([acu, rec, pre, f1])\n",
    "\n",
    "#%% Train\n",
    "Accuracy_teacher = []\n",
    "Recall_teacher = []\n",
    "Precision_teacher = []\n",
    "F1score_teacher = []\n",
    "\n",
    "Accuracy_student = []\n",
    "Recall_student = []\n",
    "Precision_student = []\n",
    "F1score_student = []\n",
    "\n",
    "\n",
    "Accuracy_teacher_student = []\n",
    "Recall_teacher_student = []\n",
    "Precision_teacher_student = []\n",
    "F1score_teacher_student = []\n",
    "\n",
    "\n",
    "teacher_train_preds = []\n",
    "teacher_train_gt = []\n",
    "\n",
    "\n",
    "student_train_preds = []\n",
    "student_train_gt = []\n",
    "\n",
    "\n",
    "\n",
    "#for i in range(Average_times):\n",
    "lis = [1,2,3,4,5,7,8,9,10,11,12,13,14,15,16,18,19,21,22,23]\n",
    "for sub in lis:\n",
    "    print(\"\\nSubject\",sub)\n",
    "    test = (sliding_waist_subject == sub)\n",
    "    train = ~test\n",
    "\n",
    "    norm_sliding_waist_data = []\n",
    "    norm_sliding_waist_test = []\n",
    "    min_norm_value = []\n",
    "    min_norm_index = []\n",
    "    max_norm_value = []\n",
    "    max_norm_index = []\n",
    "    hori_sliding_waist_data = []\n",
    "    hori_sliding_waist_test = []\n",
    "    sliding_waist_data_hori = sliding_waist_data\n",
    "    sliding_waist_data_hori[:,:,0] = sliding_waist_data_hori[:,:,0]*0\n",
    "    min_hori_value = []\n",
    "    min_hori_index = []\n",
    "    max_hori_value = []\n",
    "    max_hori_index = []\n",
    "    \n",
    "    \n",
    "    X_train = sliding_waist_data[train]\n",
    "    Y_train_gd = sliding_waist_label[train]\n",
    "    X_test = sliding_waist_data[test]\n",
    "    Y_test_gd = sliding_waist_label[test]\n",
    "\n",
    "    X_train_hori = sliding_waist_data_hori[train]\n",
    "    X_test_hori = sliding_waist_data_hori[test]\n",
    "\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        c1 = np.linalg.norm(X_train[i].astype('float16'), axis=1)\n",
    "        norm_sliding_waist_data.append(c1)\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        c2 = np.linalg.norm(X_test[i].astype('float16'), axis=1)\n",
    "        norm_sliding_waist_test.append(c2)\n",
    "\n",
    "    for i in range(len(X_train_hori)):\n",
    "        c1 = np.linalg.norm(X_train_hori[i].astype('float16'), axis=1)\n",
    "        hori_sliding_waist_data.append(c1)\n",
    "\n",
    "    for i in range(len(X_test_hori)):\n",
    "        c2 = np.linalg.norm(X_test_hori[i].astype('float16'), axis=1)\n",
    "        hori_sliding_waist_test.append(c2)\n",
    "\n",
    "    \n",
    "    for i in range(len(Y_train_gd)):\n",
    "        if Y_train_gd[i] ==1:\n",
    "            d1 = np.min(norm_sliding_waist_data[i])\n",
    "            min_norm_value.append(d1)\n",
    "            d2 = np.argmin(norm_sliding_waist_data[i])\n",
    "            min_norm_index.append(d2)\n",
    "\n",
    "            d3 = np.min(hori_sliding_waist_data[i])\n",
    "            min_hori_value.append(d3)\n",
    "            d4 = np.argmin(hori_sliding_waist_data[i])\n",
    "            min_hori_index.append(d4)\n",
    "            \n",
    "        else:\n",
    "            if len(norm_sliding_waist_data[i]) > 0:\n",
    "                e1 = np.max(norm_sliding_waist_data[i])\n",
    "                max_norm_value.append(e1)\n",
    "                e2 = np.argmax(norm_sliding_waist_data[i])\n",
    "                max_norm_index.append(e2)\n",
    "\n",
    "                e3 = np.max(hori_sliding_waist_data[i])\n",
    "                max_hori_value.append(e3)\n",
    "                e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "                max_hori_index.append(e4)\n",
    "            else:\n",
    "                e1 = 0\n",
    "                max_norm_value.append(e1)\n",
    "                e2 = 0\n",
    "                max_norm_index.append(e2)\n",
    "\n",
    "                e3 = np.max(hori_sliding_waist_data[i])\n",
    "                max_hori_value.append(e3)\n",
    "                e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "                max_hori_index.append(e4)\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "    MIN_train = np.min(min_norm_value)\n",
    "    MAX_train = np.max(max_norm_value)\n",
    "\n",
    "    MIN_train_hori = np.min(min_hori_value)\n",
    "    MAX_train_hori = np.max(max_hori_value)\n",
    "\n",
    "    unidentified_data = []\n",
    "    unidentified_label = []\n",
    "\n",
    "    for i in range(len(norm_sliding_waist_test)):\n",
    "       if max(norm_sliding_waist_test[i]) > MIN_train and max(norm_sliding_waist_test[i]) < MAX_train\\\n",
    "          and max(hori_sliding_waist_test[i]) > MIN_train_hori and max(hori_sliding_waist_test[i]) < MAX_train_hori:\n",
    "            unidentified_data.append(X_test[i])\n",
    "            unidentified_label.append(Y_test_gd[i])\n",
    "\n",
    "    unidentified_data = np.array(unidentified_data)\n",
    "    unidentified_label = np.array(unidentified_label)\n",
    "\n",
    "    #breakpoint() # insert breakpoint   \n",
    "    \n",
    "    # Initializes the train and validation dataset in Torch format\n",
    "    x_train_tensor = torch.from_numpy(X_train.astype('float16')).to(device) \n",
    "    x_train_tensor = x_train_tensor.reshape([-1,1,14,3]) \n",
    "    x_test_tensor = torch.from_numpy(unidentified_data.astype('float16')).to(device)\n",
    "    x_test_tensor = x_test_tensor.reshape([-1,1,14,3])\n",
    "    \n",
    "    y_train_gd_tensor =  torch.from_numpy(Y_train_gd).to(device)\n",
    "    y_test_gd_tensor =  torch.from_numpy(unidentified_label).to(device)\n",
    "    \n",
    "    # Dataset wrapping tensors.Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "    deal_dataset = TensorDataset(x_train_tensor, y_train_gd_tensor) \n",
    "    test_dataset = TensorDataset(x_test_tensor,y_test_gd_tensor)\n",
    "    \n",
    "    config['window_size'] = X_train.shape[1]\n",
    "    config['nb_channels'] = X_train.shape[2]\n",
    "    config['nb_classes'] = 2\n",
    "    \n",
    "    # Sends network to the GPU and sets it to training mode\n",
    "    ResNet = Net_Teacher(block, [3, 4, 6, 3], 1, 2)\n",
    "    model_teacher = ResNet.to(device) \n",
    "    model_teacher.train()\n",
    "\n",
    "    model_student = Net_Student(config).to(device) \n",
    "    model_student.train()\n",
    "    \n",
    "    # DataLoader represents a Python iterable over a dataset\n",
    "    trainloader = DataLoader(deal_dataset, batch_size=config['batch_size'],shuffle=True) \n",
    "    testloader = DataLoader(test_dataset, batch_size=config['batch_size'],shuffle=False)\n",
    "    # Initialize the optimizer and loss\n",
    "    optimizer_teacher = torch.optim.Adam(model_teacher.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    optimizer_student = torch.optim.Adam(model_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Start training and testing Teacher and Student Model\n",
    "    training_teacher(trainloader,optimizer_teacher,criterion,model_teacher,testloader)\n",
    "    #testing_teacher(testloader,optimizer_teacher,criterion,model_teacher)\n",
    "  \n",
    "    training_student(trainloader,optimizer_student,criterion,model_student,testloader)    \n",
    "    #testing_student(testloader,optimizer_student,criterion,model_student)\n",
    "\n",
    "#print(teacher_train_preds)\n",
    "#print(teacher_train_gt)\n",
    "Teacher_eval_table = evaluation(teacher_train_preds, teacher_train_gt)\n",
    "\n",
    "Student_eval_table = evaluation(student_train_preds, student_train_gt)\n",
    "\n",
    "show_CM_teacher(teacher_train_preds, teacher_train_gt)\n",
    "show_CM_student(student_train_preds, student_train_gt)\n",
    "\n",
    "print(\"Teacher(RestNet50)_Acc:\",Teacher_eval_table[0])\n",
    "print(\"Teacher(RestNet50)_Rec:\",Teacher_eval_table[1])\n",
    "print(\"Teacher(RestNet50)_Pre:\",Teacher_eval_table[2])\n",
    "print(\"Teacher(RestNet50)_F1:\",Teacher_eval_table[3])\n",
    "\n",
    "print(\"Original_Student(CNN_Small)_Acc:\",Student_eval_table[0])\n",
    "print(\"Original_Student(CNN_Small)_Rec:\",Student_eval_table[1])\n",
    "print(\"Original_Student(CNN_Small)_Pre:\",Student_eval_table[2])\n",
    "print(\"Original_Student(CNN_Small)_F1:\",Student_eval_table[3]) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_student_train_preds = []\n",
    "teacher_student_train_gt = []\n",
    "\n",
    "for sub in lis:\n",
    "    print(\"\\nSubject\",sub)\n",
    "    test = (sliding_waist_subject == sub)\n",
    "    train = ~test\n",
    "\n",
    "    norm_sliding_waist_data = []\n",
    "    norm_sliding_waist_test = []\n",
    "    min_norm_value = []\n",
    "    min_norm_index = []\n",
    "    max_norm_value = []\n",
    "    max_norm_index = []\n",
    "    hori_sliding_waist_data = []\n",
    "    hori_sliding_waist_test = []\n",
    "    sliding_waist_data_hori = sliding_waist_data\n",
    "    sliding_waist_data_hori[:,:,0] = sliding_waist_data_hori[:,:,0]*0\n",
    "    min_hori_value = []\n",
    "    min_hori_index = []\n",
    "    max_hori_value = []\n",
    "    max_hori_index = []\n",
    "    \n",
    "    X_train = sliding_waist_data[train]\n",
    "    Y_train_gd = sliding_waist_label[train]\n",
    "    X_test = sliding_waist_data[test]\n",
    "    Y_test_gd = sliding_waist_label[test]\n",
    "\n",
    "    X_train_hori = sliding_waist_data_hori[train]\n",
    "    X_test_hori = sliding_waist_data_hori[test]\n",
    "\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        c1 = np.linalg.norm(X_train[i].astype('float16'), axis=1)\n",
    "        norm_sliding_waist_data.append(c1)\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        c2 = np.linalg.norm(X_test[i].astype('float16'), axis=1)\n",
    "        norm_sliding_waist_test.append(c2)\n",
    "\n",
    "    for i in range(len(X_train_hori)):\n",
    "        c1 = np.linalg.norm(X_train_hori[i].astype('float16'), axis=1)\n",
    "        hori_sliding_waist_data.append(c1)\n",
    "\n",
    "    for i in range(len(X_test_hori)):\n",
    "        c2 = np.linalg.norm(X_test_hori[i].astype('float16'), axis=1)\n",
    "        hori_sliding_waist_test.append(c2)\n",
    "\n",
    "    \n",
    "    for i in range(len(Y_train_gd)):\n",
    "        if Y_train_gd[i] ==1:\n",
    "            d1 = np.min(norm_sliding_waist_data[i])\n",
    "            min_norm_value.append(d1)\n",
    "            d2 = np.argmin(norm_sliding_waist_data[i])\n",
    "            min_norm_index.append(d2)\n",
    "\n",
    "            d3 = np.min(hori_sliding_waist_data[i])\n",
    "            min_hori_value.append(d3)\n",
    "            d4 = np.argmin(hori_sliding_waist_data[i])\n",
    "            min_hori_index.append(d4)\n",
    "            \n",
    "        else:\n",
    "            e1 = np.max(norm_sliding_waist_data[i])\n",
    "            max_norm_value.append(e1)\n",
    "            e2 = np.argmax(norm_sliding_waist_data[i])\n",
    "            max_norm_index.append(e2)\n",
    "\n",
    "            e3 = np.max(hori_sliding_waist_data[i])\n",
    "            max_hori_value.append(e3)\n",
    "            e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "            max_hori_index.append(e4)\n",
    "        \n",
    "\n",
    "    MIN_train = np.min(min_norm_value)\n",
    "    MAX_train = np.max(max_norm_value)\n",
    "\n",
    "    MIN_train_hori = np.min(min_hori_value)\n",
    "    MAX_train_hori = np.max(max_hori_value)\n",
    "\n",
    "    unidentified_data = []\n",
    "    unidentified_label = []\n",
    "\n",
    "    for i in range(len(norm_sliding_waist_test)):\n",
    "       if max(norm_sliding_waist_test[i]) > MIN_train and max(norm_sliding_waist_test[i]) < MAX_train\\\n",
    "          and max(hori_sliding_waist_test[i]) > MIN_train_hori and max(hori_sliding_waist_test[i]) < MAX_train_hori:\n",
    "            unidentified_data.append(X_test[i])\n",
    "            unidentified_label.append(Y_test_gd[i])\n",
    "\n",
    "    unidentified_data = np.array(unidentified_data)\n",
    "    unidentified_label = np.array(unidentified_label)\n",
    "\n",
    "    #breakpoint() # insert breakpoint   \n",
    "    \n",
    "    # Initializes the train and validation dataset in Torch format\n",
    "    x_train_tensor = torch.from_numpy(X_train.astype('float16')).to(device) \n",
    "    x_train_tensor = x_train_tensor.reshape([-1,1,14,3]) \n",
    "    x_test_tensor = torch.from_numpy(unidentified_data.astype('float16')).to(device)\n",
    "    x_test_tensor = x_test_tensor.reshape([-1,1,14,3])\n",
    "    \n",
    "    y_train_gd_tensor =  torch.from_numpy(Y_train_gd).to(device)\n",
    "    y_test_gd_tensor =  torch.from_numpy(unidentified_label).to(device)\n",
    "    \n",
    "    # Dataset wrapping tensors.Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "    deal_dataset = TensorDataset(x_train_tensor, y_train_gd_tensor) \n",
    "    test_dataset = TensorDataset(x_test_tensor,y_test_gd_tensor)\n",
    "    \n",
    "    config['window_size'] = X_train.shape[1]\n",
    "    config['nb_channels'] = X_train.shape[2]\n",
    "    config['nb_classes'] = 2\n",
    "    \n",
    "    # Sends network to the GPU and sets it to training mode\n",
    "\n",
    "    model_teacher_student = Net_Teacher_Student(config).to(device) \n",
    "    model_teacher_student.train() \n",
    "    \n",
    "    # DataLoader represents a Python iterable over a dataset\n",
    "    trainloader = DataLoader(deal_dataset, batch_size=config['batch_size'],shuffle=True) \n",
    "    testloader = DataLoader(test_dataset, batch_size=config['batch_size'],shuffle=False)\n",
    "    # Initialize the optimizer and loss\n",
    "    optimizer_teacher_student = torch.optim.Adam(model_teacher_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Establishing Teacher-Student Model\n",
    "    alpha = 0.5\n",
    "    training_teacher_student(trainloader,optimizer_teacher_student,criterion,model_teacher,model_teacher_student,alpha,testloader)\n",
    "    #testing_teacher_student(testloader,optimizer_teacher_student,criterion,model_teacher,model_teacher_student)\n",
    "\n",
    "show_CM_teacher_student(teacher_student_train_preds, teacher_student_train_gt)\n",
    "teacher_Student_eval_table = evaluation(teacher_student_train_preds, teacher_student_train_gt)    \n",
    "\n",
    "print(\"Distillation_Student(CNN_Small)_Acc:\",teacher_Student_eval_table[0])\n",
    "print(\"Distillation_Student(CNN_Small)_Rec:\",teacher_Student_eval_table[1])\n",
    "print(\"Distillation_Student(CNN_Small)_Pre:\",teacher_Student_eval_table[2])\n",
    "print(\"Distillation_Student(CNN_Small)_F1:\",teacher_Student_eval_table[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Teacher(RestNet50)_Acc:\",sum(Accuracy_teacher)/14)\n",
    "# print(\"Teacher(RestNet50)_Rec:\",sum(Recall_teacher)/11)\n",
    "# print(\"Teacher(RestNet50)_Pre:\",sum(Precision_teacher)/11)\n",
    "# print(\"Teacher(RestNet50)_F1:\",sum(F1score_teacher)/11)\n",
    "\n",
    "# print(\"Original_Student(CNN)_Acc:\",sum(Accuracy_student)/14)\n",
    "# print(\"Original_Student(CNN)_Rec:\",sum(Recall_student)/11)\n",
    "# print(\"Original_Student(CNN)_Pre:\",sum(Precision_student)/11)\n",
    "# print(\"Original_Student(CNN)_F1:\",sum(F1score_student)/11)\n",
    "\n",
    "# print(\"Distillation_Student(CNN)_Acc:\",sum(Accuracy_teacher_student)/14)\n",
    "# print(\"Distillation_Student(CNN)_Rec:\",sum(Recall_teacher_student)/11)\n",
    "# print(\"Distillation_Student(CNN)_Pre:\",sum(Precision_teacher_student)/11)\n",
    "# print(\"Distillation_Student(CNN)_F1:\",sum(F1score_teacher_student)/11)\n",
    "\n",
    "\n",
    "\n",
    "dummy_input = torch.randn(64,1,14,3,device=device)\n",
    "flops_teacher, params_teachar = profile(model_teacher,(dummy_input,))\n",
    "print('\\n\\nflops_teacher: %.3f M, params_teachar: %.3f M' % (flops_teacher / 1000000.0, params_teachar / 1000000.0))\n",
    "dummy_input = torch.randn(64,1,14,3,device=device)\n",
    "flops_student, params_student = profile(model_student,(dummy_input,))\n",
    "print('flops:',flops_student, 'params', params_student)\n",
    "print('\\nflops_student: %.3f K, params_student: %.3f K' % (flops_student / 1000.0, params_student / 1000.0))\n",
    "dummy_input = torch.randn(64,1,14,3,device=device)\n",
    "flops_teacher_student, params_teacher_student = profile(model_teacher_student,(dummy_input,))\n",
    "print('flops:',flops_teacher_student, 'params', params_teacher_student)\n",
    "print('\\nflops_teacher_student: %.3f K, params_teacher_student: %.3f K' % (flops_teacher_student / 1000.0, params_teacher_student / 1000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e1d61c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
