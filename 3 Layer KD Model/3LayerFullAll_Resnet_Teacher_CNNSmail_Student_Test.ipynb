{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7962c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import time\n",
    "from torch.nn import init\n",
    "from thop import profile\n",
    "import datetime\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8715560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to install CUDA and cuDNN\n",
    "# Determine if your system supports CUDA.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  \n",
    "#%% Read data\n",
    "data=pd.read_hdf(\"D:/conda/FallAllD.h5\") #\n",
    "data = data.drop(columns = ['Gyr','Mag','Bar'])\n",
    "#%% Label\n",
    "data_waist = []\n",
    "label_waist = []\n",
    "subject_waist = []\n",
    "PATH = \"D:/conda/111_1117/FullAll_3LayerModel_CNNSmall.pt\"\n",
    "for i in range(len(data)):\n",
    "    if(data['Device'][i] == 'Waist'):\n",
    "        a = data['Acc'][i]*0.000244\n",
    "        # Normalize each data between 0 and 1\n",
    "        b = b = (a-np.amin(a))/(np.amax(a)-np.amin(a)) \n",
    "        data_waist.append(b) \n",
    "        subject_waist.append(data['SubjectID'][i])\n",
    "        # Class: fall = 0 ,ADL = 1 \n",
    "        if (data['ActivityID'][i] > 100): \n",
    "            label_waist.append(1)\n",
    "        else:\n",
    "            label_waist.append(0)\n",
    "\n",
    "# Change list to array\n",
    "data_waist = np.array(data_waist)            \n",
    "label_waist = np.array(label_waist)\n",
    "subject_waist = np.array(subject_waist)\n",
    "\n",
    "# Downsampling 1/128\n",
    "down_data = []\n",
    "# (start:size:step)\n",
    "down_data = data_waist[::1,::128,::1]           \n",
    "#%% Sliding_window\n",
    "sliding_waist_data =[]\n",
    "sliding_waist_label =[]\n",
    "sliding_waist_subject =[]   \n",
    "\n",
    "# The signal of each second \n",
    "per_sec = 38/20\n",
    "sec = 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba63d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(len(down_data)):\n",
    "    if label_waist[num] ==1:\n",
    "        # Data[index][start:end,axis]\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.5)):int(np.uint(sec*per_sec*1.5)+1),:])\n",
    "        # Label each window with class and subject\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.75)):int(np.uint(sec*per_sec*1.75-1))+1,:])\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1)):int(np.uint(sec*per_sec*2))+1,:])\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1.25)):int(np.uint(sec*per_sec*2.25))+1,:])\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        \n",
    "    else:\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.5)):int(np.uint(sec*per_sec*1.5))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.75)):int(np.uint(sec*per_sec*1.75-1))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1)):int(np.uint(sec*per_sec*2))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1.25)):int(np.uint(sec*per_sec*2.25))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        \n",
    "sliding_waist_data = np.array(sliding_waist_data)\n",
    "sliding_waist_label = np.array(sliding_waist_label)\n",
    "sliding_waist_subject = np.array(sliding_waist_subject) \n",
    "\n",
    "np.save('FallALLD_SW_data',sliding_waist_data)\n",
    "np.save('FallALLD_SW_label',sliding_waist_label)\n",
    "np.save('FallALLD_SW_subject',sliding_waist_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2388cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Show Teacher confusion matrix\n",
    "def show_CM_teacher(validation,prediction):     \n",
    "    matrix = metrics.confusion_matrix(validation,prediction)\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(matrix,cmap = 'coolwarm',linecolor= 'white',\n",
    "                linewidths= 1,annot= True,fmt = 'd')\n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.show()\n",
    "    plt.savefig('Prediction (Teacher) SubjectID' + str(sub) + '.png')\n",
    "\n",
    "#%% Show Student confusion matrix\n",
    "def show_CM_student(validation,prediction):     \n",
    "    matrix = metrics.confusion_matrix(validation,prediction)\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(matrix,cmap = 'coolwarm',linecolor= 'white',\n",
    "                linewidths= 1,annot= True,fmt = 'd')\n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.show()\n",
    "    plt.savefig('Prediction (Student) SubjectID' + str(sub) + '.png')\n",
    "    \n",
    "#%% Show Teacher-Student confusion matrix\n",
    "def show_CM_teacher_student(validation,prediction):     \n",
    "    matrix = metrics.confusion_matrix(validation,prediction)\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(matrix,cmap = 'coolwarm',linecolor= 'white',\n",
    "                linewidths= 1,annot= True,fmt = 'd')\n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.show()\n",
    "    plt.savefig('Prediction (Teacher-Student) SubjectID' + str(sub) + '.png')\n",
    "def show_loss_teacher(loss_train,loss_test):     \n",
    "    plt.plot(loss_train, label = 'trian_loss')\n",
    "    plt.plot(loss_test, label = 'test_loss')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend([\"Train Loss\",\"Test Loss\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Loss (teacher) SubjectID' + str(sub) + '.png')\n",
    "def show_Acc_teacher(train_acc, test_acc):     \n",
    "    plt.plot(train_acc, label = 'trian_Acc')\n",
    "    plt.plot(test_acc, label = 'test_Acc')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend([\"Train Acc\",\"Test Acc\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Acc (teacher) SubjectID' + str(sub) + '.png')\n",
    "#Loss_student\n",
    "def show_loss_student(loss_train,loss_test):     \n",
    "    plt.plot(loss_train, label = 'trian_loss')\n",
    "    plt.plot(loss_test, label = 'test_loss')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend([\"Train Loss\",\"Test Loss\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Loss (Student) SubjectID' + str(sub) + '.png')   \n",
    "def show_Acc_student(train_acc, test_acc):     \n",
    "    plt.plot(train_acc, label = 'trian_Acc')\n",
    "    plt.plot(test_acc, label = 'test_Acc')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend([\"Train Acc\",\"Test Acc\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Acc (student) SubjectID' + str(sub) + '.png')\n",
    "def show_loss_teacher_student(loss_train,loss_test):     \n",
    "    plt.plot(loss_train, label = 'trian_loss')\n",
    "    plt.plot(loss_test, label = 'test_loss')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend([\"Train Loss\",\"Test Loss\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Loss (teacher_Student) SubjectID' + str(sub) + '.png')   \n",
    "def show_Acc_teacher_student(train_acc, test_acc):     \n",
    "    plt.plot(train_acc, label = 'trian_Acc')\n",
    "    plt.plot(test_acc, label = 'test_Acc')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend([\"Train Acc\",\"Test Acc\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Acc (teacher_student) SubjectID' + str(sub) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f7e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):\n",
    "    # 一般的Cross Entropy\n",
    "    labels=labels.type(torch.cuda.LongTensor)\n",
    "    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    # 讓logits的log_softmax對目標機率(teacher的logits/T後softmax)做KL Divergence。\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs/T, dim=1),\n",
    "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)\n",
    "    return hard_loss + soft_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "823877e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hswish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        out = x * F.relu6(x + 3, inplace=True) / 6\n",
    "        return out\n",
    "\n",
    "\n",
    "class hsigmoid(nn.Module):\n",
    "    def forward(self, x):\n",
    "        out = F.relu6(x + 3, inplace=True) / 6\n",
    "        return out\n",
    "\n",
    "\n",
    "class SeModule(nn.Module):\n",
    "    def __init__(self, in_size, reduction=4):\n",
    "        super(SeModule, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_size, in_size // reduction, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_size // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_size // reduction, in_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_size),\n",
    "            hsigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''expand + depthwise + pointwise'''\n",
    "    def __init__(self, kernel_size, in_size, expand_size, out_size, nolinear, semodule, stride):\n",
    "        super(Block, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.se = semodule\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(expand_size)\n",
    "        self.nolinear1 = nolinear\n",
    "        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=expand_size, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(expand_size)\n",
    "        self.nolinear2 = nolinear\n",
    "        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_size)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1 and in_size != out_size:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_size),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.nolinear1(self.bn1(self.conv1(x)))\n",
    "        out = self.nolinear2(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        if self.se != None:\n",
    "            out = self.se(out)\n",
    "        out = out + self.shortcut(x) if self.stride==1 else out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1446d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% This is the config object which contains all relevant settings.    \n",
    "config = {\n",
    "    'nb_filters': 64,\n",
    "    'filter_width': 1,\n",
    "    'drop_prob': 0.5,\n",
    "    'epochs': 200,#200\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-3,#1e-3\n",
    "    'weight_decay': 2e-6,\n",
    "    'gpu_name': 'cuda:0',\n",
    "    'print_counts': False,\n",
    "    'lr_factor' : 2,\n",
    "    'lr_warmup' : 40\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35895351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define teacher neural network\n",
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "      super(block, self).__init__()\n",
    "      self.expansion = 4\n",
    "      self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "      self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "      self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1,bias=False)\n",
    "      self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "      self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "      self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "      self.relu = nn.ReLU()\n",
    "      self.identity_downsample = identity_downsample\n",
    "      self.stride = stride\n",
    "\n",
    "  def forward(self, x):\n",
    "      identity = x.clone()\n",
    "      x = self.conv1(x)\n",
    "      x = self.bn1(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.bn2(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.conv3(x)\n",
    "      x = self.bn3(x)\n",
    "\n",
    "      if self.identity_downsample is not None:\n",
    "          identity = self.identity_downsample(identity)\n",
    "\n",
    "      x += identity\n",
    "      x = self.relu(x)\n",
    "      return x\n",
    "\n",
    "class Net_Teacher(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes): \n",
    "        # Return a proxy object that delegates method calls to a parent or sibling class of type, which is nn.Module in this case\n",
    "        super(Net_Teacher, self).__init__()\n",
    "        self.in_channels = 64        \n",
    "        # This way can print out every layers output in forward propagation, which is more convenient to adjust. nn.Sequential(nn.Conv2d(1, self.nb_filters, 3),nn.MaxPool2d([2,1]),...) is more visibility \n",
    "        # Output shape after convolution layer = (input shape – filter shape + 1) + (2*padding/stride)\n",
    "        # nn.Conv2d(input channel, output channel, kernel size(x,x),stride,padding)\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=3, stride=2, padding=3, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
    "        self.layer1 = self._make_layer(block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self._make_layer(block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], intermediate_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x            \n",
    "        # The loss function, nn.CrossEntropyLoss(), combines LogSoftmax and NLLLoss, so we should use softmax after loss calculation\n",
    "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
    "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
    "        # to the layer that's ahead\n",
    "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    intermediate_channels * 4,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(intermediate_channels * 4),\n",
    "            )\n",
    "\n",
    "        layers.append(\n",
    "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
    "        )\n",
    "\n",
    "        # The expansion size is always 4 for ResNet 50,101,152\n",
    "        self.in_channels = intermediate_channels * 4\n",
    "\n",
    "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
    "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
    "        # and also same amount of channels.\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, intermediate_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "#%% Define student neural network\n",
    "class Net_Student(nn.Module):\n",
    "    def __init__(self,config): \n",
    "        # Return a proxy object that delegates method calls to a parent or sibling class of type, which is nn.Module in this case\n",
    "        super(Net_Student, self).__init__()\n",
    "        # Input hyperparameter \n",
    "        self.window_size = config['window_size']\n",
    "        self.drop_prob = config['drop_prob']\n",
    "        self.nb_channels = config['nb_channels']\n",
    "        self.nb_classes = config['nb_classes']\n",
    "        self.nb_filters = config['nb_filters']\n",
    "        self.filter_width = config['filter_width']\n",
    "        \n",
    "        # This way can print out every layers output in forward propagation, which is more convenient to adjust. nn.Sequential(nn.Conv2d(1, self.nb_filters, 3),nn.MaxPool2d([2,1]),...) is more visibility \n",
    "        # Output shape after convolution layer = (input shape – filter shape + 1) + (2*padding/stride)\n",
    "        # nn.Conv2d(input channel, output channel, kernel size(x,x),stride,padding)\n",
    "        self.conv1 = nn.Conv2d(1, self.nb_filters//32, 3) \n",
    "        self.pool = nn.MaxPool2d([2,1])\n",
    "        self.conv2 = nn.Conv2d(2, self.nb_filters//32,(3,1))\n",
    "        \n",
    "\n",
    "        \n",
    "        # nn.Linear(input size,output size)\n",
    "        self.fc1 = nn.Linear(2*4,2)  \n",
    "        #self.fc2 = nn.Linear(8,4)\n",
    "        #self.fc3 = nn.Linear(8, 4)\n",
    "        # In this case, we need to classify 2 classes, fall and ADL, so the last output size need to be 2\n",
    "        self.fc4 = nn.Linear(2, 2)\n",
    "         \n",
    "    # Define forward propagation    \n",
    "    def forward(self, x):  \n",
    "        # Reshape input size (batch, input channels, window size, window channels)\n",
    "        x = x.view(-1, 1, 14, 3) \n",
    "        x = F.relu(self.conv1(x)) \n",
    "        # output(64,64,12,1)\n",
    "        x = self.pool(x)\n",
    "        #(64,64,6,1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #(64,64,4,1)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = F.relu(self.fc3(x)) \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x      \n",
    "        # The loss function, nn.CrossEntropyLoss(), combines LogSoftmax and NLLLoss, so we should use softmax after loss calculation\n",
    "\n",
    "#%% Define student neural network\n",
    "class Net_Teacher_Student(nn.Module):\n",
    "    def __init__(self,config): \n",
    "        # Return a proxy object that delegates method calls to a parent or sibling class of type, which is nn.Module in this case\n",
    "        super(Net_Teacher_Student, self).__init__()\n",
    "        # Input hyperparameter \n",
    "        self.window_size = config['window_size']\n",
    "        self.drop_prob = config['drop_prob']\n",
    "        self.nb_channels = config['nb_channels']\n",
    "        self.nb_classes = config['nb_classes']\n",
    "        self.nb_filters = config['nb_filters']\n",
    "        self.filter_width = config['filter_width']\n",
    "        \n",
    "        # This way can print out every layers output in forward propagation, which is more convenient to adjust. nn.Sequential(nn.Conv2d(1, self.nb_filters, 3),nn.MaxPool2d([2,1]),...) is more visibility \n",
    "        # Output shape after convolution layer = (input shape – filter shape + 1) + (2*padding/stride)\n",
    "        # nn.Conv2d(input channel, output channel, kernel size(x,x),stride,padding)\n",
    "        self.conv1 = nn.Conv2d(1, self.nb_filters//32, 3) \n",
    "        self.pool = nn.MaxPool2d([2,1])\n",
    "        self.conv2 = nn.Conv2d(2, self.nb_filters//32,(3,1))\n",
    "        \n",
    "\n",
    "        \n",
    "        # nn.Linear(input size,output size)\n",
    "        self.fc1 = nn.Linear(2*4,2)  \n",
    "        #self.fc2 = nn.Linear(8,4)\n",
    "        #self.fc3 = nn.Linear(8, 4)\n",
    "        # In this case, we need to classify 2 classes, fall and ADL, so the last output size need to be 2\n",
    "        self.fc4 = nn.Linear(2, 2)\n",
    "         \n",
    "    # Define forward propagation    \n",
    "    def forward(self, x):  \n",
    "        # Reshape input size (batch, input channels, window size, window channels)\n",
    "        x = x.view(-1, 1, 14, 3) \n",
    "        x = F.relu(self.conv1(x)) \n",
    "        # output(64,64,12,1)\n",
    "        x = self.pool(x)\n",
    "        #(64,64,6,1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #(64,64,4,1)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = F.relu(self.fc3(x)) \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x      \n",
    "        # The loss function, nn.CrossEntropyLoss(), combines LogSoftmax and NLLLoss, so we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd17de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3_Small(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MobileNetV3_Small, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.hs1 = hswish()\n",
    "\n",
    "        self.bneck = nn.Sequential(\n",
    "            Block(3, 64, 64, 16, nn.ReLU(inplace=True), SeModule(16), 2),\n",
    "            Block(3, 16, 72, 24, nn.ReLU(inplace=True), None, 2),\n",
    "            Block(3, 24, 88, 24, nn.ReLU(inplace=True), None, 1),\n",
    "            Block(5, 24, 96, 40, hswish(), SeModule(40), 2),\n",
    "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
    "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
    "            Block(5, 40, 120, 48, hswish(), SeModule(48), 1),\n",
    "            Block(5, 48, 144, 48, hswish(), SeModule(48), 1),\n",
    "            Block(5, 48, 288, 96, hswish(), SeModule(96), 2),\n",
    "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
    "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(576)\n",
    "        self.hs2 = hswish()\n",
    "        self.linear3 = nn.Linear(576, 1280)\n",
    "        self.bn3 = nn.BatchNorm1d(1280)\n",
    "        self.hs3 = hswish()\n",
    "        self.linear4 = nn.Linear(1280, num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hs1(self.bn1(self.conv1(x)))\n",
    "        out = self.bneck(out)\n",
    "        out = self.hs2(self.bn2(self.conv2(out)))\n",
    "        out = F.avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.hs3(self.bn3(self.linear3(out)))\n",
    "        out = self.linear4(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c174952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3_Large(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MobileNetV3_Large, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.hs1 = hswish()\n",
    "\n",
    "        self.bneck = nn.Sequential(\n",
    "            Block(3, 64, 64, 16, nn.ReLU(inplace=True), None, 1),\n",
    "            Block(3, 16, 64, 24, nn.ReLU(inplace=True), None, 2),\n",
    "            Block(3, 24, 72, 24, nn.ReLU(inplace=True), None, 1),\n",
    "            Block(5, 24, 72, 40, nn.ReLU(inplace=True), SeModule(40), 2),\n",
    "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
    "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
    "            Block(3, 40, 240, 80, hswish(), None, 2),\n",
    "            Block(3, 80, 200, 80, hswish(), None, 1),\n",
    "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
    "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
    "            Block(3, 80, 480, 112, hswish(), SeModule(112), 1),\n",
    "            Block(3, 112, 672, 112, hswish(), SeModule(112), 1),\n",
    "            Block(5, 112, 672, 160, hswish(), SeModule(160), 1),\n",
    "            Block(5, 160, 672, 160, hswish(), SeModule(160), 2),\n",
    "            Block(5, 160, 960, 160, hswish(), SeModule(160), 1),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(160, 960, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(960)\n",
    "        self.hs2 = hswish()\n",
    "        self.linear3 = nn.Linear(960, 1280)\n",
    "        self.bn3 = nn.BatchNorm1d(1280)\n",
    "        self.hs3 = hswish()\n",
    "        self.linear4 = nn.Linear(1280, num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hs1(self.bn1(self.conv1(x)))\n",
    "        out = self.bneck(out)\n",
    "        out = self.hs2(self.bn2(self.conv2(out)))\n",
    "        out = F.avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.hs3(self.bn3(self.linear3(out)))\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e09cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3_Small_KD(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MobileNetV3_Small_KD, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.hs1 = hswish()\n",
    "\n",
    "        self.bneck = nn.Sequential(\n",
    "            Block(3, 64, 64, 16, nn.ReLU(inplace=True), SeModule(16), 2),\n",
    "            Block(3, 16, 72, 24, nn.ReLU(inplace=True), None, 2),\n",
    "            Block(3, 24, 88, 24, nn.ReLU(inplace=True), None, 1),\n",
    "            Block(5, 24, 96, 40, hswish(), SeModule(40), 2),\n",
    "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
    "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
    "            Block(5, 40, 120, 48, hswish(), SeModule(48), 1),\n",
    "            Block(5, 48, 144, 48, hswish(), SeModule(48), 1),\n",
    "            Block(5, 48, 288, 96, hswish(), SeModule(96), 2),\n",
    "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
    "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(576)\n",
    "        self.hs2 = hswish()\n",
    "        self.linear3 = nn.Linear(576, 1280)\n",
    "        self.bn3 = nn.BatchNorm1d(1280)\n",
    "        self.hs3 = hswish()\n",
    "        self.linear4 = nn.Linear(1280, num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hs1(self.bn1(self.conv1(x)))\n",
    "        out = self.bneck(out)\n",
    "        out = self.hs2(self.bn2(self.conv2(out)))\n",
    "        out = F.avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.hs3(self.bn3(self.linear3(out)))\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2659401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define teacher training loop\n",
    "def training_teacher(trainloader,optimizer,criterion,model,testloader,lis):\n",
    "    loss_train =[]\n",
    "    train_acc = []\n",
    "    total_train=0\n",
    "    correct_train=0\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_gt = []\n",
    "\n",
    "        # Set start time to count the calaulated time of each epoch\n",
    "        start_time = time.time()\n",
    "        # Iterate the trainloader \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            # Send data to GPU\n",
    "            inputs,labels = x.to(device), y.to(device)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass (compute output)\n",
    "            pred = model(inputs) \n",
    "            # Compute loss\n",
    "            loss = criterion(pred, labels.long())\n",
    "            # Backpropagaton\n",
    "            loss.backward() \n",
    "            # Update parameter\n",
    "            optimizer.step() \n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum()\n",
    "            \n",
    "            # Use softmax to normalize the output to a probability distribution over predicted output classes\n",
    "            train_output = F.softmax(pred, dim =1)\n",
    "            y_preds = np.argmax(train_output.cpu().detach().numpy(), axis=-1)\n",
    "            y_true = labels.cpu().numpy().flatten()\n",
    "            train_preds = np.concatenate((np.array(train_preds, int), np.array(y_preds, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "        \n",
    "        train_acc.append(100 * (correct_train / total_train).cpu())\n",
    "        cur_loss = np.mean(train_losses)\n",
    "        loss_train.append(cur_loss)\n",
    "        # Get the time this epoch used\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5.4f} s/epoch | train loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))\n",
    "        \n",
    "    eval_table = evaluation(train_preds, train_gt)\n",
    "\n",
    "    print('Train Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "    print(' Train Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "    print(' Train Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "    print(' Train F1-score:{:.5f}'.format(eval_table[3]), end='')\n",
    "\n",
    "#%% Define teacher testing loop\n",
    "#def testing_teacher(testloader,optimizer,criterion,model):\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_test=0\n",
    "    correct_test=0\n",
    "    # Sets network to eval mod\n",
    "    model.eval() \n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):\n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                inputs ,labels = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                labels = labels.to(torch.float32)\n",
    "\n",
    "                test_output = model(inputs)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(test_output, labels.long())\n",
    "                # Backpropagaton\n",
    "                #loss.backward()\n",
    "                # Update parameter\n",
    "                optimizer.step() \n",
    "                test_losses.append(loss.item())        \n",
    "                _, predicted = torch.max(test_output.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum()\n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)                \n",
    "\n",
    "                y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_true = labels.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))            \n",
    "            \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            test_acc.append(100 * (correct_test / total_test).cpu())\n",
    "#         eval_table = evaluation(test_preds,test_gt)\n",
    "#         print('\\nVal Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_teacher(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_teacher.append(eval_table[0])\n",
    "#         Recall_teacher.append(eval_table[1])\n",
    "#         Precision_teacher.append(eval_table[2])\n",
    "#         F1score_teacher.append(eval_table[3])\n",
    "    #Plot train_loss\n",
    "        teacher_train_preds.extend(test_preds)\n",
    "        teacher_train_gt.extend(test_gt)\n",
    "\n",
    "    show_loss_teacher(loss_train, loss_test)\n",
    "    show_Acc_teacher(train_acc, test_acc)\n",
    "    #return Accuracy_teacher, Recall_teacher, Precision_teacher, F1score_teacher\n",
    "    return teacher_train_preds, teacher_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7945a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define student training loop\n",
    "def training_student(trainloader,optimizer,criterion,model,testloader):\n",
    "    loss_train =[]\n",
    "    train_acc = []\n",
    "    total_train=0\n",
    "    correct_train=0\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_gt = []\n",
    "        # Set start time to count the calaulated time of each epoch\n",
    "        start_time = time.time()\n",
    "        # Iterate the trainloader \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            # Send data to GPU\n",
    "            inputs,labels = x.to(device), y.to(device)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass (compute output)\n",
    "            pred = model(inputs) \n",
    "            # Compute loss\n",
    "            loss = criterion(pred, labels.long())\n",
    "            # Backpropagaton\n",
    "            loss.backward() \n",
    "            # Update parameter\n",
    "            optimizer.step() \n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum()\n",
    "            # Use softmax to normalize the output to a probability distribution over predicted output classes\n",
    "            train_output = F.softmax(pred, dim =1)\n",
    "            y_preds = np.argmax(train_output.cpu().detach().numpy(), axis=-1)\n",
    "            y_true = labels.cpu().numpy().flatten()\n",
    "            train_preds = np.concatenate((np.array(train_preds, int), np.array(y_preds, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "        \n",
    "        cur_loss = np.mean(train_losses)\n",
    "        loss_train.append(cur_loss)\n",
    "        train_acc.append(100 * (correct_train / total_train).cpu())\n",
    "        # Get the time this epoch used\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5.4f} s/epoch | train loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))\n",
    "    \n",
    "    eval_table = evaluation(train_preds, train_gt)\n",
    "    print('Train Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "    print(' Train Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "    print(' Train Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "    print(' Train F1-score:{:.5f}'.format(eval_table[3]), end='')\n",
    "\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_test=0\n",
    "    correct_test=0\n",
    "    # Sets network to eval mod\n",
    "    model.eval()\n",
    "    #start_time = time.time()\n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):\n",
    "            for i, (x, y) in enumerate(testloader):            \n",
    "                inputs ,labels = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                labels = labels.to(torch.float32)\n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer.zero_grad()                \n",
    "                # Compute loss                \n",
    "\n",
    "                test_output = model(inputs)\n",
    "                \n",
    "                loss = criterion(test_output, labels.long())\n",
    "                # Backpropagaton\n",
    "                #loss.backward()\n",
    "                # Update parameter\n",
    "                optimizer.step()\n",
    "                test_losses.append(loss.item())                \n",
    "                \n",
    "                _, predicted = torch.max(test_output.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum()\n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)\n",
    "\n",
    "                y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_true = labels.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))\n",
    "                \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            test_acc.append(100 * (correct_test / total_test).cpu())\n",
    "            elapsed = time.time() - start_time\n",
    "            #print('| epoch {:3d} | {:5.4f} s/epoch | test loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))    \n",
    "#         eval_table = evaluation(test_preds,test_gt)\n",
    "#         print('\\nTest Val Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_student(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_student.append(eval_table[0])\n",
    "#         Recall_student.append(eval_table[1])\n",
    "#         Precision_student.append(eval_table[2])\n",
    "#         F1score_student.append(eval_table[3])\n",
    "        \n",
    "        student_train_preds.extend(test_preds)\n",
    "        student_train_gt.extend(test_gt)\n",
    "    #Plot train_loss\n",
    "    show_loss_student(loss_train, loss_test)\n",
    "    show_Acc_student(train_acc, test_acc)\n",
    "    \n",
    "    #return Accuracy_student, Recall_student, Precision_student, F1score_student\n",
    "    return student_train_preds, student_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94fcb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_teacher_student(trainloader,optimizer,criterion,model_teacher,model_student,alpha,testloader,Layer,lis):\n",
    "    loss_train =[]\n",
    "    train_acc = []\n",
    "    total_train=0\n",
    "    correct_train=0\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Freeze the Teacher model\n",
    "    model_teacher.eval()\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_losses = []\n",
    "        train_preds_student = []\n",
    "        train_preds_teacher = []\n",
    "        train_gt = []\n",
    "        # Set start time to count the calaulated time of each epoch\n",
    "        start_time = time.time()\n",
    "        # Iterate the trainloader \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            \n",
    "            # Send data to GPU\n",
    "            inputs,labels = x.to(device), y.to(device)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (compute output)\n",
    "            output_student = model_student(inputs)\n",
    "            output_teacher = model_teacher(inputs)\n",
    "\n",
    "            # Use softmax to normalize the output to a probability distribution over predicted output classes\n",
    "            output_student_soft = F.softmax(output_student, dim =1)\n",
    "            y_preds_student = np.argmax(output_student_soft.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "            output_teacher_soft = F.softmax(output_teacher, dim =1)\n",
    "            y_preds_teacher = np.argmax(output_teacher_soft.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn_kd(output_student, labels, output_teacher, 20, alpha)\n",
    "            # Backpropagaton\n",
    "            loss.backward() \n",
    "            # Update parameter\n",
    "            optimizer.step() \n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            total_hit += torch.sum(torch.argmax(output_student, dim=1) == labels).item()\n",
    "            total_num += len(inputs)\n",
    "            y_true = labels.cpu().numpy().flatten()\n",
    "            train_preds_student = np.concatenate((np.array(train_preds_student, int), np.array(y_preds_student, int)))\n",
    "            train_preds_teacher = np.concatenate((np.array(train_preds_teacher, int), np.array(y_preds_teacher, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "        \n",
    "        cur_loss = np.mean(train_losses)\n",
    "        loss_train.append(cur_loss)\n",
    "        train_acc.append(total_hit/total_num)\n",
    "        # Get the time this epoch used\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5.4f} s/epoch | train loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))\n",
    "    \n",
    "    eval_table = evaluation(train_preds_student, train_gt)\n",
    "    print('Train Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "    print(' Train Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "    print(' Train Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "    print(' Train F1-score:{:.5f}'.format(eval_table[3]), end='')\n",
    "    \n",
    "\n",
    "#%% Define Teacher-Student testing loop\n",
    "#def testing_teacher_student(testloader,optimizer,criterion,model_teacher,model_student):\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Sets network to eval mod\n",
    "    model_teacher.eval()\n",
    "    model_student.eval() \n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):            \n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                inputs ,targets = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                targets = targets.to(torch.float32)\n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                test_output = model_student(inputs)\n",
    "                test_teacher = model_teacher(inputs)\n",
    "                # Compute loss\n",
    "                #loss = loss_fn_kd(test_output, labels, output_teacher, 20, alpha)                \n",
    "                loss = loss_fn_kd(test_output, targets, test_teacher, 20, alpha)                \n",
    "                # Backpropagaton\n",
    "                #loss.backward() \n",
    "                # Update parameter\n",
    "                #optimizer.step() \n",
    "                test_losses.append(loss.item())     \n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)                \n",
    "                total_hit += torch.sum(torch.argmax(test_output, dim=1) == targets).item()\n",
    "                total_num += len(inputs)\n",
    "                \n",
    "                y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_true = targets.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))            \n",
    "            \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            #eval_table = evaluation(test_preds,test_gt)\n",
    "            test_acc.append(total_hit/total_num)\n",
    "#         print('\\nVal Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_teacher_student(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_teacher_student.append(eval_table[0])\n",
    "#         Recall_teacher_student.append(eval_table[1])\n",
    "#         Precision_teacher_student.append(eval_table[2])\n",
    "#         F1score_teacher_student.append(eval_table[3])      \n",
    "        teacher_student_train_preds.extend(test_preds)\n",
    "        teacher_student_train_gt.extend(test_gt)\n",
    "    show_loss_teacher_student(loss_train, loss_test)\n",
    "    show_Acc_teacher_student(train_acc,test_acc)\n",
    "    #return Accuracy_teacher_student, Recall_teacher_student, Precision_teacher_student, F1score_teacher_student\n",
    "    if (Layer==False) and (lis == 15):\n",
    "        save(config['epochs'], model_teacher_student, optimizer, loss,0) \n",
    "        #save(config['epochs'], model_teacher_student, optimizer, loss,1)  \n",
    "    return teacher_student_train_preds, teacher_student_train_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87168443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_distillation_student(trainloader,optimizer,criterion,model_teacher,model_student,alpha,testloader,Layer,lis):\n",
    "    loss_train =[]\n",
    "    train_acc = []\n",
    "    total_train=0\n",
    "    correct_train=0\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Freeze the Teacher model\n",
    "    model_teacher.eval()\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_losses = []\n",
    "        train_preds_student = []\n",
    "        train_preds_teacher = []\n",
    "        train_gt = []\n",
    "        # Set start time to count the calaulated time of each epoch\n",
    "        start_time = time.time()\n",
    "        # Iterate the trainloader \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            \n",
    "            # Send data to GPU\n",
    "            inputs,labels = x.to(device), y.to(device)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (compute output)\n",
    "            output_student = model_student(inputs)\n",
    "            output_teacher = model_teacher(inputs)\n",
    "\n",
    "            # Use softmax to normalize the output to a probability distribution over predicted output classes\n",
    "            output_student_soft = F.softmax(output_student, dim =1)\n",
    "            y_preds_student = np.argmax(output_student_soft.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "            output_teacher_soft = F.softmax(output_teacher, dim =1)\n",
    "            y_preds_teacher = np.argmax(output_teacher_soft.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn_kd(output_student, labels, output_teacher, 20, alpha)\n",
    "            # Backpropagaton\n",
    "            loss.backward() \n",
    "            # Update parameter\n",
    "            optimizer.step() \n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            total_hit += torch.sum(torch.argmax(output_student, dim=1) == labels).item()\n",
    "            total_num += len(inputs)\n",
    "            y_true = labels.cpu().numpy().flatten()\n",
    "            train_preds_student = np.concatenate((np.array(train_preds_student, int), np.array(y_preds_student, int)))\n",
    "            train_preds_teacher = np.concatenate((np.array(train_preds_teacher, int), np.array(y_preds_teacher, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "        \n",
    "        cur_loss = np.mean(train_losses)\n",
    "        loss_train.append(cur_loss)\n",
    "        train_acc.append(total_hit/total_num)\n",
    "        # Get the time this epoch used\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5.4f} s/epoch | train loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))\n",
    "    \n",
    "    eval_table = evaluation(train_preds_student, train_gt)\n",
    "    print('Train Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "    print(' Train Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "    print(' Train Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "    print(' Train F1-score:{:.5f}'.format(eval_table[3]), end='')\n",
    "    \n",
    "\n",
    "#%% Define Teacher-Student testing loop\n",
    "#def testing_teacher_student(testloader,optimizer,criterion,model_teacher,model_student):\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Sets network to eval mod\n",
    "    model_teacher.eval()\n",
    "    model_student.eval() \n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):            \n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                inputs ,targets = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                targets = targets.to(torch.float32)\n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                test_output = model_student(inputs)\n",
    "                test_teacher = model_teacher(inputs)\n",
    "                # Compute loss\n",
    "                #loss = loss_fn_kd(test_output, labels, output_teacher, 20, alpha)                \n",
    "                loss = loss_fn_kd(test_output, targets, test_teacher, 20, alpha)                \n",
    "                # Backpropagaton\n",
    "                #loss.backward() \n",
    "                # Update parameter\n",
    "                #optimizer.step() \n",
    "                test_losses.append(loss.item())     \n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)                \n",
    "                total_hit += torch.sum(torch.argmax(test_output, dim=1) == targets).item()\n",
    "                total_num += len(inputs)\n",
    "                \n",
    "                y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_true = targets.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))            \n",
    "            \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            #eval_table = evaluation(test_preds,test_gt)\n",
    "            test_acc.append(total_hit/total_num)\n",
    "#         print('\\nVal Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_teacher_student(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_teacher_student.append(eval_table[0])\n",
    "#         Recall_teacher_student.append(eval_table[1])\n",
    "#         Precision_teacher_student.append(eval_table[2])\n",
    "#         F1score_teacher_student.append(eval_table[3])      \n",
    "        Distillation_student_train_preds.extend(test_preds)\n",
    "        Distillation_student_train_gt.extend(test_gt)\n",
    "    show_loss_teacher_student(loss_train, loss_test)\n",
    "    show_Acc_teacher_student(train_acc,test_acc)\n",
    "    #return Accuracy_teacher_student, Recall_teacher_student, Precision_teacher_student, F1score_teacher_student\n",
    "#     if (Layer==True) and (lis == 15):\n",
    "#         save(config['epochs'], model_Distillation_student, optimizer, loss,2)\n",
    "    return Distillation_student_train_preds, Distillation_student_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abb61085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(epoch, student, optimizer, loss,num):\n",
    "#     trial_id = self.config['trial_id']\n",
    "#     if name is None:\n",
    "        if num == 0:\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': student.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #'loss_state_dict': loss.state_dice()\n",
    "            'loss_state_dict': loss.item()\n",
    "            }, PATH)\n",
    "        elif num == 1:\n",
    "            P1=\"D:/conda/111_1117/Model/FullAll_3LayerModel_DTA_Mobile.pt\"\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': student.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #'loss_state_dict': loss.state_dice()\n",
    "            'loss_state_dict': loss.item()\n",
    "            }, P1)\n",
    "        elif num == 2:\n",
    "            P2=\"D:/conda/111_1117/Model/FullAll_3LayerModel_DStudent_CNNSmall.pt\"\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': student.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #'loss_state_dict': loss.state_dice()\n",
    "            'loss_state_dict': loss.item()\n",
    "            }, P2)\n",
    "        elif num == 3:\n",
    "            P3=\"D:/conda/111_1117/Model/SisFull_Original_MobileNet.pt\"\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': student.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #'loss_state_dict': loss.state_dice()\n",
    "            'loss_state_dict': loss.item()\n",
    "            }, P3)\n",
    "            \n",
    "#     else:\n",
    "#         torch.save({\n",
    "#         'model_state_dict': self.student.state_dict(),\n",
    "#         'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "#         'epoch': epoch,\n",
    "#         }, name)\n",
    "\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0d8f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint_path):\n",
    "        \"\"\"\n",
    "        Loads weights from checkpoint\n",
    "        :param model: a pytorch nn student\n",
    "        :param str checkpoint_path: address/path of a file\n",
    "        :return: pytorch nn student with weights loaded from checkpoint\n",
    "        \"\"\"\n",
    "        model_ckp = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(model_ckp['model_state_dict'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e939dc90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% Evaluation\n",
    "def evaluation(pred,target):\n",
    "    acu = accuracy_score(pred, target)\n",
    "    rec = recall_score(pred, target)\n",
    "    pre = precision_score(pred, target)\n",
    "    f1 = f1_score(pred, target)  \n",
    "    return np.array([acu, rec, pre, f1])\n",
    "\n",
    "# #%% Train\n",
    "# Accuracy_teacher = []\n",
    "# Recall_teacher = []\n",
    "# Precision_teacher = []\n",
    "# F1score_teacher = []\n",
    "\n",
    "# Accuracy_student = []\n",
    "# Recall_student = []\n",
    "# Precision_student = []\n",
    "# F1score_student = []\n",
    "\n",
    "\n",
    "# Accuracy_teacher_student = []\n",
    "# Recall_teacher_student = []\n",
    "# Precision_teacher_student = []\n",
    "# F1score_teacher_student = []\n",
    "\n",
    "\n",
    "# teacher_train_preds = []\n",
    "# teacher_train_gt = []\n",
    "\n",
    "\n",
    "# student_train_preds = []\n",
    "# student_train_gt = []\n",
    "\n",
    "# #for i in range(Average_times):\n",
    "# lis = [1,2,3,4,5,7,8,9,10,11,12,13,14,15]\n",
    "# for sub in lis:\n",
    "#     print(\"\\nSubject\",sub)\n",
    "#     test = (sliding_waist_subject == sub)\n",
    "#     train = ~test\n",
    "\n",
    "#     norm_sliding_waist_data = []\n",
    "#     norm_sliding_waist_test = []\n",
    "#     min_norm_value = []\n",
    "#     min_norm_index = []\n",
    "#     max_norm_value = []\n",
    "#     max_norm_index = []\n",
    "#     hori_sliding_waist_data = []\n",
    "#     hori_sliding_waist_test = []\n",
    "#     sliding_waist_data_hori = sliding_waist_data\n",
    "#     sliding_waist_data_hori[:,:,0] = sliding_waist_data_hori[:,:,0]*0\n",
    "#     min_hori_value = []\n",
    "#     min_hori_index = []\n",
    "#     max_hori_value = []\n",
    "#     max_hori_index = []\n",
    "    \n",
    "    \n",
    "#     X_train = sliding_waist_data[train]\n",
    "#     Y_train_gd = sliding_waist_label[train]\n",
    "#     X_test = sliding_waist_data[test]\n",
    "#     Y_test_gd = sliding_waist_label[test]\n",
    "\n",
    "#     X_train_hori = sliding_waist_data_hori[train]\n",
    "#     X_test_hori = sliding_waist_data_hori[test]\n",
    "\n",
    "\n",
    "#     for i in range(len(X_train)):\n",
    "#         c1 = np.linalg.norm(X_train[i], axis=1)\n",
    "#         norm_sliding_waist_data.append(c1)\n",
    "\n",
    "#     for i in range(len(X_test)):\n",
    "#         c2 = np.linalg.norm(X_test[i], axis=1)\n",
    "#         norm_sliding_waist_test.append(c2)\n",
    "\n",
    "#     for i in range(len(X_train_hori)):\n",
    "#         c1 = np.linalg.norm(X_train_hori[i], axis=1)\n",
    "#         hori_sliding_waist_data.append(c1)\n",
    "\n",
    "#     for i in range(len(X_test_hori)):\n",
    "#         c2 = np.linalg.norm(X_test_hori[i], axis=1)\n",
    "#         hori_sliding_waist_test.append(c2)\n",
    "\n",
    "    \n",
    "#     for i in range(len(Y_train_gd)):\n",
    "#         if Y_train_gd[i] ==1:\n",
    "#             d1 = np.min(norm_sliding_waist_data[i])\n",
    "#             min_norm_value.append(d1)\n",
    "#             d2 = np.argmin(norm_sliding_waist_data[i])\n",
    "#             min_norm_index.append(d2)\n",
    "\n",
    "#             d3 = np.min(hori_sliding_waist_data[i])\n",
    "#             min_hori_value.append(d3)\n",
    "#             d4 = np.argmin(hori_sliding_waist_data[i])\n",
    "#             min_hori_index.append(d4)\n",
    "            \n",
    "#         else:\n",
    "#             e1 = np.max(norm_sliding_waist_data[i])\n",
    "#             max_norm_value.append(e1)\n",
    "#             e2 = np.argmax(norm_sliding_waist_data[i])\n",
    "#             max_norm_index.append(e2)\n",
    "\n",
    "#             e3 = np.max(hori_sliding_waist_data[i])\n",
    "#             max_hori_value.append(e3)\n",
    "#             e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "#             max_hori_index.append(e4)\n",
    "\n",
    "            \n",
    "\n",
    "#     MIN_train = np.min(min_norm_value)\n",
    "#     MAX_train = np.max(max_norm_value)\n",
    "\n",
    "#     MIN_train_hori = np.min(min_hori_value)\n",
    "#     MAX_train_hori = np.max(max_hori_value)\n",
    "\n",
    "#     unidentified_data = []\n",
    "#     unidentified_label = []\n",
    "\n",
    "#     for i in range(len(norm_sliding_waist_test)):\n",
    "#        if max(norm_sliding_waist_test[i]) > MIN_train and max(norm_sliding_waist_test[i]) < MAX_train\\\n",
    "#           and max(hori_sliding_waist_test[i]) > MIN_train_hori and max(hori_sliding_waist_test[i]) < MAX_train_hori:\n",
    "#             unidentified_data.append(X_test[i])\n",
    "#             unidentified_label.append(Y_test_gd[i])\n",
    "\n",
    "#     unidentified_data = np.array(unidentified_data)\n",
    "#     unidentified_label = np.array(unidentified_label)\n",
    "\n",
    "#     #breakpoint() # insert breakpoint   \n",
    "    \n",
    "#     # Initializes the train and validation dataset in Torch format\n",
    "#     x_train_tensor = torch.from_numpy(X_train).to(device) \n",
    "#     x_train_tensor = x_train_tensor.reshape([-1,1,14,3]) \n",
    "#     x_test_tensor = torch.from_numpy(unidentified_data).to(device)\n",
    "#     x_test_tensor = x_test_tensor.reshape([-1,1,14,3])\n",
    "    \n",
    "#     y_train_gd_tensor =  torch.from_numpy(Y_train_gd).to(device)\n",
    "#     y_test_gd_tensor =  torch.from_numpy(unidentified_label).to(device)\n",
    "    \n",
    "#     # Dataset wrapping tensors.Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "#     deal_dataset = TensorDataset(x_train_tensor, y_train_gd_tensor) \n",
    "#     test_dataset = TensorDataset(x_test_tensor,y_test_gd_tensor)\n",
    "    \n",
    "#     config['window_size'] = X_train.shape[1]\n",
    "#     config['nb_channels'] = X_train.shape[2]\n",
    "#     config['nb_classes'] = 2\n",
    "    \n",
    "#     # Sends network to the GPU and sets it to training mode\n",
    "#     ResNet = Net_Teacher(block, [2, 2, 2, 2], 1, 2)\n",
    "#     model_teacher = ResNet.to(device) \n",
    "#     model_teacher.train()\n",
    "\n",
    "#     model_student = Net_Student(config).to(device) \n",
    "#     model_student.train()\n",
    "    \n",
    "#     # DataLoader represents a Python iterable over a dataset\n",
    "#     trainloader = DataLoader(deal_dataset, batch_size=config['batch_size'],shuffle=True) \n",
    "#     testloader = DataLoader(test_dataset, batch_size=config['batch_size'],shuffle=False)\n",
    "#     # Initialize the optimizer and loss\n",
    "#     optimizer_teacher = torch.optim.Adam(model_teacher.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "#     optimizer_student = torch.optim.Adam(model_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "#     criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "#     # Start training and testing Teacher and Student Model\n",
    "#     training_teacher(trainloader,optimizer_teacher,criterion,model_teacher,testloader,lis)\n",
    "#     #testing_teacher(testloader,optimizer_teacher,criterion,model_teacher)\n",
    "  \n",
    "#     #training_student(trainloader,optimizer_student,criterion,model_student,testloader)    \n",
    "#     #testing_student(testloader,optimizer_student,criterion,model_student)\n",
    "\n",
    "# #print(teacher_train_preds)\n",
    "# #print(teacher_train_gt)\n",
    "# Teacher_eval_table = evaluation(teacher_train_preds, teacher_train_gt)\n",
    "\n",
    "# #Student_eval_table = evaluation(student_train_preds, student_train_gt)\n",
    "\n",
    "# show_CM_teacher(teacher_train_preds, teacher_train_gt)\n",
    "# #show_CM_student(student_train_preds, student_train_gt)\n",
    "\n",
    "# print(\"Teacher(RestNet18)_Acc:\",Teacher_eval_table[0])\n",
    "# print(\"Teacher(RestNet18)_Rec:\",Teacher_eval_table[1])\n",
    "# print(\"Teacher(RestNet18)_Pre:\",Teacher_eval_table[2])\n",
    "# print(\"Teacher(RestNet18)_F1:\",Teacher_eval_table[3])\n",
    "\n",
    "# # print(\"Original_Student(CNN_Small)_Acc:\",Student_eval_table[0])\n",
    "# # print(\"Original_Student(CNN_Small)_Rec:\",Student_eval_table[1])\n",
    "# # print(\"Original_Student(CNN_Small)_Pre:\",Student_eval_table[2])\n",
    "# # print(\"Original_Student(CNN_Small)_F1:\",Student_eval_table[3]) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a105dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Layer 1 Training teacher\n",
    "# teacher_student_train_preds = []\n",
    "# teacher_student_train_gt = []\n",
    "\n",
    "\n",
    "# for sub in lis:\n",
    "#     print(\"\\nSubject\",sub)\n",
    "#     test = (sliding_waist_subject == sub)\n",
    "#     train = ~test\n",
    "\n",
    "#     norm_sliding_waist_data = []\n",
    "#     norm_sliding_waist_test = []\n",
    "#     min_norm_value = []\n",
    "#     min_norm_index = []\n",
    "#     max_norm_value = []\n",
    "#     max_norm_index = []\n",
    "#     hori_sliding_waist_data = []\n",
    "#     hori_sliding_waist_test = []\n",
    "#     sliding_waist_data_hori = sliding_waist_data\n",
    "#     sliding_waist_data_hori[:,:,0] = sliding_waist_data_hori[:,:,0]*0\n",
    "#     min_hori_value = []\n",
    "#     min_hori_index = []\n",
    "#     max_hori_value = []\n",
    "#     max_hori_index = []\n",
    "    \n",
    "#     X_train = sliding_waist_data[train]\n",
    "#     Y_train_gd = sliding_waist_label[train]\n",
    "#     X_test = sliding_waist_data[test]\n",
    "#     Y_test_gd = sliding_waist_label[test]\n",
    "\n",
    "#     X_train_hori = sliding_waist_data_hori[train]\n",
    "#     X_test_hori = sliding_waist_data_hori[test]\n",
    "\n",
    "\n",
    "#     for i in range(len(X_train)):\n",
    "#         c1 = np.linalg.norm(X_train[i], axis=1)\n",
    "#         norm_sliding_waist_data.append(c1)\n",
    "\n",
    "#     for i in range(len(X_test)):\n",
    "#         c2 = np.linalg.norm(X_test[i], axis=1)\n",
    "#         norm_sliding_waist_test.append(c2)\n",
    "\n",
    "#     for i in range(len(X_train_hori)):\n",
    "#         c1 = np.linalg.norm(X_train_hori[i], axis=1)\n",
    "#         hori_sliding_waist_data.append(c1)\n",
    "\n",
    "#     for i in range(len(X_test_hori)):\n",
    "#         c2 = np.linalg.norm(X_test_hori[i], axis=1)\n",
    "#         hori_sliding_waist_test.append(c2)\n",
    "\n",
    "    \n",
    "#     for i in range(len(Y_train_gd)):\n",
    "#         if Y_train_gd[i] ==1:\n",
    "#             d1 = np.min(norm_sliding_waist_data[i])\n",
    "#             min_norm_value.append(d1)\n",
    "#             d2 = np.argmin(norm_sliding_waist_data[i])\n",
    "#             min_norm_index.append(d2)\n",
    "\n",
    "#             d3 = np.min(hori_sliding_waist_data[i])\n",
    "#             min_hori_value.append(d3)\n",
    "#             d4 = np.argmin(hori_sliding_waist_data[i])\n",
    "#             min_hori_index.append(d4)\n",
    "            \n",
    "#         else:\n",
    "#             e1 = np.max(norm_sliding_waist_data[i])\n",
    "#             max_norm_value.append(e1)\n",
    "#             e2 = np.argmax(norm_sliding_waist_data[i])\n",
    "#             max_norm_index.append(e2)\n",
    "\n",
    "#             e3 = np.max(hori_sliding_waist_data[i])\n",
    "#             max_hori_value.append(e3)\n",
    "#             e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "#             max_hori_index.append(e4)\n",
    "        \n",
    "\n",
    "#     MIN_train = np.min(min_norm_value)\n",
    "#     MAX_train = np.max(max_norm_value)\n",
    "\n",
    "#     MIN_train_hori = np.min(min_hori_value)\n",
    "#     MAX_train_hori = np.max(max_hori_value)\n",
    "\n",
    "#     unidentified_data = []\n",
    "#     unidentified_label = []\n",
    "\n",
    "#     for i in range(len(norm_sliding_waist_test)):\n",
    "#        if max(norm_sliding_waist_test[i]) > MIN_train and max(norm_sliding_waist_test[i]) < MAX_train\\\n",
    "#           and max(hori_sliding_waist_test[i]) > MIN_train_hori and max(hori_sliding_waist_test[i]) < MAX_train_hori:\n",
    "#             unidentified_data.append(X_test[i])\n",
    "#             unidentified_label.append(Y_test_gd[i])\n",
    "\n",
    "#     unidentified_data = np.array(unidentified_data)\n",
    "#     unidentified_label = np.array(unidentified_label)\n",
    "\n",
    "#     #breakpoint() # insert breakpoint   \n",
    "    \n",
    "#     # Initializes the train and validation dataset in Torch format\n",
    "#     x_train_tensor = torch.from_numpy(X_train).to(device) \n",
    "#     x_train_tensor = x_train_tensor.reshape([-1,1,14,3]) \n",
    "#     x_test_tensor = torch.from_numpy(unidentified_data).to(device)\n",
    "#     x_test_tensor = x_test_tensor.reshape([-1,1,14,3])\n",
    "    \n",
    "#     y_train_gd_tensor =  torch.from_numpy(Y_train_gd).to(device)\n",
    "#     y_test_gd_tensor =  torch.from_numpy(unidentified_label).to(device)\n",
    "    \n",
    "#     # Dataset wrapping tensors.Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "#     deal_dataset = TensorDataset(x_train_tensor, y_train_gd_tensor) \n",
    "#     test_dataset = TensorDataset(x_test_tensor,y_test_gd_tensor)\n",
    "    \n",
    "#     config['window_size'] = X_train.shape[1]\n",
    "#     config['nb_channels'] = X_train.shape[2]\n",
    "#     config['nb_classes'] = 2\n",
    "    \n",
    "#     # Sends network to the GPU and sets it to training mode\n",
    "#     model_teacher_student = MobileNetV3_Small_KD().to(device)\n",
    "#     #model_teacher_student = Net_Teacher_Student(config).to(device) \n",
    "#     model_teacher_student.train() \n",
    "    \n",
    "#     # DataLoader represents a Python iterable over a dataset\n",
    "#     trainloader = DataLoader(deal_dataset, batch_size=config['batch_size'],shuffle=True) \n",
    "#     testloader = DataLoader(test_dataset, batch_size=config['batch_size'],shuffle=False)\n",
    "#     # Initialize the optimizer and loss\n",
    "#     optimizer_teacher_student = torch.optim.Adam(model_teacher_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "#     criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "#     # Establishing Teacher-Student Model\n",
    "#     alpha = 0.5\n",
    "#     Layer = False\n",
    "#     training_teacher_student(trainloader,optimizer_teacher_student,criterion,model_teacher,model_teacher_student,alpha,testloader,Layer,sub)\n",
    "#     #testing_teacher_student(testloader,optimizer_teacher_student,criterion,model_teacher,model_teacher_student)\n",
    "\n",
    "\n",
    "# show_CM_teacher_student(teacher_student_train_preds, teacher_student_train_gt)\n",
    "# teacher_Student_eval_table = evaluation(teacher_student_train_preds, teacher_student_train_gt)    \n",
    "\n",
    "# print(\"Distillation_teacher(Mobilnet)_Acc:\",teacher_Student_eval_table[0])\n",
    "# print(\"Distillation_teacher(Mobilnet)_Rec:\",teacher_Student_eval_table[1])\n",
    "# print(\"Distillation_teacher(Mobilnet)_Pre:\",teacher_Student_eval_table[2])\n",
    "# print(\"Distillation_teacher(Mobilnet)_F1:\",teacher_Student_eval_table[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3909e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Layer 2 Training Student\n",
    "# # Distillation_teacher_model = None\n",
    "# Distillation_teacher_model = load_checkpoint(model_teacher_student, PATH)\n",
    "\n",
    "# Distillation_student_train_preds = []\n",
    "# Distillation_student_train_gt = []\n",
    "\n",
    "# for sub in lis:\n",
    "#     print(\"\\nSubject\",sub)\n",
    "#     test = (sliding_waist_subject == sub)\n",
    "#     train = ~test\n",
    "\n",
    "#     norm_sliding_waist_data = []\n",
    "#     norm_sliding_waist_test = []\n",
    "#     min_norm_value = []\n",
    "#     min_norm_index = []\n",
    "#     max_norm_value = []\n",
    "#     max_norm_index = []\n",
    "#     hori_sliding_waist_data = []\n",
    "#     hori_sliding_waist_test = []\n",
    "#     sliding_waist_data_hori = sliding_waist_data\n",
    "#     sliding_waist_data_hori[:,:,0] = sliding_waist_data_hori[:,:,0]*0\n",
    "#     min_hori_value = []\n",
    "#     min_hori_index = []\n",
    "#     max_hori_value = []\n",
    "#     max_hori_index = []\n",
    "    \n",
    "#     X_train = sliding_waist_data[train]\n",
    "#     Y_train_gd = sliding_waist_label[train]\n",
    "#     X_test = sliding_waist_data[test]\n",
    "#     Y_test_gd = sliding_waist_label[test]\n",
    "\n",
    "#     X_train_hori = sliding_waist_data_hori[train]\n",
    "#     X_test_hori = sliding_waist_data_hori[test]\n",
    "\n",
    "\n",
    "#     for i in range(len(X_train)):\n",
    "#         c1 = np.linalg.norm(X_train[i], axis=1)\n",
    "#         norm_sliding_waist_data.append(c1)\n",
    "\n",
    "#     for i in range(len(X_test)):\n",
    "#         c2 = np.linalg.norm(X_test[i], axis=1)\n",
    "#         norm_sliding_waist_test.append(c2)\n",
    "\n",
    "#     for i in range(len(X_train_hori)):\n",
    "#         c1 = np.linalg.norm(X_train_hori[i], axis=1)\n",
    "#         hori_sliding_waist_data.append(c1)\n",
    "\n",
    "#     for i in range(len(X_test_hori)):\n",
    "#         c2 = np.linalg.norm(X_test_hori[i], axis=1)\n",
    "#         hori_sliding_waist_test.append(c2)\n",
    "\n",
    "    \n",
    "#     for i in range(len(Y_train_gd)):\n",
    "#         if Y_train_gd[i] ==1:\n",
    "#             d1 = np.min(norm_sliding_waist_data[i])\n",
    "#             min_norm_value.append(d1)\n",
    "#             d2 = np.argmin(norm_sliding_waist_data[i])\n",
    "#             min_norm_index.append(d2)\n",
    "\n",
    "#             d3 = np.min(hori_sliding_waist_data[i])\n",
    "#             min_hori_value.append(d3)\n",
    "#             d4 = np.argmin(hori_sliding_waist_data[i])\n",
    "#             min_hori_index.append(d4)\n",
    "            \n",
    "#         else:\n",
    "#             e1 = np.max(norm_sliding_waist_data[i])\n",
    "#             max_norm_value.append(e1)\n",
    "#             e2 = np.argmax(norm_sliding_waist_data[i])\n",
    "#             max_norm_index.append(e2)\n",
    "\n",
    "#             e3 = np.max(hori_sliding_waist_data[i])\n",
    "#             max_hori_value.append(e3)\n",
    "#             e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "#             max_hori_index.append(e4)\n",
    "        \n",
    "\n",
    "#     MIN_train = np.min(min_norm_value)\n",
    "#     MAX_train = np.max(max_norm_value)\n",
    "\n",
    "#     MIN_train_hori = np.min(min_hori_value)\n",
    "#     MAX_train_hori = np.max(max_hori_value)\n",
    "\n",
    "#     unidentified_data = []\n",
    "#     unidentified_label = []\n",
    "\n",
    "#     for i in range(len(norm_sliding_waist_test)):\n",
    "#        if max(norm_sliding_waist_test[i]) > MIN_train and max(norm_sliding_waist_test[i]) < MAX_train\\\n",
    "#           and max(hori_sliding_waist_test[i]) > MIN_train_hori and max(hori_sliding_waist_test[i]) < MAX_train_hori:\n",
    "#             unidentified_data.append(X_test[i])\n",
    "#             unidentified_label.append(Y_test_gd[i])\n",
    "\n",
    "#     unidentified_data = np.array(unidentified_data)\n",
    "#     unidentified_label = np.array(unidentified_label)\n",
    "\n",
    "#     #breakpoint() # insert breakpoint   \n",
    "    \n",
    "#     # Initializes the train and validation dataset in Torch format\n",
    "#     x_train_tensor = torch.from_numpy(X_train).to(device) \n",
    "#     x_train_tensor = x_train_tensor.reshape([-1,1,14,3]) \n",
    "#     x_test_tensor = torch.from_numpy(unidentified_data).to(device)\n",
    "#     x_test_tensor = x_test_tensor.reshape([-1,1,14,3])\n",
    "    \n",
    "#     y_train_gd_tensor =  torch.from_numpy(Y_train_gd).to(device)\n",
    "#     y_test_gd_tensor =  torch.from_numpy(unidentified_label).to(device)\n",
    "    \n",
    "#     # Dataset wrapping tensors.Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "#     deal_dataset = TensorDataset(x_train_tensor, y_train_gd_tensor) \n",
    "#     test_dataset = TensorDataset(x_test_tensor,y_test_gd_tensor)\n",
    "    \n",
    "#     config['window_size'] = X_train.shape[1]\n",
    "#     config['nb_channels'] = X_train.shape[2]\n",
    "#     config['nb_classes'] = 2\n",
    "    \n",
    "#     # Sends network to the GPU and sets it to training mode\n",
    "\n",
    "#     model_Distillation_student = Net_Teacher_Student(config).to(device) \n",
    "#     model_Distillation_student.train() \n",
    "    \n",
    "#     # DataLoader represents a Python iterable over a dataset\n",
    "#     trainloader = DataLoader(deal_dataset, batch_size=config['batch_size'],shuffle=True) \n",
    "#     testloader = DataLoader(test_dataset, batch_size=config['batch_size'],shuffle=False)\n",
    "#     # Initialize the optimizer and loss\n",
    "#     optimizer_teacher_student = torch.optim.Adam(model_Distillation_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "#     criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "#     # Establishing Teacher-Student Model\n",
    "#     alpha = 0.5\n",
    "#     Layer = True\n",
    "#     training_distillation_student(trainloader,optimizer_teacher_student,criterion,Distillation_teacher_model,model_Distillation_student,alpha,testloader,Layer,sub)\n",
    "#     #testing_teacher_student(testloader,optimizer_teacher_student,criterion,model_teacher,model_teacher_student)\n",
    "\n",
    "# show_CM_teacher_student(teacher_student_train_preds, teacher_student_train_gt)\n",
    "# teacher_Student_eval_table = evaluation(teacher_student_train_preds, teacher_student_train_gt)    \n",
    "\n",
    "# print(\"Distillation_student(CNNSmall)_Acc:\",teacher_Student_eval_table[0])\n",
    "# print(\"Distillation_student(CNNSmall)_Rec:\",teacher_Student_eval_table[1])\n",
    "# print(\"Distillation_student(CNNSmall)_Pre:\",teacher_Student_eval_table[2])\n",
    "# print(\"Distillation_student(CNNSmall)_F1:\",teacher_Student_eval_table[3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a81c1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Threshold(test_output,ThresholdH,ThresholdL,Layer):\n",
    "    global Layer1\n",
    "    global Layer2\n",
    "    global Layer3\n",
    "    global LayerUP\n",
    "    test_preds=[]\n",
    "    numpyArray = np.array(test_output)\n",
    "    #print(numpyArray.shape)\n",
    "    for i, (x, y) in enumerate(test_output):\n",
    "        #print(i,(x, y))\n",
    "        if x > ThresholdH:\n",
    "            test_preds.append(0)\n",
    "            LayerUP=False\n",
    "            \n",
    "            if Layer==1:\n",
    "                Layer1+=1\n",
    "            elif Layer==2:\n",
    "                Layer2+=1\n",
    "            else:\n",
    "                Layer3+=1\n",
    "                \n",
    "        elif (x < ThresholdH) and (x > ThresholdL):\n",
    "            if Layer == 3:\n",
    "                LayerUP=False\n",
    "                test_preds.append(0)\n",
    "                Layer3+=1\n",
    "            else:\n",
    "                LayerUP=True\n",
    "        else:\n",
    "            test_preds.append(1)\n",
    "            LayerUP=False\n",
    "            \n",
    "            if Layer==1:\n",
    "                Layer1+=1\n",
    "            elif Layer==2:\n",
    "                Layer2+=1\n",
    "            else:\n",
    "                Layer3+=1\n",
    "    #print(test_preds)\n",
    "    if LayerUP==False:\n",
    "        return test_preds\n",
    "\n",
    "def testing_3LayerModel(testloader,optimizer_d_s,optimizer_t,optimizer_t_s,criterion,model_DS,model_DTA,model_T,alpha,TH,TL,lis):\n",
    "\n",
    "    global Layer1_Time\n",
    "    global Layer2_Time\n",
    "    global Layer3_Time    \n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_num, total_hit = 0, 0\n",
    "\n",
    "    # Sets network to eval mod\n",
    "    model_T.eval()\n",
    "    model_DTA.eval()\n",
    "    model_DS.eval() \n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):\n",
    "            start_time = time.time()\n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                inputs ,targets = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                targets = targets.to(torch.float32)\n",
    "                \n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer_d_s.zero_grad()\n",
    "                optimizer_t_s.zero_grad()\n",
    "                optimizer_t.zero_grad()\n",
    "                \n",
    "                \n",
    "                \n",
    "                test_output = model_DS(inputs)\n",
    "       \n",
    " \n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)                \n",
    "#                 total_hit += torch.sum(torch.argmax(test_output, dim=1) == targets).item()\n",
    "#                 total_num += len(inputs)\n",
    "                #print(test_output)\n",
    "#                 y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                Total_Time=datetime.datetime.now()\n",
    "                y_preds = Threshold(test_output.cpu().numpy(),TH,TL,1)\n",
    "    \n",
    "                if LayerUP == True:\n",
    "#                     y_preds = y_preds.pop()\n",
    "                    start = datetime.datetime.now()\n",
    "                    test_output = model_DTA(inputs)\n",
    "                    test_output = F.softmax(test_output, dim=1)\n",
    "                    y_preds = Threshold(test_output.cpu().numpy(),TH,TL,2)\n",
    "                    end = datetime.datetime.now()\n",
    "                    Layer2_Time =Layer2_Time + (end-start)\n",
    "                    if LayerUP == True:\n",
    "                        start = datetime.datetime.now()\n",
    "                        test_output = model_T(inputs)\n",
    "                        test_output = F.softmax(test_output, dim=1)\n",
    "                        y_preds = Threshold(test_output.cpu().numpy(),TH,TL,3)\n",
    "                        end = datetime.datetime.now()\n",
    "                        Layer3_Time = Layer3_Time + (end-start)\n",
    "                        \n",
    "                Total_Time_END=datetime.datetime.now()\n",
    "                Layer1_Time = Layer1_Time + (Total_Time_END-Total_Time)\n",
    "                y_true = targets.cpu().numpy().flatten()\n",
    "        \n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))            \n",
    "            \n",
    "\n",
    "        print('Layer1:{} | Layer2:{} | Layer3:{} | TH:{} | TL:{} | Lis:{} | LayerTotal:{}'.format(Layer1,Layer2,Layer3,TH,TL,lis,Layer1+Layer2+Layer3))\n",
    "        if lis==15:\n",
    "            print('Layer1_TimeTatol:{} | Layer2_Time:{} | Layer3_Time:{}'.format(Layer1_Time,Layer2_Time,Layer3_Time))\n",
    "            print(psutil.cpu_stats())\n",
    "        Distillation_student_train_preds.extend(test_preds)\n",
    "        Distillation_student_train_gt.extend(test_gt)\n",
    "        print('Distillation_student_train_preds:{} | Distillation_student_train_gt:{}'.format(len(Distillation_student_train_preds),len(Distillation_student_train_gt)))\n",
    "#     show_loss_teacher_student(loss_train, loss_test)\n",
    "#     show_Acc_teacher_student(train_acc,test_acc)\n",
    "    #return Accuracy_teacher_student, Recall_teacher_student, Precision_teacher_student, F1score_teacher_student\n",
    "    return Distillation_student_train_preds, Distillation_student_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7363ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer1:83400 | Layer2:10000 | Layer3:12800 | TH:0.8 | TL:0.2 | Lis:1 | LayerTotal:106200\n",
      "Distillation_student_train_preds:87000 | Distillation_student_train_gt:92800\n",
      "Layer1:192600 | Layer2:21600 | Layer3:12800 | TH:0.8 | TL:0.2 | Lis:2 | LayerTotal:227000\n",
      "Distillation_student_train_preds:199800 | Distillation_student_train_gt:224800\n",
      "Layer1:304600 | Layer2:31400 | Layer3:12800 | TH:0.8 | TL:0.2 | Lis:3 | LayerTotal:348800\n",
      "Distillation_student_train_preds:312800 | Distillation_student_train_gt:353600\n",
      "Layer1:359000 | Layer2:55600 | Layer3:12800 | TH:0.8 | TL:0.2 | Lis:4 | LayerTotal:427400\n",
      "Distillation_student_train_preds:369800 | Distillation_student_train_gt:417600\n",
      "Layer1:463000 | Layer2:57400 | Layer3:12800 | TH:0.8 | TL:0.2 | Lis:5 | LayerTotal:533200\n",
      "Distillation_student_train_preds:473600 | Distillation_student_train_gt:536400\n",
      "Layer1:586200 | Layer2:89200 | Layer3:25600 | TH:0.8 | TL:0.2 | Lis:7 | LayerTotal:701000\n",
      "Distillation_student_train_preds:606600 | Distillation_student_train_gt:682600\n",
      "Layer1:666600 | Layer2:89200 | Layer3:25600 | TH:0.8 | TL:0.2 | Lis:8 | LayerTotal:781400\n",
      "Distillation_student_train_preds:687000 | Distillation_student_train_gt:769800\n",
      "Layer1:755000 | Layer2:101600 | Layer3:25600 | TH:0.8 | TL:0.2 | Lis:9 | LayerTotal:882200\n",
      "Distillation_student_train_preds:780800 | Distillation_student_train_gt:875400\n",
      "Layer1:818800 | Layer2:101600 | Layer3:25600 | TH:0.8 | TL:0.2 | Lis:10 | LayerTotal:946000\n",
      "Distillation_student_train_preds:844600 | Distillation_student_train_gt:939800\n",
      "Layer1:888800 | Layer2:115600 | Layer3:25600 | TH:0.8 | TL:0.2 | Lis:11 | LayerTotal:1030000\n",
      "Distillation_student_train_preds:915800 | Distillation_student_train_gt:1018200\n",
      "Layer1:1006000 | Layer2:127000 | Layer3:25600 | TH:0.8 | TL:0.2 | Lis:12 | LayerTotal:1158600\n",
      "Distillation_student_train_preds:1039600 | Distillation_student_train_gt:1160600\n",
      "Layer1:1130000 | Layer2:127000 | Layer3:25600 | TH:0.8 | TL:0.2 | Lis:13 | LayerTotal:1282600\n",
      "Distillation_student_train_preds:1163600 | Distillation_student_train_gt:1302200\n",
      "Layer1:1216400 | Layer2:129400 | Layer3:25600 | TH:0.8 | TL:0.2 | Lis:14 | LayerTotal:1371400\n",
      "Distillation_student_train_preds:1250800 | Distillation_student_train_gt:1394200\n",
      "\n",
      "Subject 15\n",
      "Layer1:1233000 | Layer2:143600 | Layer3:47200 | TH:0.8 | TL:0.2 | Lis:15 | LayerTotal:1423800\n",
      "Layer1_TimeTatol:0:00:40.471213 | Layer2_Time:0:00:26.573153 | Layer3_Time:0:00:04.583694\n",
      "scpustats(ctx_switches=3748439048, interrupts=1043603709, soft_interrupts=0, syscalls=2311088667)\n",
      "Distillation_student_train_preds:1278600 | Distillation_student_train_gt:1428600\n"
     ]
    }
   ],
   "source": [
    "#%%Testing\n",
    "\n",
    "\n",
    "#FullAIID D\n",
    "PDS = \"D:/conda/111_1117/Model/FullAll_3LayerModel_DStudent_CNNSmall.pt\"\n",
    "PTA = \"D:/conda/111_1117/Model/FullAll_3LayerModel_DTA_Mobile.pt\"\n",
    "PT = \"D:/conda/111_1117/Model/FullAll_3LayerModel_T_ResNet18.pt\"\n",
    "\n",
    "#FullAIID O\n",
    "# PDS = \"D:/conda/111_1117/Model/FullAll_3LayerModel_S_CNNSmall.pt\"\n",
    "# PTA = \"D:/conda/111_1117/Model/FullAll_Original_MobileNet.pt\"\n",
    "# PT = \"D:/conda/111_1117/Model/FullAll_Original_ResNet18.pt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Layer1 = 0\n",
    "Layer2 = 0\n",
    "Layer3 = 0\n",
    "LayerUP=False\n",
    "Layer1_Time = datetime.timedelta()\n",
    "Layer2_Time = datetime.timedelta()\n",
    "Layer3_Time = datetime.timedelta()\n",
    "Distillation_student_train_preds=[]\n",
    "Distillation_student_train_gt=[]\n",
    "teacher_student_train_preds=[]\n",
    "teacher_student_train_gt=[]\n",
    "teacher_train_preds=[]\n",
    "teacher_train_gt=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lis = [1,2,3,4,5,7,8,9,10,11,12,13,14,15]\n",
    "TH=0.8\n",
    "TL=0.2\n",
    "\n",
    "# while op:\n",
    "\n",
    "for sub in lis:\n",
    "        if(sub==15):\n",
    "            print(\"\\nSubject\",sub)\n",
    "        test = (sliding_waist_subject == sub)\n",
    "        train = ~test\n",
    "\n",
    "        norm_sliding_waist_data = []\n",
    "        norm_sliding_waist_test = []\n",
    "        min_norm_value = []\n",
    "        min_norm_index = []\n",
    "        max_norm_value = []\n",
    "        max_norm_index = []\n",
    "        hori_sliding_waist_data = []\n",
    "        hori_sliding_waist_test = []\n",
    "        sliding_waist_data_hori = sliding_waist_data\n",
    "        sliding_waist_data_hori[:,:,0] = sliding_waist_data_hori[:,:,0]*0\n",
    "        min_hori_value = []\n",
    "        min_hori_index = []\n",
    "        max_hori_value = []\n",
    "        max_hori_index = []\n",
    "\n",
    "\n",
    "        X_train = sliding_waist_data[train]\n",
    "        Y_train_gd = sliding_waist_label[train]\n",
    "        X_test = sliding_waist_data[test]\n",
    "        Y_test_gd = sliding_waist_label[test]\n",
    "\n",
    "        X_train_hori = sliding_waist_data_hori[train]\n",
    "        X_test_hori = sliding_waist_data_hori[test]\n",
    "\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            c1 = np.linalg.norm(X_train[i], axis=1)\n",
    "            norm_sliding_waist_data.append(c1)\n",
    "\n",
    "        for i in range(len(X_test)):\n",
    "            c2 = np.linalg.norm(X_test[i], axis=1)\n",
    "            norm_sliding_waist_test.append(c2)\n",
    "\n",
    "        for i in range(len(X_train_hori)):\n",
    "            c1 = np.linalg.norm(X_train_hori[i], axis=1)\n",
    "            hori_sliding_waist_data.append(c1)\n",
    "\n",
    "        for i in range(len(X_test_hori)):\n",
    "            c2 = np.linalg.norm(X_test_hori[i], axis=1)\n",
    "            hori_sliding_waist_test.append(c2)\n",
    "\n",
    "\n",
    "        for i in range(len(Y_train_gd)):\n",
    "            if Y_train_gd[i] ==1:\n",
    "                d1 = np.min(norm_sliding_waist_data[i])\n",
    "                min_norm_value.append(d1)\n",
    "                d2 = np.argmin(norm_sliding_waist_data[i])\n",
    "                min_norm_index.append(d2)\n",
    "\n",
    "                d3 = np.min(hori_sliding_waist_data[i])\n",
    "                min_hori_value.append(d3)\n",
    "                d4 = np.argmin(hori_sliding_waist_data[i])\n",
    "                min_hori_index.append(d4)\n",
    "\n",
    "            else:\n",
    "                e1 = np.max(norm_sliding_waist_data[i])\n",
    "                max_norm_value.append(e1)\n",
    "                e2 = np.argmax(norm_sliding_waist_data[i])\n",
    "                max_norm_index.append(e2)\n",
    "\n",
    "                e3 = np.max(hori_sliding_waist_data[i])\n",
    "                max_hori_value.append(e3)\n",
    "                e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "                max_hori_index.append(e4)\n",
    "\n",
    "\n",
    "\n",
    "        MIN_train = np.min(min_norm_value)\n",
    "        MAX_train = np.max(max_norm_value)\n",
    "\n",
    "        MIN_train_hori = np.min(min_hori_value)\n",
    "        MAX_train_hori = np.max(max_hori_value)\n",
    "\n",
    "        unidentified_data = []\n",
    "        unidentified_label = []\n",
    "\n",
    "        for i in range(len(norm_sliding_waist_test)):\n",
    "           if max(norm_sliding_waist_test[i]) > MIN_train and max(norm_sliding_waist_test[i]) < MAX_train\\\n",
    "              and max(hori_sliding_waist_test[i]) > MIN_train_hori and max(hori_sliding_waist_test[i]) < MAX_train_hori:\n",
    "                unidentified_data.append(X_test[i])\n",
    "                unidentified_label.append(Y_test_gd[i])\n",
    "\n",
    "        unidentified_data = np.array(unidentified_data)\n",
    "        unidentified_label = np.array(unidentified_label)\n",
    "\n",
    "        #breakpoint() # insert breakpoint   \n",
    "\n",
    "        # Initializes the train and validation dataset in Torch format\n",
    "        x_train_tensor = torch.from_numpy(X_train).to(device) \n",
    "        x_train_tensor = x_train_tensor.reshape([-1,1,14,3]) \n",
    "        x_test_tensor = torch.from_numpy(unidentified_data).to(device)\n",
    "        x_test_tensor = x_test_tensor.reshape([-1,1,14,3])\n",
    "\n",
    "        y_train_gd_tensor =  torch.from_numpy(Y_train_gd).to(device)\n",
    "        y_test_gd_tensor =  torch.from_numpy(unidentified_label).to(device)\n",
    "\n",
    "        # Dataset wrapping tensors.Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "        deal_dataset = TensorDataset(x_train_tensor, y_train_gd_tensor) \n",
    "        test_dataset = TensorDataset(x_test_tensor,y_test_gd_tensor)\n",
    "\n",
    "        config['window_size'] = X_train.shape[1]\n",
    "        config['nb_channels'] = X_train.shape[2]\n",
    "        config['nb_classes'] = 2\n",
    "\n",
    "        # Sends network to the GPU and sets it to training mode\n",
    "        ResNet = Net_Teacher(block, [2, 2, 2, 2], 1, 2)\n",
    "        model_teacher = ResNet.to(device) \n",
    "\n",
    "        model_student = Net_Student(config).to(device) \n",
    "\n",
    "        #model_teacher_student = MobileNetV3_Small_KD().to(device)\n",
    "        model_teacher_student = MobileNetV3_Small().to(device)\n",
    "        \n",
    "        Distillation_S_model = load_checkpoint(model_student, PDS)\n",
    "        model_TA = load_checkpoint(model_teacher_student, PTA)\n",
    "        model_t = load_checkpoint(model_teacher, PT)\n",
    "\n",
    "    \n",
    "        \n",
    "#         Distillation_S_model = model_Distillation_student\n",
    "#         model_TA = model_teacher_student\n",
    "#         model_t = model_teacher\n",
    "        \n",
    "        \n",
    "        \n",
    "        # DataLoader represents a Python iterable over a dataset\n",
    "        trainloader = DataLoader(deal_dataset, batch_size=config['batch_size'],shuffle=True) \n",
    "        testloader = DataLoader(test_dataset, batch_size=config['batch_size'],shuffle=False)\n",
    "    #     # Initialize the optimizer and loss\n",
    "    \n",
    "        optimizer_d_s = torch.optim.Adam(Distillation_S_model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "\n",
    "        optimizer_t_s = torch.optim.Adam(model_TA.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "        optimizer_t = torch.optim.Adam(model_t.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "        #     optimizer_student = torch.optim.Adam(model_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "        # Start training and testing Teacher and Student Model\n",
    "        #training_teacher(trainloader,optimizer_teacher,criterion,model_teacher,testloader)    \n",
    "        #testing_teacher(testloader,optimizer_teacher,criterion,model_teacher)\n",
    "\n",
    "        #training_student(trainloader,optimizer_student,criterion,model_student,testloader)\n",
    "\n",
    "        alpha=0.5\n",
    "        \n",
    "        \n",
    "        testing_3LayerModel(testloader,optimizer_d_s,optimizer_t,optimizer_t_s,criterion,Distillation_S_model,model_TA,model_t,alpha,TH,TL,sub)\n",
    "\n",
    "        \n",
    "#         if(sub==15):\n",
    "# #             print(tt)\n",
    "#             teacher_Student_eval_table = evaluation(Distillation_student_train_preds, Distillation_student_train_gt)             \n",
    "#             print('Distillation_student_Acc:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[0],TH))\n",
    "#             print('Distillation_student_Rec:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[1],TH))\n",
    "#             print('Distillation_student_Pre:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[2],TH))\n",
    "#             print('Distillation_student_F1:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[3],TH))\n",
    "#             show_CM_teacher_student(Distillation_student_train_preds, Distillation_student_train_gt)\n",
    "\n",
    "\n",
    "#             if(teacher_Student_eval_table[0]<0.84):\n",
    "#                 if(sub==23):\n",
    "#             TA_eval_table = evaluation(teacher_student_train_preds, teacher_student_train_gt)\n",
    "#             print('Teacher_Student_Acc:{:5.4f} | TH:{:5.4f}'.format(TA_eval_table[0], TH))\n",
    "#             print('Teacher_Student_Rec:{:5.4f} | TH:{:5.4f}'.format(TA_eval_table[1], TH))\n",
    "#             print('Teacher_Student_Pre:{:5.4f} | TH:{:5.4f}'.format(TA_eval_table[2], TH))\n",
    "#             print('Teacher_Student_F1:{:5.4f} | TH:{:5.4f}'.format(TA_eval_table[3], TH))\n",
    "#             show_CM_teacher_student(teacher_student_train_preds, teacher_student_train_gt)   \n",
    "\n",
    "\n",
    "# #                     if(teacher_Student_eval_table[0]<0.84):\n",
    "# #                         if(sub==15):\n",
    "#             Teacher_eval_table = evaluation(teacher_train_preds, teacher_train_gt)\n",
    "#             print('Teacher_Acc:{:5.4f} | TH:{:5.4f}'.format(Teacher_eval_table[0], TH))\n",
    "#             print('Teacher_Rec:{:5.4f} | TH:{:5.4f}'.format(Teacher_eval_table[1], TH))\n",
    "#             print('Teacher_Pre:{:5.4f} | TH:{:5.4f}'.format(Teacher_eval_table[2], TH))\n",
    "#             print('Teacher_F1:{:5.4f} | TH:{:5.4f}'.format(Teacher_eval_table[3], TH))\n",
    "#             show_CM_teacher(teacher_train_preds, teacher_train_gt)\n",
    "\n",
    "Distillation_student_train_preds=[]\n",
    "Distillation_student_train_gt=[]\n",
    "teacher_student_train_preds=[]\n",
    "teacher_student_train_gt=[]\n",
    "teacher_train_preds=[]\n",
    "teacher_train_gt=[]\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8cf51dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\n",
      "\n",
      "flops: 1292091392.0 params 13941442.0\n",
      "flops_teacher: 1292.091 M, params_teachar: 13.941 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\n",
      "flops: 18176.0 params 58.0\n",
      "flops_student: 18.176 K, params_student: 0.058 K\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "\n",
      "flops: 85869568.0 params 1236782.0\n",
      "flops_teacher_student: 85869.568 K, params_teacher_student: 1236.782 K\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_Distillation_student' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_346688\\1634391243.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'flops_teacher_student: %.3f K, params_teacher_student: %.3f K'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflops_teacher_student\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1000.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_teacher_student\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1000.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mflops_Distillation_student\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_Distillation_student\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_Distillation_student\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nflops:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflops_Distillation_student\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'params'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_Distillation_student\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'flops_teacher_student: %.3f K, params_teacher_student: %.3f K'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflops_Distillation_student\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1000.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_Distillation_student\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1000.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_Distillation_student' is not defined"
     ]
    }
   ],
   "source": [
    "# print(\"Teacher(RestNet50)_Acc:\",sum(Accuracy_teacher)/14)\n",
    "# print(\"Teacher(RestNet50)_Rec:\",sum(Recall_teacher)/11)\n",
    "# print(\"Teacher(RestNet50)_Pre:\",sum(Precision_teacher)/11)\n",
    "# print(\"Teacher(RestNet50)_F1:\",sum(F1score_teacher)/11)\n",
    "\n",
    "# print(\"Original_Student(CNN)_Acc:\",sum(Accuracy_student)/14)\n",
    "# print(\"Original_Student(CNN)_Rec:\",sum(Recall_student)/11)\n",
    "# print(\"Original_Student(CNN)_Pre:\",sum(Precision_student)/11)\n",
    "# print(\"Original_Student(CNN)_F1:\",sum(F1score_student)/11)\n",
    "\n",
    "# print(\"Distillation_Student(CNN)_Acc:\",sum(Accuracy_teacher_student)/14)\n",
    "# print(\"Distillation_Student(CNN)_Rec:\",sum(Recall_teacher_student)/11)\n",
    "# print(\"Distillation_Student(CNN)_Pre:\",sum(Precision_teacher_student)/11)\n",
    "# print(\"Distillation_Student(CNN)_F1:\",sum(F1score_teacher_student)/11)\n",
    "\n",
    "\n",
    "\n",
    "dummy_input = torch.randn(64,1,14,3,device=device)\n",
    "flops_teacher, params_teachar = profile(model_teacher,(dummy_input,))\n",
    "print('\\n\\nflops:',flops_teacher, 'params', params_teachar)\n",
    "print('flops_teacher: %.3f M, params_teachar: %.3f M' % (flops_teacher / 1000000.0, params_teachar / 1000000.0))\n",
    "\n",
    "\n",
    "dummy_input = torch.randn(64,1,14,3,device=device)\n",
    "flops_student, params_student = profile(model_student,(dummy_input,))\n",
    "print('\\nflops:',flops_student, 'params', params_student)\n",
    "print('flops_student: %.3f K, params_student: %.3f K' % (flops_student / 1000.0, params_student / 1000.0))\n",
    "dummy_input = torch.randn(64,1,14,3,device=device)\n",
    "flops_teacher_student, params_teacher_student = profile(model_teacher_student,(dummy_input,))\n",
    "print('\\nflops:',flops_teacher_student, 'params', params_teacher_student)\n",
    "print('flops_teacher_student: %.3f K, params_teacher_student: %.3f K' % (flops_teacher_student / 1000.0, params_teacher_student / 1000.0))\n",
    "\n",
    "flops_Distillation_student, params_Distillation_student = profile(model_Distillation_student,(dummy_input,))\n",
    "print('\\nflops:',flops_Distillation_student, 'params', params_Distillation_student)\n",
    "print('flops_teacher_student: %.3f K, params_teacher_student: %.3f K' % (flops_Distillation_student / 1000.0, params_Distillation_student / 1000.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
