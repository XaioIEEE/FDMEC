{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7962c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from torch.nn import init\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8715560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3233, 2393, 3)\n",
      "(3233, 39, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remember to install CUDA and cuDNN\n",
    "# Determine if your system supports CUDA.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  \n",
    "#%% Read FallAllD data\n",
    "data=pd.read_hdf(\"D:/conda/FallAllD.h5\") #\n",
    "data = data.drop(columns = ['Gyr','Mag','Bar'])\n",
    "\n",
    "#%% Read SisFull\n",
    "data_Sis_pash=\"D:/conda/111_1117/SisFall_DL-master/SisFall_dataset/\"\n",
    "df=pd.read_csv(data_Sis_pash+\"SisFall_DataNameList.csv\",header=0)\n",
    "df_r=df[1:]\n",
    "\n",
    "df_v=df['DataName'].values[:]\n",
    "\n",
    "\n",
    "PATH = \"D:/conda/111_1117/SisFull_3LayerModel.pt\"\n",
    "#%% Label\n",
    "testmin=[]\n",
    "test_waist = []\n",
    "data_waist = []\n",
    "label_waist = []\n",
    "subject_waist = []\n",
    "# for i in range(len(data)):\n",
    "for i in range (len(df_r)):\n",
    "\n",
    "#     if(data['Device'][i] == 'Waist'):\n",
    "#         a = data['Acc'][i]*0.000244        \n",
    "        C_df = pd.read_csv(data_Sis_pash+df['DataName'][i])\n",
    "        testmin.append(len(C_df))\n",
    "        C_df = C_df[:].values[:2393].astype('float64')\n",
    "        a= C_df\n",
    "        # Normalize each data between 0 and 1\n",
    "        b = b = (a-np.amin(a))/(np.amax(a)-np.amin(a))\n",
    "        data_waist.append(b)        \n",
    "        subject_waist.append(df['Subject'][i])\n",
    "        # Class: fall = 0 ,ADL = 1 \n",
    "        if (df['FALL(1)_ADL(0)'][i]): \n",
    "                label_waist.append(1)\n",
    "        else:\n",
    "                label_waist.append(0)\n",
    "\n",
    "#extend\n",
    "\n",
    "# Change list to array\n",
    "data_waist = np.array(data_waist, dtype=object)            \n",
    "label_waist = np.array(label_waist, dtype=object)\n",
    "subject_waist = np.array(subject_waist, dtype=object)\n",
    "print(data_waist.shape)\n",
    "# Downsampling 1/128\n",
    "down_data = []\n",
    "# (start:size:step)\n",
    "# def down_data_SisADD(data_waist):\n",
    "#down_data = data_waist[::1,::128,::1]\n",
    "# data_waist = data_waist.reshape((len(data_waist), 1, 1))\n",
    "down_data = data_waist[::1, ::62, ::1]\n",
    "down_data = np.array(down_data, dtype=object)\n",
    "print(down_data.shape)\n",
    "#%% Sliding_window\n",
    "sliding_waist_data =[]\n",
    "sliding_waist_label =[]\n",
    "sliding_waist_subject =[]  \n",
    "# The signal of each second \n",
    "per_sec = 38/20\n",
    "sec = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba63d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12932, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "# for num in range(len(down_data)):\n",
    "\n",
    "#     if label_waist[num] ==1:\n",
    "#         # Data[index][start:end,axis]\n",
    "#         sliding_waist_data = np.append(sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*0.5)):int(np.uint(sec*per_sec*1.5))+1,:])\n",
    "#         # Label each window with class and subject\n",
    "#         sliding_waist_label = np.append(sliding_waist_label, 1)\n",
    "#         sliding_waist_subject = np.append(sliding_waist_subject, subject_waist[num])\n",
    "        \n",
    "#         sliding_waist_data = np.append(sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*0.75)):int(np.uint(sec*per_sec*1.75-1))+1,:])\n",
    "#         sliding_waist_label = np.append(sliding_waist_label, 1)\n",
    "#         sliding_waist_subject = np.append(sliding_waist_subject, subject_waist[num])\n",
    "        \n",
    "#         sliding_waist_data = np.append(sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*1)):int(np.uint(sec*per_sec*2))+1,:])\n",
    "#         sliding_waist_label = np.append(sliding_waist_label, 1)\n",
    "#         sliding_waist_subject = np.append(sliding_waist_subject, subject_waist[num])\n",
    "        \n",
    "#         sliding_waist_data = np.append(sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*1.25)):int(np.uint(sec*per_sec*2.25))+1,:])\n",
    "#         sliding_waist_label = np.append(sliding_waist_label, 1)\n",
    "#         sliding_waist_subject = np.append(sliding_waist_subject, subject_waist[num])\n",
    "        \n",
    "#     else:\n",
    "#         #print(down_data[num])\n",
    "#         sliding_waist_data = np.concatenate([sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*0.5)):int(np.uint(sec*per_sec*1.5))+1,:]])\n",
    "#         #print(sliding_waist_data)\n",
    "#         sliding_waist_label = np.concatenate([sliding_waist_label, 0])\n",
    "#         sliding_waist_subject = np.concatenate([sliding_waist_subject, subject_waist[num]])\n",
    "        \n",
    "#         sliding_waist_data = np.concatenate([sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*0.75)):int(np.uint(sec*per_sec*1.75-1))+1,:]])\n",
    "#         sliding_waist_label = np.concatenate([sliding_waist_label, 0])\n",
    "#         sliding_waist_subject = np.concatenate([sliding_waist_subject, subject_waist[num]])\n",
    "        \n",
    "#         sliding_waist_data = np.concatenate([sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*1)):int(np.uint(sec*per_sec*2))+1,:]])\n",
    "#         sliding_waist_label = np.concatenate([sliding_waist_label, 0])\n",
    "#         sliding_waist_subject = np.concatenate([sliding_waist_subject, subject_waist[num]])\n",
    "        \n",
    "#         sliding_waist_data = np.concatenate([sliding_waist_data, down_data[num][int(np.uint(sec*per_sec*1.25)):int(np.uint(sec*per_sec*2.25))+1,:]])\n",
    "#         sliding_waist_label = np.concatenate([sliding_waist_label, 0])\n",
    "#         sliding_waist_subject = np.concatenate([sliding_waist_subject, subject_waist[num]])\n",
    "\n",
    "\n",
    "for num in range(len(down_data)):\n",
    "    if label_waist[num] ==1:\n",
    "        # Data[index][start:end,axis]\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.5)):int(np.uint(sec*per_sec*1.5)+1),:])\n",
    "        # Label each window with class and subject\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.75)):int(np.uint(sec*per_sec*1.75-1))+1,:])\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1)):int(np.uint(sec*per_sec*2))+1,:])\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1.25)):int(np.uint(sec*per_sec*2.25))+1,:])\n",
    "        sliding_waist_label.append(1)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        \n",
    "    else:\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.5)):int(np.uint(sec*per_sec*1.5))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*0.75)):int(np.uint(sec*per_sec*1.75-1))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1)):int(np.uint(sec*per_sec*2))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "        sliding_waist_data.append(down_data[num][int(np.uint(sec*per_sec*1.25)):int(np.uint(sec*per_sec*2.25))+1,:])\n",
    "        sliding_waist_label.append(0)\n",
    "        sliding_waist_subject.append(subject_waist[num])\n",
    "\n",
    "sliding_waist_data = np.array(sliding_waist_data)\n",
    "sliding_waist_label = np.array(sliding_waist_label)\n",
    "sliding_waist_subject = np.array(sliding_waist_subject) \n",
    "print(sliding_waist_data.shape)\n",
    "np.save('FallALLD_SW_data',sliding_waist_data)\n",
    "np.save('FallALLD_SW_label',sliding_waist_label)\n",
    "np.save('FallALLD_SW_subject',sliding_waist_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2388cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Show Teacher confusion matrix\n",
    "def show_CM_teacher(validation,prediction):     \n",
    "    matrix = metrics.confusion_matrix(validation,prediction)\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(matrix,cmap = 'coolwarm',linecolor= 'white',\n",
    "                linewidths= 1,annot= True,fmt = 'd')\n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.show()\n",
    "    plt.savefig('Prediction (Teacher) SubjectID' + str(sub) + '.png')\n",
    "\n",
    "#%% Show Student confusion matrix\n",
    "def show_CM_student(validation,prediction):     \n",
    "    matrix = metrics.confusion_matrix(validation,prediction)\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(matrix,cmap = 'coolwarm',linecolor= 'white',\n",
    "                linewidths= 1,annot= True,fmt = 'd')\n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.show()\n",
    "    plt.savefig('Prediction (Student) SubjectID' + str(sub) + '.png')\n",
    "    \n",
    "#%% Show Teacher-Student confusion matrix\n",
    "def show_CM_teacher_student(validation,prediction):     \n",
    "    matrix = metrics.confusion_matrix(validation,prediction)\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(matrix,cmap = 'coolwarm',linecolor= 'white',\n",
    "                linewidths= 1,annot= True,fmt = 'd')\n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.show()\n",
    "    plt.savefig('Prediction (Teacher-Student) SubjectID' + str(sub) + '.png')\n",
    "def show_loss_teacher(loss_train,loss_test):     \n",
    "    plt.plot(loss_train, label = 'trian_loss')\n",
    "    plt.plot(loss_test, label = 'test_loss')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend([\"Train Loss\",\"Test Loss\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Loss (teacher) SubjectID' + str(sub) + '.png')\n",
    "def show_Acc_teacher(train_acc, test_acc):     \n",
    "    plt.plot(train_acc, label = 'trian_Acc')\n",
    "    plt.plot(test_acc, label = 'test_Acc')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend([\"Train Acc\",\"Test Acc\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Acc (teacher) SubjectID' + str(sub) + '.png')\n",
    "#Loss_student\n",
    "def show_loss_student(loss_train,loss_test):     \n",
    "    plt.plot(loss_train, label = 'trian_loss')\n",
    "    plt.plot(loss_test, label = 'test_loss')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend([\"Train Loss\",\"Test Loss\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Loss (Student) SubjectID' + str(sub) + '.png')   \n",
    "def show_Acc_student(train_acc, test_acc):     \n",
    "    plt.plot(train_acc, label = 'trian_Acc')\n",
    "    plt.plot(test_acc, label = 'test_Acc')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend([\"Train Acc\",\"Test Acc\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Acc (student) SubjectID' + str(sub) + '.png')\n",
    "def show_loss_teacher_student(loss_train,loss_test):     \n",
    "    plt.plot(loss_train, label = 'trian_loss')\n",
    "    plt.plot(loss_test, label = 'test_loss')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend([\"Train Loss\",\"Test Loss\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Loss (teacher_Student) SubjectID' + str(sub) + '.png')   \n",
    "def show_Acc_teacher_student(train_acc, test_acc):     \n",
    "    plt.plot(train_acc, label = 'trian_Acc')\n",
    "    plt.plot(test_acc, label = 'test_Acc')    \n",
    "    plt.title('SubjectID' + str(sub))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend([\"Train Acc\",\"Test Acc\"],loc = 'upper right')\n",
    "    plt.show()\n",
    "    #plt.savefig('Train_Test Acc (teacher_student) SubjectID' + str(sub) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f7e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):\n",
    "    # 一般的Cross Entropy\n",
    "    labels=labels.type(torch.cuda.LongTensor)\n",
    "    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    # 讓logits的log_softmax對目標機率(teacher的logits/T後softmax)做KL Divergence。\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs/T, dim=1),\n",
    "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)\n",
    "    return hard_loss + soft_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e162ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hswish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        out = x * F.relu6(x + 3, inplace=True) / 6\n",
    "        return out\n",
    "\n",
    "\n",
    "class hsigmoid(nn.Module):\n",
    "    def forward(self, x):\n",
    "        out = F.relu6(x + 3, inplace=True) / 6\n",
    "        return out\n",
    "\n",
    "\n",
    "class SeModule(nn.Module):\n",
    "    def __init__(self, in_size, reduction=4):\n",
    "        super(SeModule, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_size, in_size // reduction, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_size // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_size // reduction, in_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_size),\n",
    "            hsigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''expand + depthwise + pointwise'''\n",
    "    def __init__(self, kernel_size, in_size, expand_size, out_size, nolinear, semodule, stride):\n",
    "        super(Block, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.se = semodule\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(expand_size)\n",
    "        self.nolinear1 = nolinear\n",
    "        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=expand_size, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(expand_size)\n",
    "        self.nolinear2 = nolinear\n",
    "        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_size)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1 and in_size != out_size:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_size),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.nolinear1(self.bn1(self.conv1(x)))\n",
    "        out = self.nolinear2(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        if self.se != None:\n",
    "            out = self.se(out)\n",
    "        out = out + self.shortcut(x) if self.stride==1 else out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1446d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% This is the config object which contains all relevant settings.    \n",
    "config = {\n",
    "    'nb_filters': 64,\n",
    "    'filter_width': 1,\n",
    "    'drop_prob': 0.5,\n",
    "    'epochs': 200,#200\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-3,#1e-3\n",
    "    'weight_decay': 2e-6,\n",
    "    'gpu_name': 'cuda:0',\n",
    "    'print_counts': False,\n",
    "    'lr_factor' : 2,\n",
    "    'lr_warmup' : 40\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0471818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3_Small(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MobileNetV3_Small, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.hs1 = hswish()\n",
    "\n",
    "        self.bneck = nn.Sequential(\n",
    "            Block(3, 64, 64, 16, nn.ReLU(inplace=True), SeModule(16), 2),\n",
    "            Block(3, 16, 72, 24, nn.ReLU(inplace=True), None, 2),\n",
    "            Block(3, 24, 88, 24, nn.ReLU(inplace=True), None, 1),\n",
    "            Block(5, 24, 96, 40, hswish(), SeModule(40), 2),\n",
    "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
    "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
    "            Block(5, 40, 120, 48, hswish(), SeModule(48), 1),\n",
    "            Block(5, 48, 144, 48, hswish(), SeModule(48), 1),\n",
    "            Block(5, 48, 288, 96, hswish(), SeModule(96), 2),\n",
    "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
    "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(576)\n",
    "        self.hs2 = hswish()\n",
    "        self.linear3 = nn.Linear(576, 1280)\n",
    "        self.bn3 = nn.BatchNorm1d(1280)\n",
    "        self.hs3 = hswish()\n",
    "        self.linear4 = nn.Linear(1280, num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hs1(self.bn1(self.conv1(x)))\n",
    "        out = self.bneck(out)\n",
    "        out = self.hs2(self.bn2(self.conv2(out)))\n",
    "        out = F.avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.hs3(self.bn3(self.linear3(out)))\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aabbde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3_Large(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MobileNetV3_Large, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.hs1 = hswish()\n",
    "\n",
    "        self.bneck = nn.Sequential(\n",
    "            Block(3, 64, 64, 16, nn.ReLU(inplace=True), None, 1),\n",
    "            Block(3, 16, 64, 24, nn.ReLU(inplace=True), None, 2),\n",
    "            Block(3, 24, 72, 24, nn.ReLU(inplace=True), None, 1),\n",
    "            Block(5, 24, 72, 40, nn.ReLU(inplace=True), SeModule(40), 2),\n",
    "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
    "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
    "            Block(3, 40, 240, 80, hswish(), None, 2),\n",
    "            Block(3, 80, 200, 80, hswish(), None, 1),\n",
    "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
    "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
    "            Block(3, 80, 480, 112, hswish(), SeModule(112), 1),\n",
    "            Block(3, 112, 672, 112, hswish(), SeModule(112), 1),\n",
    "            Block(5, 112, 672, 160, hswish(), SeModule(160), 1),\n",
    "            Block(5, 160, 672, 160, hswish(), SeModule(160), 2),\n",
    "            Block(5, 160, 960, 160, hswish(), SeModule(160), 1),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(160, 960, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(960)\n",
    "        self.hs2 = hswish()\n",
    "        self.linear3 = nn.Linear(960, 1280)\n",
    "        self.bn3 = nn.BatchNorm1d(1280)\n",
    "        self.hs3 = hswish()\n",
    "        self.linear4 = nn.Linear(1280, num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hs1(self.bn1(self.conv1(x)))\n",
    "        out = self.bneck(out)\n",
    "        out = self.hs2(self.bn2(self.conv2(out)))\n",
    "        out = F.avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.hs3(self.bn3(self.linear3(out)))\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb3aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3_Small_KD(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(MobileNetV3_Small_KD, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.hs1 = hswish()\n",
    "\n",
    "        self.bneck = nn.Sequential(\n",
    "            Block(3, 64, 64, 16, nn.ReLU(inplace=True), SeModule(16), 2),\n",
    "            Block(3, 16, 72, 24, nn.ReLU(inplace=True), None, 2),\n",
    "            Block(3, 24, 88, 24, nn.ReLU(inplace=True), None, 1),\n",
    "            Block(5, 24, 96, 40, hswish(), SeModule(40), 2),\n",
    "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
    "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
    "            Block(5, 40, 120, 48, hswish(), SeModule(48), 1),\n",
    "            Block(5, 48, 144, 48, hswish(), SeModule(48), 1),\n",
    "            Block(5, 48, 288, 96, hswish(), SeModule(96), 2),\n",
    "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
    "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(576)\n",
    "        self.hs2 = hswish()\n",
    "        self.linear3 = nn.Linear(576, 1280)\n",
    "        self.bn3 = nn.BatchNorm1d(1280)\n",
    "        self.hs3 = hswish()\n",
    "        self.linear4 = nn.Linear(1280, num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hs1(self.bn1(self.conv1(x)))\n",
    "        out = self.bneck(out)\n",
    "        out = self.hs2(self.bn2(self.conv2(out)))\n",
    "        out = F.avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.hs3(self.bn3(self.linear3(out)))\n",
    "        out = self.linear4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35895351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define teacher neural network\n",
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "      super(block, self).__init__()\n",
    "      self.expansion = 4\n",
    "      self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "      self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "      self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1,bias=False)\n",
    "      self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "      self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "      self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "      self.relu = nn.ReLU()\n",
    "      self.identity_downsample = identity_downsample\n",
    "      self.stride = stride\n",
    "\n",
    "  def forward(self, x):\n",
    "      identity = x.clone()\n",
    "      x = self.conv1(x)\n",
    "      x = self.bn1(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.bn2(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.conv3(x)\n",
    "      x = self.bn3(x)\n",
    "\n",
    "      if self.identity_downsample is not None:\n",
    "          identity = self.identity_downsample(identity)\n",
    "\n",
    "      x += identity\n",
    "      x = self.relu(x)\n",
    "      return x\n",
    "\n",
    "class Net_Teacher(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes): \n",
    "        # Return a proxy object that delegates method calls to a parent or sibling class of type, which is nn.Module in this case\n",
    "        super(Net_Teacher, self).__init__()\n",
    "        self.in_channels = 64        \n",
    "        # This way can print out every layers output in forward propagation, which is more convenient to adjust. nn.Sequential(nn.Conv2d(1, self.nb_filters, 3),nn.MaxPool2d([2,1]),...) is more visibility \n",
    "        # Output shape after convolution layer = (input shape – filter shape + 1) + (2*padding/stride)\n",
    "        # nn.Conv2d(input channel, output channel, kernel size(x,x),stride,padding)\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=3, stride=2, padding=3, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
    "        self.layer1 = self._make_layer(block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self._make_layer(block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], intermediate_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x            \n",
    "        # The loss function, nn.CrossEntropyLoss(), combines LogSoftmax and NLLLoss, so we should use softmax after loss calculation\n",
    "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
    "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
    "        # to the layer that's ahead\n",
    "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    intermediate_channels * 4,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(intermediate_channels * 4),\n",
    "            )\n",
    "\n",
    "        layers.append(\n",
    "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
    "        )\n",
    "\n",
    "        # The expansion size is always 4 for ResNet 50,101,152\n",
    "        self.in_channels = intermediate_channels * 4\n",
    "\n",
    "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
    "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
    "        # and also same amount of channels.\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, intermediate_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "#%% Define student neural network\n",
    "class Net_Student(nn.Module):\n",
    "    def __init__(self,config): \n",
    "        # Return a proxy object that delegates method calls to a parent or sibling class of type, which is nn.Module in this case\n",
    "        super(Net_Student, self).__init__()\n",
    "        # Input hyperparameter \n",
    "        self.window_size = config['window_size']\n",
    "        self.drop_prob = config['drop_prob']\n",
    "        self.nb_channels = config['nb_channels']\n",
    "        self.nb_classes = config['nb_classes']\n",
    "        self.nb_filters = config['nb_filters']\n",
    "        self.filter_width = config['filter_width']\n",
    "        \n",
    "        # This way can print out every layers output in forward propagation, which is more convenient to adjust. nn.Sequential(nn.Conv2d(1, self.nb_filters, 3),nn.MaxPool2d([2,1]),...) is more visibility \n",
    "        # Output shape after convolution layer = (input shape – filter shape + 1) + (2*padding/stride)\n",
    "        # nn.Conv2d(input channel, output channel, kernel size(x,x),stride,padding)\n",
    "        self.conv1 = nn.Conv2d(1, self.nb_filters, 3) \n",
    "        self.pool = nn.MaxPool2d([2,1])\n",
    "        self.conv2 = nn.Conv2d(64, self.nb_filters,(3,1))\n",
    "        \n",
    "        # nn.Linear(input size,output size)\n",
    "        self.fc1 = nn.Linear(64*4,256)  \n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        # In this case, we need to classify 2 classes, fall and ADL, so the last output size need to be 2\n",
    "        self.fc4 = nn.Linear(64, 2)\n",
    "         \n",
    "    # Define forward propagation    \n",
    "    def forward(self, x):  \n",
    "        # Reshape input size (batch, input channels, window size, window channels)\n",
    "        x = x.view(-1, 1, 14, 3) \n",
    "        x = F.relu(self.conv1(x)) \n",
    "        # output(64,64,12,1)\n",
    "        x = self.pool(x)\n",
    "        #(64,64,6,1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #(64,64,4,1)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x)) \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x    \n",
    "        # The loss function, nn.CrossEntropyLoss(), combines LogSoftmax and NLLLoss, so we should use softmax after loss calculation\n",
    "\n",
    "#%% Define student neural network\n",
    "class Net_Teacher_Student(nn.Module):\n",
    "    def __init__(self,config): \n",
    "        # Return a proxy object that delegates method calls to a parent or sibling class of type, which is nn.Module in this case\n",
    "        super(Net_Teacher_Student, self).__init__()\n",
    "        # Input hyperparameter \n",
    "        self.window_size = config['window_size']\n",
    "        self.drop_prob = config['drop_prob']\n",
    "        self.nb_channels = config['nb_channels']\n",
    "        self.nb_classes = config['nb_classes']\n",
    "        self.nb_filters = config['nb_filters']\n",
    "        self.filter_width = config['filter_width']\n",
    "        \n",
    "        # This way can print out every layers output in forward propagation, which is more convenient to adjust. nn.Sequential(nn.Conv2d(1, self.nb_filters, 3),nn.MaxPool2d([2,1]),...) is more visibility \n",
    "        # Output shape after convolution layer = (input shape – filter shape + 1) + (2*padding/stride)\n",
    "        # nn.Conv2d(input channel, output channel, kernel size(x,x),stride,padding)\n",
    "        self.conv1 = nn.Conv2d(1, self.nb_filters, 3) \n",
    "        self.pool = nn.MaxPool2d([2,1])\n",
    "        self.conv2 = nn.Conv2d(64, self.nb_filters,(3,1))\n",
    "        \n",
    "        # nn.Linear(input size,output size)\n",
    "        self.fc1 = nn.Linear(64*4,256)  \n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        # In this case, we need to classify 2 classes, fall and ADL, so the last output size need to be 2\n",
    "        self.fc4 = nn.Linear(64, 2)\n",
    "         \n",
    "    # Define forward propagation    \n",
    "    def forward(self, x):  \n",
    "        # Reshape input size (batch, input channels, window size, window channels)\n",
    "        x = x.view(-1, 1, 14, 3) \n",
    "        x = F.relu(self.conv1(x)) \n",
    "        # output(64,64,12,1)\n",
    "        x = self.pool(x)\n",
    "        #(64,64,6,1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #(64,64,4,1)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x)) \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x   \n",
    "        # The loss function, nn.CrossEntropyLoss(), combines LogSoftmax and NLLLoss, so we should use softmax after loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2659401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define teacher training loop\n",
    "def training_teacher(trainloader,optimizer,criterion,model,testloader):\n",
    "    loss_train =[]\n",
    "    train_acc = []\n",
    "    total_train=0\n",
    "    correct_train=0\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_gt = []\n",
    "\n",
    "        # Set start time to count the calaulated time of each epoch\n",
    "        start_time = time.time()\n",
    "        # Iterate the trainloader \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            # Send data to GPU\n",
    "            inputs,labels = x.to(device), y.to(device)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass (compute output)\n",
    "            pred = model(inputs) \n",
    "            # Compute loss\n",
    "            loss = criterion(pred, labels.long())\n",
    "            # Backpropagaton\n",
    "            loss.backward() \n",
    "            # Update parameter\n",
    "            optimizer.step() \n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum()\n",
    "            \n",
    "            # Use softmax to normalize the output to a probability distribution over predicted output classes\n",
    "            train_output = F.softmax(pred, dim =1)\n",
    "            y_preds = np.argmax(train_output.cpu().detach().numpy(), axis=-1)\n",
    "            y_true = labels.cpu().numpy().flatten()\n",
    "            train_preds = np.concatenate((np.array(train_preds, int), np.array(y_preds, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "        \n",
    "        train_acc.append(100 * (correct_train / total_train).cpu())\n",
    "        cur_loss = np.mean(train_losses)\n",
    "        loss_train.append(cur_loss)\n",
    "        # Get the time this epoch used\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5.4f} s/epoch | train loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))\n",
    "    #save(config['epochs'], model, optimizer, loss)\n",
    "    \n",
    "    eval_table = evaluation(train_preds, train_gt)\n",
    "\n",
    "    print('Train Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "    print(' Train Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "    print(' Train Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "    print(' Train F1-score:{:.5f}'.format(eval_table[3]), end='')\n",
    "\n",
    "#%% Define teacher testing loop\n",
    "#def testing_teacher(testloader,optimizer,criterion,model):\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_test=0\n",
    "    correct_test=0\n",
    "    # Sets network to eval mod\n",
    "    model.eval() \n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):\n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                inputs ,labels = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                labels = labels.to(torch.float32)\n",
    "\n",
    "                test_output = model(inputs)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(test_output, labels.long())\n",
    "                # Backpropagaton\n",
    "                #loss.backward()\n",
    "                # Update parameter\n",
    "                optimizer.step() \n",
    "                test_losses.append(loss.item())        \n",
    "                _, predicted = torch.max(test_output.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum()\n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)                \n",
    "\n",
    "                y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_true = labels.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))            \n",
    "            \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            test_acc.append(100 * (correct_test / total_test).cpu())\n",
    "#         eval_table = evaluation(test_preds,test_gt)\n",
    "#         print('\\nVal Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_teacher(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_teacher.append(eval_table[0])\n",
    "#         Recall_teacher.append(eval_table[1])\n",
    "#         Precision_teacher.append(eval_table[2])\n",
    "#         F1score_teacher.append(eval_table[3])\n",
    "    #Plot train_loss\n",
    "        teacher_train_preds.extend(test_preds)\n",
    "        teacher_train_gt.extend(test_gt)\n",
    "\n",
    "    show_loss_teacher(loss_train, loss_test)\n",
    "    show_Acc_teacher(train_acc, test_acc)\n",
    "    #return Accuracy_teacher, Recall_teacher, Precision_teacher, F1score_teacher\n",
    "    return teacher_train_preds, teacher_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7945a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define student training loop\n",
    "def training_student(trainloader,optimizer,criterion,model,testloader):\n",
    "    loss_train =[]\n",
    "    train_acc = []\n",
    "    total_train=0\n",
    "    correct_train=0\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_gt = []\n",
    "        # Set start time to count the calaulated time of each epoch\n",
    "        start_time = time.time()\n",
    "        # Iterate the trainloader \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            # Send data to GPU\n",
    "            inputs,labels = x.to(device), y.to(device)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass (compute output)\n",
    "            pred = model(inputs) \n",
    "            # Compute loss\n",
    "            loss = criterion(pred, labels.long())\n",
    "            # Backpropagaton\n",
    "            loss.backward() \n",
    "            # Update parameter\n",
    "            optimizer.step() \n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum()\n",
    "            # Use softmax to normalize the output to a probability distribution over predicted output classes\n",
    "            train_output = F.softmax(pred, dim =1)\n",
    "            y_preds = np.argmax(train_output.cpu().detach().numpy(), axis=-1)\n",
    "            y_true = labels.cpu().numpy().flatten()\n",
    "            train_preds = np.concatenate((np.array(train_preds, int), np.array(y_preds, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "        \n",
    "        cur_loss = np.mean(train_losses)\n",
    "        loss_train.append(cur_loss)\n",
    "        train_acc.append(100 * (correct_train / total_train).cpu())\n",
    "        # Get the time this epoch used\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5.4f} s/epoch | train loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))\n",
    "    \n",
    "    eval_table = evaluation(train_preds, train_gt)\n",
    "    print('Train Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "    print(' Train Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "    print(' Train Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "    print(' Train F1-score:{:.5f}'.format(eval_table[3]), end='')\n",
    "\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_test=0\n",
    "    correct_test=0\n",
    "    # Sets network to eval mod\n",
    "    model.eval()\n",
    "    #start_time = time.time()\n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):\n",
    "            for i, (x, y) in enumerate(testloader):            \n",
    "                inputs ,labels = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                labels = labels.to(torch.float32)\n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer.zero_grad()                \n",
    "                # Compute loss                \n",
    "\n",
    "                test_output = model(inputs)\n",
    "                \n",
    "                loss = criterion(test_output, labels.long())\n",
    "                # Backpropagaton\n",
    "                #loss.backward()\n",
    "                # Update parameter\n",
    "                optimizer.step()\n",
    "                test_losses.append(loss.item())                \n",
    "                \n",
    "                _, predicted = torch.max(test_output.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum()\n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)\n",
    "\n",
    "                y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_true = labels.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))\n",
    "                \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            test_acc.append(100 * (correct_test / total_test).cpu())\n",
    "            elapsed = time.time() - start_time\n",
    "            #print('| epoch {:3d} | {:5.4f} s/epoch | test loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))    \n",
    "#         eval_table = evaluation(test_preds,test_gt)\n",
    "#         print('\\nTest Val Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_student(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_student.append(eval_table[0])\n",
    "#         Recall_student.append(eval_table[1])\n",
    "#         Precision_student.append(eval_table[2])\n",
    "#         F1score_student.append(eval_table[3])\n",
    "        \n",
    "        student_train_preds.extend(test_preds)\n",
    "        student_train_gt.extend(test_gt)\n",
    "    #Plot train_loss\n",
    "    show_loss_student(loss_train, loss_test)\n",
    "    show_Acc_student(train_acc, test_acc)\n",
    "    \n",
    "    #return Accuracy_student, Recall_student, Precision_student, F1score_student\n",
    "    return student_train_preds, student_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94fcb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_teacher_student(trainloader,optimizer,criterion,model_teacher,model_student,alpha,testloader,Layer,lis):\n",
    "    loss_train =[]\n",
    "    train_acc = []\n",
    "    total_train=0\n",
    "    correct_train=0\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Freeze the Teacher model\n",
    "    model_teacher.eval()\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_losses = []\n",
    "        train_preds_student = []\n",
    "        train_preds_teacher = []\n",
    "        train_gt = []\n",
    "        # Set start time to count the calaulated time of each epoch\n",
    "        start_time = time.time()\n",
    "        # Iterate the trainloader \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            \n",
    "            # Send data to GPU\n",
    "            inputs,labels = x.to(device), y.to(device)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (compute output)\n",
    "            output_student = model_student(inputs)\n",
    "            output_teacher = model_teacher(inputs)\n",
    "\n",
    "            # Use softmax to normalize the output to a probability distribution over predicted output classes\n",
    "            output_student_soft = F.softmax(output_student, dim =1)\n",
    "            y_preds_student = np.argmax(output_student_soft.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "            output_teacher_soft = F.softmax(output_teacher, dim =1)\n",
    "            y_preds_teacher = np.argmax(output_teacher_soft.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn_kd(output_student, labels, output_teacher, 20, alpha)\n",
    "            # Backpropagaton\n",
    "            loss.backward() \n",
    "            # Update parameter\n",
    "            optimizer.step() \n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            total_hit += torch.sum(torch.argmax(output_student, dim=1) == labels).item()\n",
    "            total_num += len(inputs)\n",
    "            y_true = labels.cpu().numpy().flatten()\n",
    "            train_preds_student = np.concatenate((np.array(train_preds_student, int), np.array(y_preds_student, int)))\n",
    "            train_preds_teacher = np.concatenate((np.array(train_preds_teacher, int), np.array(y_preds_teacher, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "        \n",
    "        cur_loss = np.mean(train_losses)\n",
    "        loss_train.append(cur_loss)\n",
    "        train_acc.append(total_hit/total_num)\n",
    "        # Get the time this epoch used\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5.4f} s/epoch | train loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))   \n",
    "\n",
    "    \n",
    "    eval_table = evaluation(train_preds_student, train_gt)\n",
    "    print('Train Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "    print(' Train Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "    print(' Train Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "    print(' Train F1-score:{:.5f}'.format(eval_table[3]), end='')\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "#%% Define Teacher-Student testing loop\n",
    "#def testing_teacher_student(testloader,optimizer,criterion,model_teacher,model_student):\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Sets network to eval mod\n",
    "    model_teacher.eval()\n",
    "    model_student.eval() \n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):            \n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                inputs ,targets = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                targets = targets.to(torch.float32)\n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                test_output = model_student(inputs)\n",
    "                test_teacher = model_teacher(inputs)\n",
    "                # Compute loss\n",
    "                #loss = loss_fn_kd(test_output, labels, output_teacher, 20, alpha)                \n",
    "                loss = loss_fn_kd(test_output, targets, test_teacher, 20, alpha)                \n",
    "                # Backpropagaton\n",
    "                #loss.backward() \n",
    "                # Update parameter\n",
    "                #optimizer.step() \n",
    "                test_losses.append(loss.item())     \n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)                \n",
    "                total_hit += torch.sum(torch.argmax(test_output, dim=1) == targets).item()\n",
    "                total_num += len(inputs)\n",
    "                \n",
    "                y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_true = targets.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))            \n",
    "            \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            #eval_table = evaluation(test_preds,test_gt)\n",
    "            test_acc.append(total_hit/total_num)\n",
    "#         print('\\nVal Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_teacher_student(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_teacher_student.append(eval_table[0])\n",
    "#         Recall_teacher_student.append(eval_table[1])\n",
    "#         Precision_teacher_student.append(eval_table[2])\n",
    "#         F1score_teacher_student.append(eval_table[3])      \n",
    "        teacher_student_train_preds.extend(test_preds)\n",
    "        teacher_student_train_gt.extend(test_gt)\n",
    "    show_loss_teacher_student(loss_train, loss_test)\n",
    "    show_Acc_teacher_student(train_acc,test_acc)\n",
    "    #return Accuracy_teacher_student, Recall_teacher_student, Precision_teacher_student, F1score_teacher_student\n",
    "    if (Layer==False) and (lis == 23):\n",
    "            save(config['epochs'], model_student, optimizer, loss)  \n",
    "            #return teacher_student_train_preds, teacher_student_train_gt, teacher_student_model, teacher_student_optimizer, teacher_student_loss\n",
    "    if (Layer==False) and (lis == 1):\n",
    "            save(config['epochs'], model_student, optimizer, loss)\n",
    "    return teacher_student_train_preds, teacher_student_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29e3c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_distillation_student(trainloader,optimizer,criterion,model_teacher,model_student,alpha,testloader,Layer,lis):\n",
    "    loss_train =[]\n",
    "    train_acc = []\n",
    "    total_train=0\n",
    "    correct_train=0\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Freeze the Teacher model\n",
    "    model_teacher.eval()\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_losses = []\n",
    "        train_preds_student = []\n",
    "        train_preds_teacher = []\n",
    "        train_gt = []\n",
    "        # Set start time to count the calaulated time of each epoch\n",
    "        start_time = time.time()\n",
    "        # Iterate the trainloader \n",
    "        for i, (x,y) in enumerate(trainloader):\n",
    "            \n",
    "            # Send data to GPU\n",
    "            inputs,labels = x.to(device), y.to(device)\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            labels = labels.to(torch.float32)\n",
    "            # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (compute output)\n",
    "            output_student = model_student(inputs)\n",
    "            output_teacher = model_teacher(inputs)\n",
    "\n",
    "            # Use softmax to normalize the output to a probability distribution over predicted output classes\n",
    "            output_student_soft = F.softmax(output_student, dim =1)\n",
    "            y_preds_student = np.argmax(output_student_soft.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "            output_teacher_soft = F.softmax(output_teacher, dim =1)\n",
    "            y_preds_teacher = np.argmax(output_teacher_soft.cpu().detach().numpy(), axis=-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn_kd(output_student, labels, output_teacher, 20, alpha)\n",
    "            # Backpropagaton\n",
    "            loss.backward() \n",
    "            # Update parameter\n",
    "            optimizer.step() \n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            total_hit += torch.sum(torch.argmax(output_student, dim=1) == labels).item()\n",
    "            total_num += len(inputs)\n",
    "            y_true = labels.cpu().numpy().flatten()\n",
    "            train_preds_student = np.concatenate((np.array(train_preds_student, int), np.array(y_preds_student, int)))\n",
    "            train_preds_teacher = np.concatenate((np.array(train_preds_teacher, int), np.array(y_preds_teacher, int)))\n",
    "            train_gt = np.concatenate((np.array(train_gt, int), np.array(y_true, int)))\n",
    "        \n",
    "        cur_loss = np.mean(train_losses)\n",
    "        loss_train.append(cur_loss)\n",
    "        train_acc.append(total_hit/total_num)\n",
    "        # Get the time this epoch used\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5.4f} s/epoch | train loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))\n",
    "    \n",
    "    eval_table = evaluation(train_preds_student, train_gt)\n",
    "    print('Train Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "    print(' Train Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "    print(' Train Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "    print(' Train F1-score:{:.5f}'.format(eval_table[3]), end='')\n",
    "    \n",
    "\n",
    "#%% Define Teacher-Student testing loop\n",
    "#def testing_teacher_student(testloader,optimizer,criterion,model_teacher,model_student):\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Sets network to eval mod\n",
    "    model_teacher.eval()\n",
    "    model_student.eval() \n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):            \n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                inputs ,targets = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                targets = targets.to(torch.float32)\n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                test_output = model_student(inputs)\n",
    "                test_teacher = model_teacher(inputs)\n",
    "                # Compute loss\n",
    "                #loss = loss_fn_kd(test_output, labels, output_teacher, 20, alpha)                \n",
    "                loss = loss_fn_kd(test_output, targets, test_teacher, 20, alpha)                \n",
    "                # Backpropagaton\n",
    "                #loss.backward() \n",
    "                # Update parameter\n",
    "                #optimizer.step() \n",
    "                test_losses.append(loss.item())     \n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)                \n",
    "                total_hit += torch.sum(torch.argmax(test_output, dim=1) == targets).item()\n",
    "                total_num += len(inputs)\n",
    "                \n",
    "                y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_true = targets.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))            \n",
    "            \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            #eval_table = evaluation(test_preds,test_gt)\n",
    "            test_acc.append(total_hit/total_num)\n",
    "#         print('\\nVal Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_teacher_student(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_teacher_student.append(eval_table[0])\n",
    "#         Recall_teacher_student.append(eval_table[1])\n",
    "#         Precision_teacher_student.append(eval_table[2])\n",
    "#         F1score_teacher_student.append(eval_table[3])      \n",
    "        Distillation_student_train_preds.extend(test_preds)\n",
    "        Distillation_student_train_gt.extend(test_gt)\n",
    "    show_loss_teacher_student(loss_train, loss_test)\n",
    "    show_Acc_teacher_student(train_acc,test_acc)\n",
    "    #return Accuracy_teacher_student, Recall_teacher_student, Precision_teacher_student, F1score_teacher_student\n",
    "    return Distillation_student_train_preds, Distillation_student_train_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c1d49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_teacher(testloader,optimizer,criterion,model,TH):\n",
    "\n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_test=0\n",
    "    correct_test=0\n",
    "    # Sets network to eval mod\n",
    "    model.eval()\n",
    "    #start_time = time.time()\n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):\n",
    "            start_time = time.time()\n",
    "            for i, (x, y) in enumerate(testloader):            \n",
    "                inputs ,labels = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                labels = labels.to(torch.float32)\n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer.zero_grad()                \n",
    "                # Compute loss                \n",
    "\n",
    "                test_output = model(inputs)\n",
    "                \n",
    "                loss = criterion(test_output, labels.long())\n",
    "                # Backpropagaton\n",
    "                #loss.backward()\n",
    "                # Update parameter\n",
    "                optimizer.step()\n",
    "                test_losses.append(loss.item())                \n",
    "                \n",
    "                _, predicted = torch.max(test_output.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum()\n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)\n",
    "\n",
    "                #y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_preds = Threshold(test_output.cpu().numpy(),TH)\n",
    "                y_true = labels.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))\n",
    "                #print(test_output.shape)\n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            test_acc.append(100 * (correct_test / total_test).cpu())\n",
    "            elapsed = time.time() - start_time\n",
    "            #print('| epoch {:3d} | {:5.4f} s/epoch | test loss {:5.4f}'.format(epoch, elapsed/60, cur_loss))    \n",
    "#         eval_table = evaluation(test_preds,test_gt)\n",
    "#         print('\\nTest Val Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_student(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_student.append(eval_table[0])\n",
    "#         Recall_student.append(eval_table[1])\n",
    "#         Precision_student.append(eval_table[2])\n",
    "#         F1score_student.append(eval_table[3])\n",
    "        teacher_train_preds.extend(test_preds)\n",
    "        teacher_train_gt.extend(test_gt)\n",
    "\n",
    "#     show_loss_teacher(loss_train, loss_test)\n",
    "#     show_Acc_teacher(train_acc, test_acc)\n",
    "    #return Accuracy_teacher, Recall_teacher, Precision_teacher, F1score_teacher\n",
    "    return teacher_train_preds, teacher_train_gt\n",
    "\n",
    "def testing_teacher_student(testloader,optimizer,criterion,model_teacher,model_student,alpha,TH):\n",
    "    \n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Sets network to eval mod\n",
    "    model_teacher.eval()\n",
    "    model_student.eval() \n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):\n",
    "            start_time = time.time()\n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                inputs ,targets = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                targets = targets.to(torch.float32)\n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                test_output = model_student(inputs)\n",
    "                test_teacher = model_teacher(inputs)\n",
    "                # Compute loss\n",
    "                #loss = loss_fn_kd(test_output, labels, output_teacher, 20, alpha)                \n",
    "                loss = loss_fn_kd(test_output, targets, test_teacher, 20, alpha)                \n",
    "                # Backpropagaton\n",
    "                #loss.backward() \n",
    "                # Update parameter\n",
    "                #optimizer.step() \n",
    "                test_losses.append(loss.item())     \n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)                \n",
    "                total_hit += torch.sum(torch.argmax(test_output, dim=1) == targets).item()\n",
    "                total_num += len(inputs)\n",
    "                \n",
    "                \n",
    "#                 y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_preds = Threshold(test_output.cpu().numpy(),TH)\n",
    "                y_true = targets.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))            \n",
    "            \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            #eval_table = evaluation(test_preds,test_gt)\n",
    "            test_acc.append(total_hit/total_num)\n",
    "#         print('\\nVal Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_teacher_student(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_teacher_student.append(eval_table[0])\n",
    "#         Recall_teacher_student.append(eval_table[1])\n",
    "#         Precision_teacher_student.append(eval_table[2])\n",
    "#         F1score_teacher_student.append(eval_table[3]) \n",
    "        teacher_student_acc = test_acc\n",
    "        teacher_student_train_preds.extend(test_preds)\n",
    "        teacher_student_train_gt.extend(test_gt)\n",
    "#     show_loss_teacher_student(loss_train, loss_test)\n",
    "#     show_Acc_teacher_student(train_acc,test_acc)\n",
    "    #return Accuracy_teacher_student, Recall_teacher_student, Precision_teacher_student, F1score_teacher_student\n",
    "    return teacher_student_train_preds, teacher_student_train_gt\n",
    "\n",
    "def testing_Distillation_student(testloader,optimizer,criterion,model_teacher,model_student,alpha,TH):\n",
    "    \n",
    "    test_preds = []\n",
    "    test_gt = []\n",
    "    loss_test =[]\n",
    "    test_losses = []\n",
    "    test_acc = []\n",
    "    total_num, total_hit = 0, 0\n",
    "    # Sets network to eval mod\n",
    "    model_teacher.eval()\n",
    "    model_student.eval() \n",
    "    # Disable gradient calculation for efficiency    \n",
    "    with torch.no_grad():\n",
    "        for epoch in range(config['epochs']):\n",
    "            start_time = time.time()\n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                inputs ,targets = x.to(device) ,y.to(device)\n",
    "                inputs = inputs.to(torch.float32)\n",
    "                targets = targets.to(torch.float32)\n",
    "                # Sets the gradients of all optimized torch.Tensors to zero.\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                test_output = model_student(inputs)\n",
    "                test_teacher = model_teacher(inputs)\n",
    "                # Compute loss\n",
    "                #loss = loss_fn_kd(test_output, labels, output_teacher, 20, alpha)                \n",
    "                loss = loss_fn_kd(test_output, targets, test_teacher, 20, alpha)                \n",
    "                # Backpropagaton\n",
    "                #loss.backward() \n",
    "                # Update parameter\n",
    "                #optimizer.step() \n",
    "                test_losses.append(loss.item())     \n",
    "                \n",
    "                test_output = F.softmax(test_output, dim=1)                \n",
    "                total_hit += torch.sum(torch.argmax(test_output, dim=1) == targets).item()\n",
    "                total_num += len(inputs)\n",
    "                \n",
    "#                 y_preds = np.argmax(test_output.cpu().numpy(), axis=-1)\n",
    "                y_preds = Threshold(test_output.cpu().numpy(),TH)\n",
    "                y_true = targets.cpu().numpy().flatten()\n",
    "                test_preds = np.concatenate((np.array(test_preds, int), np.array(y_preds, int)))\n",
    "                test_gt = np.concatenate((np.array(test_gt, int), np.array(y_true, int)))            \n",
    "            \n",
    "            cur_loss = np.mean(test_losses)\n",
    "            loss_test.append(cur_loss)\n",
    "            #eval_table = evaluation(test_preds,test_gt)\n",
    "            test_acc.append(total_hit/total_num)\n",
    "#         print('\\nVal Acc:{:.5f}'.format(eval_table[0]), end='')\n",
    "#         print(' Val Rec:{:.5f}'.format(eval_table[1]), end='')\n",
    "#         print(' Val Precis:{:.5f}'.format(eval_table[2]), end='')\n",
    "#         print(' Val F1-score:{:.5f}\\n'.format(eval_table[3]), end='')\n",
    "        #show_CM_teacher_student(test_gt, test_preds)\n",
    "        #Store each result\n",
    "#         Accuracy_teacher_student.append(eval_table[0])\n",
    "#         Recall_teacher_student.append(eval_table[1])\n",
    "#         Precision_teacher_student.append(eval_table[2])\n",
    "#         F1score_teacher_student.append(eval_table[3]) \n",
    "        teacher_student_acc = test_acc\n",
    "        Distillation_student_train_preds.extend(test_preds)\n",
    "        Distillation_student_train_gt.extend(test_gt)\n",
    "#     show_loss_teacher_student(loss_train, loss_test)\n",
    "#     show_Acc_teacher_student(train_acc,test_acc)\n",
    "    #return Accuracy_teacher_student, Recall_teacher_student, Precision_teacher_student, F1score_teacher_student\n",
    "    return Distillation_student_train_preds, Distillation_student_train_gt\n",
    "\n",
    "def Threshold(test_output,Threshold):\n",
    "    test_preds=[]\n",
    "    numpyArray = np.array(test_output)\n",
    "    #print(numpyArray.shape)\n",
    "    for i, (x, y) in enumerate(test_output):\n",
    "        #print(i,(x, y))\n",
    "        if x > Threshold :\n",
    "            test_preds.append(0)\n",
    "        else:\n",
    "            test_preds.append(1)\n",
    "    #print(test_preds)\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2518ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(epoch, student, optimizer, loss):\n",
    "#     trial_id = self.config['trial_id']\n",
    "#     if name is None:\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': student.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #'loss_state_dict': loss.state_dice()\n",
    "        'loss_state_dict': loss.item()\n",
    "        }, PATH)\n",
    "#     else:\n",
    "#         torch.save({\n",
    "#         'model_state_dict': self.student.state_dict(),\n",
    "#         'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "#         'epoch': epoch,\n",
    "#         }, name)\n",
    "\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")\n",
    "        print(\"~~~~~~~~~~~~~~~SAVE~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "162ee7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint_path):\n",
    "        \"\"\"\n",
    "        Loads weights from checkpoint\n",
    "        :param model: a pytorch nn student\n",
    "        :param str checkpoint_path: address/path of a file\n",
    "        :return: pytorch nn student with weights loaded from checkpoint\n",
    "        \"\"\"\n",
    "        model_ckp = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(model_ckp['model_state_dict'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e939dc90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subject 1\n",
      "| epoch   0 | 0.0881 s/epoch | train loss 0.3439\n",
      "| epoch   1 | 0.0502 s/epoch | train loss 0.2429\n",
      "| epoch   2 | 0.0507 s/epoch | train loss 0.2182\n",
      "| epoch   3 | 0.0512 s/epoch | train loss 0.2056\n",
      "| epoch   4 | 0.0506 s/epoch | train loss 0.1995\n",
      "| epoch   5 | 0.0501 s/epoch | train loss 0.1916\n",
      "| epoch   6 | 0.0507 s/epoch | train loss 0.1808\n",
      "| epoch   7 | 0.0500 s/epoch | train loss 0.1728\n",
      "| epoch   8 | 0.0504 s/epoch | train loss 0.1654\n",
      "| epoch   9 | 0.0498 s/epoch | train loss 0.1590\n",
      "| epoch  10 | 0.0503 s/epoch | train loss 0.1484\n",
      "| epoch  11 | 0.0501 s/epoch | train loss 0.1525\n",
      "| epoch  12 | 0.0504 s/epoch | train loss 0.1450\n",
      "| epoch  13 | 0.0495 s/epoch | train loss 0.1429\n",
      "| epoch  14 | 0.0494 s/epoch | train loss 0.1313\n",
      "| epoch  15 | 0.0501 s/epoch | train loss 0.1844\n",
      "| epoch  16 | 0.0520 s/epoch | train loss 0.1893\n",
      "| epoch  17 | 0.0515 s/epoch | train loss 0.1524\n",
      "| epoch  18 | 0.0516 s/epoch | train loss 0.1403\n",
      "| epoch  19 | 0.0513 s/epoch | train loss 0.1278\n",
      "| epoch  20 | 0.0500 s/epoch | train loss 0.1226\n",
      "| epoch  21 | 0.0511 s/epoch | train loss 0.1185\n",
      "| epoch  22 | 0.0509 s/epoch | train loss 0.1123\n",
      "| epoch  23 | 0.0503 s/epoch | train loss 0.1097\n",
      "| epoch  24 | 0.0500 s/epoch | train loss 0.1179\n",
      "| epoch  25 | 0.0493 s/epoch | train loss 0.1047\n",
      "| epoch  26 | 0.0498 s/epoch | train loss 0.1043\n",
      "| epoch  27 | 0.0508 s/epoch | train loss 0.0977\n",
      "| epoch  28 | 0.0510 s/epoch | train loss 0.0936\n",
      "| epoch  29 | 0.0516 s/epoch | train loss 0.0899\n",
      "| epoch  30 | 0.0504 s/epoch | train loss 0.0995\n",
      "| epoch  31 | 0.0513 s/epoch | train loss 0.0887\n",
      "| epoch  32 | 0.0512 s/epoch | train loss 0.0853\n",
      "| epoch  33 | 0.0522 s/epoch | train loss 0.0870\n",
      "| epoch  34 | 0.0499 s/epoch | train loss 0.0798\n",
      "| epoch  35 | 0.0510 s/epoch | train loss 0.0748\n",
      "| epoch  36 | 0.0506 s/epoch | train loss 0.0790\n",
      "| epoch  37 | 0.0517 s/epoch | train loss 0.0763\n",
      "| epoch  38 | 0.0510 s/epoch | train loss 0.0721\n",
      "| epoch  39 | 0.0505 s/epoch | train loss 0.0733\n",
      "| epoch  40 | 0.0507 s/epoch | train loss 0.0668\n",
      "| epoch  41 | 0.0516 s/epoch | train loss 0.0696\n",
      "| epoch  42 | 0.0513 s/epoch | train loss 0.0650\n",
      "| epoch  43 | 0.0506 s/epoch | train loss 0.0688\n",
      "| epoch  44 | 0.0506 s/epoch | train loss 0.0647\n",
      "| epoch  45 | 0.0508 s/epoch | train loss 0.0628\n",
      "| epoch  46 | 0.0500 s/epoch | train loss 0.0669\n",
      "| epoch  47 | 0.0513 s/epoch | train loss 0.0798\n",
      "| epoch  48 | 0.0507 s/epoch | train loss 0.0609\n",
      "| epoch  49 | 0.0508 s/epoch | train loss 0.0603\n",
      "| epoch  50 | 0.0514 s/epoch | train loss 0.0546\n",
      "| epoch  51 | 0.0514 s/epoch | train loss 0.0596\n",
      "| epoch  52 | 0.0505 s/epoch | train loss 0.0541\n",
      "| epoch  53 | 0.0498 s/epoch | train loss 0.0486\n",
      "| epoch  54 | 0.0506 s/epoch | train loss 0.0456\n",
      "| epoch  55 | 0.0515 s/epoch | train loss 0.0471\n",
      "| epoch  56 | 0.0507 s/epoch | train loss 0.0479\n",
      "| epoch  57 | 0.0505 s/epoch | train loss 0.0479\n",
      "| epoch  58 | 0.0505 s/epoch | train loss 0.0495\n",
      "| epoch  59 | 0.0504 s/epoch | train loss 0.0437\n",
      "| epoch  60 | 0.0505 s/epoch | train loss 0.0503\n",
      "| epoch  61 | 0.0498 s/epoch | train loss 0.0501\n",
      "| epoch  62 | 0.0507 s/epoch | train loss 0.0531\n",
      "| epoch  63 | 0.0517 s/epoch | train loss 0.0369\n",
      "| epoch  64 | 0.0504 s/epoch | train loss 0.0482\n",
      "| epoch  65 | 0.0501 s/epoch | train loss 0.0399\n",
      "| epoch  66 | 0.0504 s/epoch | train loss 0.0423\n",
      "| epoch  67 | 0.0504 s/epoch | train loss 0.0345\n",
      "| epoch  68 | 0.0500 s/epoch | train loss 0.0444\n",
      "| epoch  69 | 0.0502 s/epoch | train loss 0.0400\n",
      "| epoch  70 | 0.0504 s/epoch | train loss 0.0409\n",
      "| epoch  71 | 0.0500 s/epoch | train loss 0.0412\n",
      "| epoch  72 | 0.0502 s/epoch | train loss 0.0373\n",
      "| epoch  73 | 0.0514 s/epoch | train loss 0.0308\n",
      "| epoch  74 | 0.0520 s/epoch | train loss 0.0352\n",
      "| epoch  75 | 0.0505 s/epoch | train loss 0.0333\n",
      "| epoch  76 | 0.0498 s/epoch | train loss 0.0352\n",
      "| epoch  77 | 0.0503 s/epoch | train loss 0.0421\n",
      "| epoch  78 | 0.0509 s/epoch | train loss 0.0331\n",
      "| epoch  79 | 0.0509 s/epoch | train loss 0.0401\n",
      "| epoch  80 | 0.0509 s/epoch | train loss 0.0361\n",
      "| epoch  81 | 0.0510 s/epoch | train loss 0.0357\n",
      "| epoch  82 | 0.0510 s/epoch | train loss 0.0328\n",
      "| epoch  83 | 0.0511 s/epoch | train loss 0.0306\n",
      "| epoch  84 | 0.0507 s/epoch | train loss 0.0269\n",
      "| epoch  85 | 0.0500 s/epoch | train loss 0.0327\n",
      "| epoch  86 | 0.0507 s/epoch | train loss 0.0347\n",
      "| epoch  87 | 0.0500 s/epoch | train loss 0.0292\n",
      "| epoch  88 | 0.0506 s/epoch | train loss 0.0305\n",
      "| epoch  89 | 0.0499 s/epoch | train loss 0.0294\n",
      "| epoch  90 | 0.0502 s/epoch | train loss 0.0328\n",
      "| epoch  91 | 0.0502 s/epoch | train loss 0.0289\n",
      "| epoch  92 | 0.0503 s/epoch | train loss 0.0261\n",
      "| epoch  93 | 0.0507 s/epoch | train loss 0.0314\n",
      "| epoch  94 | 0.0510 s/epoch | train loss 0.0257\n",
      "| epoch  95 | 0.0515 s/epoch | train loss 0.0250\n",
      "| epoch  96 | 0.0517 s/epoch | train loss 0.0283\n",
      "| epoch  97 | 0.0509 s/epoch | train loss 0.0263\n",
      "| epoch  98 | 0.0504 s/epoch | train loss 0.0310\n",
      "| epoch  99 | 0.0500 s/epoch | train loss 0.0235\n",
      "| epoch 100 | 0.0504 s/epoch | train loss 0.0331\n",
      "| epoch 101 | 0.0498 s/epoch | train loss 0.0233\n",
      "| epoch 102 | 0.0496 s/epoch | train loss 0.0363\n",
      "| epoch 103 | 0.0503 s/epoch | train loss 0.0280\n",
      "| epoch 104 | 0.0503 s/epoch | train loss 0.0213\n",
      "| epoch 105 | 0.0505 s/epoch | train loss 0.0422\n",
      "| epoch 106 | 0.0499 s/epoch | train loss 0.0266\n",
      "| epoch 107 | 0.0501 s/epoch | train loss 0.0210\n",
      "| epoch 108 | 0.0506 s/epoch | train loss 0.0256\n",
      "| epoch 109 | 0.0502 s/epoch | train loss 0.0257\n",
      "| epoch 110 | 0.0501 s/epoch | train loss 0.0311\n",
      "| epoch 111 | 0.0476 s/epoch | train loss 0.0214\n",
      "| epoch 112 | 0.0554 s/epoch | train loss 0.0194\n",
      "| epoch 113 | 0.0555 s/epoch | train loss 0.0261\n",
      "| epoch 114 | 0.0554 s/epoch | train loss 0.0197\n",
      "| epoch 115 | 0.0548 s/epoch | train loss 0.0246\n",
      "| epoch 116 | 0.0558 s/epoch | train loss 0.0222\n",
      "| epoch 117 | 0.0553 s/epoch | train loss 0.0275\n",
      "| epoch 118 | 0.0545 s/epoch | train loss 0.0204\n",
      "| epoch 119 | 0.0550 s/epoch | train loss 0.0224\n",
      "| epoch 120 | 0.0547 s/epoch | train loss 0.0185\n",
      "| epoch 121 | 0.0553 s/epoch | train loss 0.0271\n",
      "| epoch 122 | 0.0548 s/epoch | train loss 0.0224\n",
      "| epoch 123 | 0.0551 s/epoch | train loss 0.0288\n",
      "| epoch 124 | 0.0559 s/epoch | train loss 0.0182\n",
      "| epoch 125 | 0.0557 s/epoch | train loss 0.0241\n",
      "| epoch 126 | 0.0549 s/epoch | train loss 0.0170\n",
      "| epoch 127 | 0.0553 s/epoch | train loss 0.0219\n",
      "| epoch 128 | 0.0557 s/epoch | train loss 0.0172\n",
      "| epoch 129 | 0.0562 s/epoch | train loss 0.0246\n",
      "| epoch 130 | 0.0557 s/epoch | train loss 0.0146\n",
      "| epoch 131 | 0.0555 s/epoch | train loss 0.0348\n",
      "| epoch 132 | 0.0556 s/epoch | train loss 0.0244\n",
      "| epoch 133 | 0.0556 s/epoch | train loss 0.0198\n",
      "| epoch 134 | 0.0547 s/epoch | train loss 0.0213\n",
      "| epoch 135 | 0.0550 s/epoch | train loss 0.0218\n",
      "| epoch 136 | 0.0543 s/epoch | train loss 0.0269\n",
      "| epoch 137 | 0.0557 s/epoch | train loss 0.0131\n",
      "| epoch 138 | 0.0550 s/epoch | train loss 0.0195\n",
      "| epoch 139 | 0.0564 s/epoch | train loss 0.0215\n",
      "| epoch 140 | 0.0558 s/epoch | train loss 0.0236\n",
      "| epoch 141 | 0.0551 s/epoch | train loss 0.0186\n",
      "| epoch 142 | 0.0553 s/epoch | train loss 0.0193\n",
      "| epoch 143 | 0.0549 s/epoch | train loss 0.0230\n",
      "| epoch 144 | 0.0558 s/epoch | train loss 0.0228\n",
      "| epoch 145 | 0.0558 s/epoch | train loss 0.0197\n",
      "| epoch 146 | 0.0562 s/epoch | train loss 0.0157\n",
      "| epoch 147 | 0.0571 s/epoch | train loss 0.0190\n",
      "| epoch 148 | 0.0551 s/epoch | train loss 0.0148\n",
      "| epoch 149 | 0.0550 s/epoch | train loss 0.0242\n",
      "| epoch 150 | 0.0549 s/epoch | train loss 0.0213\n",
      "| epoch 151 | 0.0547 s/epoch | train loss 0.0169\n",
      "| epoch 152 | 0.0558 s/epoch | train loss 0.0186\n",
      "| epoch 153 | 0.0553 s/epoch | train loss 0.0153\n",
      "| epoch 154 | 0.0546 s/epoch | train loss 0.0126\n",
      "| epoch 155 | 0.0546 s/epoch | train loss 0.0186\n",
      "| epoch 156 | 0.0556 s/epoch | train loss 0.0207\n",
      "| epoch 157 | 0.0555 s/epoch | train loss 0.0211\n",
      "| epoch 158 | 0.0564 s/epoch | train loss 0.0160\n",
      "| epoch 159 | 0.0556 s/epoch | train loss 0.0164\n",
      "| epoch 160 | 0.0554 s/epoch | train loss 0.0159\n",
      "| epoch 161 | 0.0552 s/epoch | train loss 0.0201\n",
      "| epoch 162 | 0.0548 s/epoch | train loss 0.0193\n",
      "| epoch 163 | 0.0550 s/epoch | train loss 0.0146\n",
      "| epoch 164 | 0.0548 s/epoch | train loss 0.0179\n",
      "| epoch 165 | 0.0551 s/epoch | train loss 0.0199\n",
      "| epoch 166 | 0.0547 s/epoch | train loss 0.0134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 167 | 0.0549 s/epoch | train loss 0.0183\n",
      "| epoch 168 | 0.0551 s/epoch | train loss 0.0172\n",
      "| epoch 169 | 0.0550 s/epoch | train loss 0.0205\n",
      "| epoch 170 | 0.0546 s/epoch | train loss 0.0186\n",
      "| epoch 171 | 0.0553 s/epoch | train loss 0.0124\n",
      "| epoch 172 | 0.0548 s/epoch | train loss 0.0134\n",
      "| epoch 173 | 0.0556 s/epoch | train loss 0.0206\n",
      "| epoch 174 | 0.0564 s/epoch | train loss 0.0141\n",
      "| epoch 175 | 0.0591 s/epoch | train loss 0.0158\n",
      "| epoch 176 | 0.0550 s/epoch | train loss 0.0165\n",
      "| epoch 177 | 0.0550 s/epoch | train loss 0.0148\n",
      "| epoch 178 | 0.0558 s/epoch | train loss 0.0155\n",
      "| epoch 179 | 0.0549 s/epoch | train loss 0.0153\n",
      "| epoch 180 | 0.0546 s/epoch | train loss 0.0159\n",
      "| epoch 181 | 0.0557 s/epoch | train loss 0.0192\n",
      "| epoch 182 | 0.0551 s/epoch | train loss 0.0203\n",
      "| epoch 183 | 0.0545 s/epoch | train loss 0.0171\n",
      "| epoch 184 | 0.0550 s/epoch | train loss 0.0122\n",
      "| epoch 185 | 0.0568 s/epoch | train loss 0.0174\n",
      "| epoch 186 | 0.0561 s/epoch | train loss 0.0127\n",
      "| epoch 187 | 0.0555 s/epoch | train loss 0.0166\n",
      "| epoch 188 | 0.0556 s/epoch | train loss 0.0152\n",
      "| epoch 189 | 0.0553 s/epoch | train loss 0.0071\n",
      "| epoch 190 | 0.0555 s/epoch | train loss 0.0196\n",
      "| epoch 191 | 0.0557 s/epoch | train loss 0.0250\n",
      "| epoch 192 | 0.0551 s/epoch | train loss 0.0135\n",
      "| epoch 193 | 0.0546 s/epoch | train loss 0.0196\n",
      "| epoch 194 | 0.0551 s/epoch | train loss 0.0240\n",
      "| epoch 195 | 0.0553 s/epoch | train loss 0.0150\n",
      "| epoch 196 | 0.0548 s/epoch | train loss 0.0124\n",
      "| epoch 197 | 0.0550 s/epoch | train loss 0.0180\n",
      "| epoch 198 | 0.0542 s/epoch | train loss 0.0135\n",
      "| epoch 199 | 0.0553 s/epoch | train loss 0.0100\n",
      "Train Acc:0.99635 Train Rec:0.99600 Train Precis:0.99650 Train F1-score:0.99625"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | 0.0613 s/epoch | train loss 0.3924\n",
      "| epoch   1 | 0.0600 s/epoch | train loss 0.2938\n",
      "| epoch   2 | 0.0595 s/epoch | train loss 0.2634\n",
      "| epoch   3 | 0.0604 s/epoch | train loss 0.2478\n",
      "| epoch   4 | 0.0584 s/epoch | train loss 0.2353\n",
      "| epoch   5 | 0.0591 s/epoch | train loss 0.2243\n",
      "| epoch   6 | 0.0606 s/epoch | train loss 0.2090\n",
      "| epoch   7 | 0.0590 s/epoch | train loss 0.2072\n",
      "| epoch   8 | 0.0591 s/epoch | train loss 0.2055\n",
      "| epoch   9 | 0.0596 s/epoch | train loss 0.1984\n",
      "| epoch  10 | 0.0599 s/epoch | train loss 0.1886\n",
      "| epoch  11 | 0.0590 s/epoch | train loss 0.1881\n",
      "| epoch  12 | 0.0591 s/epoch | train loss 0.1837\n",
      "| epoch  13 | 0.0589 s/epoch | train loss 0.1709\n",
      "| epoch  14 | 0.0591 s/epoch | train loss 0.1716\n",
      "| epoch  15 | 0.0598 s/epoch | train loss 0.1656\n",
      "| epoch  16 | 0.0601 s/epoch | train loss 0.1667\n",
      "| epoch  17 | 0.0601 s/epoch | train loss 0.1613\n",
      "| epoch  18 | 0.0600 s/epoch | train loss 0.1620\n",
      "| epoch  19 | 0.0596 s/epoch | train loss 0.1513\n",
      "| epoch  20 | 0.0602 s/epoch | train loss 0.1451\n",
      "| epoch  21 | 0.0601 s/epoch | train loss 0.1483\n",
      "| epoch  22 | 0.0588 s/epoch | train loss 0.1495\n",
      "| epoch  23 | 0.0598 s/epoch | train loss 0.1382\n",
      "| epoch  24 | 0.0591 s/epoch | train loss 0.1324\n",
      "| epoch  25 | 0.0601 s/epoch | train loss 0.1383\n",
      "| epoch  26 | 0.0596 s/epoch | train loss 0.1404\n",
      "| epoch  27 | 0.0587 s/epoch | train loss 0.1319\n",
      "| epoch  28 | 0.0596 s/epoch | train loss 0.1262\n",
      "| epoch  29 | 0.0590 s/epoch | train loss 0.1285\n",
      "| epoch  30 | 0.0593 s/epoch | train loss 0.1225\n",
      "| epoch  31 | 0.0593 s/epoch | train loss 0.1208\n",
      "| epoch  32 | 0.0603 s/epoch | train loss 0.1202\n",
      "| epoch  33 | 0.0607 s/epoch | train loss 0.1202\n",
      "| epoch  34 | 0.0601 s/epoch | train loss 0.1117\n",
      "| epoch  35 | 0.0601 s/epoch | train loss 0.1131\n",
      "| epoch  36 | 0.0610 s/epoch | train loss 0.1091\n",
      "| epoch  37 | 0.0603 s/epoch | train loss 0.1147\n",
      "| epoch  38 | 0.0595 s/epoch | train loss 0.1152\n",
      "| epoch  39 | 0.0601 s/epoch | train loss 0.1514\n",
      "| epoch  40 | 0.0599 s/epoch | train loss 0.1201\n",
      "| epoch  41 | 0.0599 s/epoch | train loss 0.1057\n",
      "| epoch  42 | 0.0602 s/epoch | train loss 0.1098\n",
      "| epoch  43 | 0.0605 s/epoch | train loss 0.1064\n",
      "| epoch  44 | 0.0595 s/epoch | train loss 0.1008\n",
      "| epoch  45 | 0.0605 s/epoch | train loss 0.0995\n",
      "| epoch  46 | 0.0600 s/epoch | train loss 0.0954\n",
      "| epoch  47 | 0.0598 s/epoch | train loss 0.1001\n",
      "| epoch  48 | 0.0586 s/epoch | train loss 0.0969\n",
      "| epoch  49 | 0.0592 s/epoch | train loss 0.0972\n",
      "| epoch  50 | 0.0599 s/epoch | train loss 0.0908\n",
      "| epoch  51 | 0.0598 s/epoch | train loss 0.0945\n",
      "| epoch  52 | 0.0587 s/epoch | train loss 0.0909\n",
      "| epoch  53 | 0.0584 s/epoch | train loss 0.0946\n",
      "| epoch  54 | 0.0606 s/epoch | train loss 0.0896\n",
      "| epoch  55 | 0.0596 s/epoch | train loss 0.0887\n",
      "| epoch  56 | 0.0604 s/epoch | train loss 0.0875\n",
      "| epoch  57 | 0.0583 s/epoch | train loss 0.0925\n",
      "| epoch  58 | 0.0585 s/epoch | train loss 0.0905\n",
      "| epoch  59 | 0.0599 s/epoch | train loss 0.0899\n",
      "| epoch  60 | 0.0607 s/epoch | train loss 0.0856\n",
      "| epoch  61 | 0.0593 s/epoch | train loss 0.0835\n",
      "| epoch  62 | 0.0600 s/epoch | train loss 0.0814\n",
      "| epoch  63 | 0.0588 s/epoch | train loss 0.0829\n",
      "| epoch  64 | 0.0600 s/epoch | train loss 0.0818\n",
      "| epoch  65 | 0.0602 s/epoch | train loss 0.0794\n",
      "| epoch  66 | 0.0591 s/epoch | train loss 0.0826\n",
      "| epoch  67 | 0.0605 s/epoch | train loss 0.0791\n",
      "| epoch  68 | 0.0599 s/epoch | train loss 0.0776\n",
      "| epoch  69 | 0.0594 s/epoch | train loss 0.0746\n",
      "| epoch  70 | 0.0596 s/epoch | train loss 0.0847\n",
      "| epoch  71 | 0.0587 s/epoch | train loss 0.0751\n",
      "| epoch  72 | 0.0600 s/epoch | train loss 0.0753\n",
      "| epoch  73 | 0.0600 s/epoch | train loss 0.0745\n",
      "| epoch  74 | 0.0599 s/epoch | train loss 0.0715\n",
      "| epoch  75 | 0.0602 s/epoch | train loss 0.0752\n",
      "| epoch  76 | 0.0590 s/epoch | train loss 0.0719\n",
      "| epoch  77 | 0.0603 s/epoch | train loss 0.0784\n",
      "| epoch  78 | 0.0604 s/epoch | train loss 0.0705\n",
      "| epoch  79 | 0.0609 s/epoch | train loss 0.0691\n",
      "| epoch  80 | 0.0606 s/epoch | train loss 0.0729\n",
      "| epoch  81 | 0.0591 s/epoch | train loss 0.0709\n",
      "| epoch  82 | 0.0587 s/epoch | train loss 0.0693\n",
      "| epoch  83 | 0.0601 s/epoch | train loss 0.0666\n",
      "| epoch  84 | 0.0584 s/epoch | train loss 0.0696\n",
      "| epoch  85 | 0.0591 s/epoch | train loss 0.0652\n",
      "| epoch  86 | 0.0602 s/epoch | train loss 0.0666\n",
      "| epoch  87 | 0.0604 s/epoch | train loss 0.0591\n",
      "| epoch  88 | 0.0609 s/epoch | train loss 0.0680\n",
      "| epoch  89 | 0.0596 s/epoch | train loss 0.0633\n",
      "| epoch  90 | 0.0596 s/epoch | train loss 0.0677\n",
      "| epoch  91 | 0.0591 s/epoch | train loss 0.0690\n",
      "| epoch  92 | 0.0601 s/epoch | train loss 0.0648\n",
      "| epoch  93 | 0.0600 s/epoch | train loss 0.0631\n",
      "| epoch  94 | 0.0600 s/epoch | train loss 0.0611\n",
      "| epoch  95 | 0.0594 s/epoch | train loss 0.0639\n",
      "| epoch  96 | 0.0608 s/epoch | train loss 0.0628\n",
      "| epoch  97 | 0.0590 s/epoch | train loss 0.0621\n",
      "| epoch  98 | 0.0599 s/epoch | train loss 0.0568\n",
      "| epoch  99 | 0.0598 s/epoch | train loss 0.0592\n",
      "| epoch 100 | 0.0597 s/epoch | train loss 0.0597\n",
      "| epoch 101 | 0.0601 s/epoch | train loss 0.0565\n",
      "| epoch 102 | 0.0595 s/epoch | train loss 0.0540\n",
      "| epoch 103 | 0.0594 s/epoch | train loss 0.0560\n",
      "| epoch 104 | 0.0602 s/epoch | train loss 0.0564\n",
      "| epoch 105 | 0.0603 s/epoch | train loss 0.0548\n",
      "| epoch 106 | 0.0589 s/epoch | train loss 0.0588\n",
      "| epoch 107 | 0.0605 s/epoch | train loss 0.0580\n",
      "| epoch 108 | 0.0598 s/epoch | train loss 0.0576\n",
      "| epoch 109 | 0.0590 s/epoch | train loss 0.0550\n",
      "| epoch 110 | 0.0603 s/epoch | train loss 0.0527\n",
      "| epoch 111 | 0.0602 s/epoch | train loss 0.0553\n",
      "| epoch 112 | 0.0596 s/epoch | train loss 0.0523\n",
      "| epoch 113 | 0.0598 s/epoch | train loss 0.0499\n",
      "| epoch 114 | 0.0604 s/epoch | train loss 0.0518\n",
      "| epoch 115 | 0.0608 s/epoch | train loss 0.0515\n",
      "| epoch 116 | 0.0600 s/epoch | train loss 0.0497\n",
      "| epoch 117 | 0.0612 s/epoch | train loss 0.0518\n",
      "| epoch 118 | 0.0590 s/epoch | train loss 0.0492\n",
      "| epoch 119 | 0.0607 s/epoch | train loss 0.0529\n",
      "| epoch 120 | 0.0602 s/epoch | train loss 0.0508\n",
      "| epoch 121 | 0.0605 s/epoch | train loss 0.0491\n",
      "| epoch 122 | 0.0590 s/epoch | train loss 0.0492\n",
      "| epoch 123 | 0.0598 s/epoch | train loss 0.0525\n",
      "| epoch 124 | 0.0594 s/epoch | train loss 0.0479\n",
      "| epoch 125 | 0.0609 s/epoch | train loss 0.0550\n",
      "| epoch 126 | 0.0596 s/epoch | train loss 0.0467\n",
      "| epoch 127 | 0.0596 s/epoch | train loss 0.0463\n",
      "| epoch 128 | 0.0601 s/epoch | train loss 0.0478\n",
      "| epoch 129 | 0.0588 s/epoch | train loss 0.0478\n",
      "| epoch 130 | 0.0578 s/epoch | train loss 0.0460\n",
      "| epoch 131 | 0.0594 s/epoch | train loss 0.0776\n",
      "| epoch 132 | 0.0587 s/epoch | train loss 0.0543\n",
      "| epoch 133 | 0.0586 s/epoch | train loss 0.0456\n",
      "| epoch 134 | 0.0604 s/epoch | train loss 0.0482\n",
      "| epoch 135 | 0.0601 s/epoch | train loss 0.0476\n",
      "| epoch 136 | 0.0596 s/epoch | train loss 0.0466\n",
      "| epoch 137 | 0.0592 s/epoch | train loss 0.0472\n",
      "| epoch 138 | 0.0593 s/epoch | train loss 0.0472\n",
      "| epoch 139 | 0.0610 s/epoch | train loss 0.0457\n",
      "| epoch 140 | 0.0595 s/epoch | train loss 0.0467\n",
      "| epoch 141 | 0.0596 s/epoch | train loss 0.0470\n",
      "| epoch 142 | 0.0588 s/epoch | train loss 0.0457\n",
      "| epoch 143 | 0.0596 s/epoch | train loss 0.0438\n",
      "| epoch 144 | 0.0597 s/epoch | train loss 0.0400\n",
      "| epoch 145 | 0.0590 s/epoch | train loss 0.0463\n",
      "| epoch 146 | 0.0598 s/epoch | train loss 0.0442\n",
      "| epoch 147 | 0.0606 s/epoch | train loss 0.0431\n",
      "| epoch 148 | 0.0595 s/epoch | train loss 0.0410\n",
      "| epoch 149 | 0.0590 s/epoch | train loss 0.0499\n",
      "| epoch 150 | 0.0591 s/epoch | train loss 0.0450\n",
      "| epoch 151 | 0.0594 s/epoch | train loss 0.0422\n",
      "| epoch 152 | 0.0594 s/epoch | train loss 0.0416\n",
      "| epoch 153 | 0.0595 s/epoch | train loss 0.0408\n",
      "| epoch 154 | 0.0585 s/epoch | train loss 0.0445\n",
      "| epoch 155 | 0.0594 s/epoch | train loss 0.0460\n",
      "| epoch 156 | 0.0582 s/epoch | train loss 0.0416\n",
      "| epoch 157 | 0.0605 s/epoch | train loss 0.0429\n",
      "| epoch 158 | 0.0606 s/epoch | train loss 0.0394\n",
      "| epoch 159 | 0.0596 s/epoch | train loss 0.0396\n",
      "| epoch 160 | 0.0596 s/epoch | train loss 0.0398\n",
      "| epoch 161 | 0.0586 s/epoch | train loss 0.0412\n",
      "| epoch 162 | 0.0605 s/epoch | train loss 0.0366\n",
      "| epoch 163 | 0.0588 s/epoch | train loss 0.0397\n",
      "| epoch 164 | 0.0597 s/epoch | train loss 0.0371\n",
      "| epoch 165 | 0.0583 s/epoch | train loss 0.0363\n",
      "| epoch 166 | 0.0597 s/epoch | train loss 0.0383\n",
      "| epoch 167 | 0.0583 s/epoch | train loss 0.0413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 168 | 0.0593 s/epoch | train loss 0.0376\n",
      "| epoch 169 | 0.0599 s/epoch | train loss 0.0401\n",
      "| epoch 170 | 0.0593 s/epoch | train loss 0.0385\n",
      "| epoch 171 | 0.0594 s/epoch | train loss 0.0357\n",
      "| epoch 172 | 0.0612 s/epoch | train loss 0.0367\n",
      "| epoch 173 | 0.0601 s/epoch | train loss 0.0398\n",
      "| epoch 174 | 0.0689 s/epoch | train loss 0.0366\n",
      "| epoch 175 | 0.0818 s/epoch | train loss 0.0383\n",
      "| epoch 176 | 0.0742 s/epoch | train loss 0.0432\n",
      "| epoch 177 | 0.0587 s/epoch | train loss 0.0298\n",
      "| epoch 178 | 0.0588 s/epoch | train loss 0.0368\n",
      "| epoch 179 | 0.0597 s/epoch | train loss 0.0380\n",
      "| epoch 180 | 0.0586 s/epoch | train loss 0.0342\n",
      "| epoch 181 | 0.0596 s/epoch | train loss 0.0318\n",
      "| epoch 182 | 0.0607 s/epoch | train loss 0.0392\n",
      "| epoch 183 | 0.0607 s/epoch | train loss 0.0346\n",
      "| epoch 184 | 0.0614 s/epoch | train loss 0.0332\n",
      "| epoch 185 | 0.0605 s/epoch | train loss 0.0343\n",
      "| epoch 186 | 0.0611 s/epoch | train loss 0.0352\n",
      "| epoch 187 | 0.0596 s/epoch | train loss 0.0345\n",
      "| epoch 188 | 0.0594 s/epoch | train loss 0.0368\n",
      "| epoch 189 | 0.0595 s/epoch | train loss 0.0388\n",
      "| epoch 190 | 0.0616 s/epoch | train loss 0.0290\n",
      "| epoch 191 | 0.0604 s/epoch | train loss 0.0361\n",
      "| epoch 192 | 0.0604 s/epoch | train loss 0.0347\n",
      "| epoch 193 | 0.0610 s/epoch | train loss 0.0378\n",
      "| epoch 194 | 0.0597 s/epoch | train loss 0.0375\n",
      "| epoch 195 | 0.0605 s/epoch | train loss 0.0288\n",
      "| epoch 196 | 0.0589 s/epoch | train loss 0.0328\n",
      "| epoch 197 | 0.0590 s/epoch | train loss 0.0339\n",
      "| epoch 198 | 0.0600 s/epoch | train loss 0.0325\n",
      "| epoch 199 | 0.0600 s/epoch | train loss 0.0368\n",
      "Train Acc:0.98701 Train Rec:0.98359 Train Precis:0.98983 Train F1-score:0.98670"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28848\\3651529419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;31m#testing_teacher(testloader,optimizer_teacher,criterion,model_teacher)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0mtraining_student\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer_student\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_student\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;31m#testing_student(testloader,optimizer_student,criterion,model_student)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28848\\3239091287.py\u001b[0m in \u001b[0;36mtraining_student\u001b[1;34m(trainloader, optimizer, criterion, model, testloader)\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;31m# Compute loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[0mtest_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28848\\663355340.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhs1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbneck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhs2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28848\\3795835952.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mse\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28848\\3795835952.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%% Evaluation\n",
    "def evaluation(pred,target):\n",
    "    acu = accuracy_score(pred, target)\n",
    "    rec = recall_score(pred, target)\n",
    "    pre = precision_score(pred, target)\n",
    "    f1 = f1_score(pred, target)  \n",
    "    return np.array([acu, rec, pre, f1])\n",
    "\n",
    "#%% Train\n",
    "Accuracy_teacher = []\n",
    "Recall_teacher = []\n",
    "Precision_teacher = []\n",
    "F1score_teacher = []\n",
    "\n",
    "Accuracy_student = []\n",
    "Recall_student = []\n",
    "Precision_student = []\n",
    "F1score_student = []\n",
    "\n",
    "\n",
    "Accuracy_teacher_student = []\n",
    "Recall_teacher_student = []\n",
    "Precision_teacher_student = []\n",
    "F1score_teacher_student = []\n",
    "\n",
    "\n",
    "teacher_train_preds = []\n",
    "teacher_train_gt = []\n",
    "\n",
    "\n",
    "student_train_preds = []\n",
    "student_train_gt = []\n",
    "\n",
    "\n",
    "\n",
    "#for i in range(Average_times):\n",
    "lis = [1,2,3,4,5,7,8,9,10,11,12,13,14,15,16,18,19,21,22,23]\n",
    "for sub in lis:\n",
    "    print(\"\\nSubject\",sub)\n",
    "    test = (sliding_waist_subject == sub)\n",
    "    train = ~test\n",
    "\n",
    "    norm_sliding_waist_data = []\n",
    "    norm_sliding_waist_test = []\n",
    "    min_norm_value = []\n",
    "    min_norm_index = []\n",
    "    max_norm_value = []\n",
    "    max_norm_index = []\n",
    "    hori_sliding_waist_data = []\n",
    "    hori_sliding_waist_test = []\n",
    "    sliding_waist_data_hori = sliding_waist_data\n",
    "    sliding_waist_data_hori[:,:,0] = sliding_waist_data_hori[:,:,0]*0\n",
    "    min_hori_value = []\n",
    "    min_hori_index = []\n",
    "    max_hori_value = []\n",
    "    max_hori_index = []\n",
    "    \n",
    "    \n",
    "    X_train = sliding_waist_data[train]\n",
    "    Y_train_gd = sliding_waist_label[train]\n",
    "    X_test = sliding_waist_data[test]\n",
    "    Y_test_gd = sliding_waist_label[test]\n",
    "\n",
    "    X_train_hori = sliding_waist_data_hori[train]\n",
    "    X_test_hori = sliding_waist_data_hori[test]\n",
    "\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        c1 = np.linalg.norm(X_train[i].astype('float32'), axis=1)\n",
    "        norm_sliding_waist_data.append(c1)\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        c2 = np.linalg.norm(X_test[i].astype('float32'), axis=1)\n",
    "        norm_sliding_waist_test.append(c2)\n",
    "\n",
    "    for i in range(len(X_train_hori)):\n",
    "        c1 = np.linalg.norm(X_train_hori[i].astype('float32'), axis=1)\n",
    "        hori_sliding_waist_data.append(c1)\n",
    "\n",
    "    for i in range(len(X_test_hori)):\n",
    "        c2 = np.linalg.norm(X_test_hori[i].astype('float32'), axis=1)\n",
    "        hori_sliding_waist_test.append(c2)\n",
    "\n",
    "    \n",
    "    for i in range(len(Y_train_gd)):\n",
    "        if Y_train_gd[i] ==1:\n",
    "            d1 = np.min(norm_sliding_waist_data[i])\n",
    "            min_norm_value.append(d1)\n",
    "            d2 = np.argmin(norm_sliding_waist_data[i])\n",
    "            min_norm_index.append(d2)\n",
    "\n",
    "            d3 = np.min(hori_sliding_waist_data[i])\n",
    "            min_hori_value.append(d3)\n",
    "            d4 = np.argmin(hori_sliding_waist_data[i])\n",
    "            min_hori_index.append(d4)\n",
    "            \n",
    "        else:\n",
    "            if len(norm_sliding_waist_data[i]) > 0:\n",
    "                e1 = np.max(norm_sliding_waist_data[i])\n",
    "                max_norm_value.append(e1)\n",
    "                e2 = np.argmax(norm_sliding_waist_data[i])\n",
    "                max_norm_index.append(e2)\n",
    "\n",
    "                e3 = np.max(hori_sliding_waist_data[i])\n",
    "                max_hori_value.append(e3)\n",
    "                e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "                max_hori_index.append(e4)\n",
    "            else:\n",
    "                e1 = 0\n",
    "                max_norm_value.append(e1)\n",
    "                e2 = 0\n",
    "                max_norm_index.append(e2)\n",
    "\n",
    "                e3 = np.max(hori_sliding_waist_data[i])\n",
    "                max_hori_value.append(e3)\n",
    "                e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "                max_hori_index.append(e4)\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "    MIN_train = np.min(min_norm_value)\n",
    "    MAX_train = np.max(max_norm_value)\n",
    "\n",
    "    MIN_train_hori = np.min(min_hori_value)\n",
    "    MAX_train_hori = np.max(max_hori_value)\n",
    "\n",
    "    unidentified_data = []\n",
    "    unidentified_label = []\n",
    "\n",
    "    for i in range(len(norm_sliding_waist_test)):\n",
    "       if max(norm_sliding_waist_test[i]) > MIN_train and max(norm_sliding_waist_test[i]) < MAX_train\\\n",
    "          and max(hori_sliding_waist_test[i]) > MIN_train_hori and max(hori_sliding_waist_test[i]) < MAX_train_hori:\n",
    "            unidentified_data.append(X_test[i])\n",
    "            unidentified_label.append(Y_test_gd[i])\n",
    "\n",
    "    unidentified_data = np.array(unidentified_data)\n",
    "    unidentified_label = np.array(unidentified_label)\n",
    "\n",
    "    #breakpoint() # insert breakpoint   \n",
    "    \n",
    "    # Initializes the train and validation dataset in Torch format\n",
    "    x_train_tensor = torch.from_numpy(X_train.astype('float32')).to(device) \n",
    "    x_train_tensor = x_train_tensor.reshape([-1,1,14,3]) \n",
    "    x_test_tensor = torch.from_numpy(unidentified_data.astype('float32')).to(device)\n",
    "    x_test_tensor = x_test_tensor.reshape([-1,1,14,3])\n",
    "    \n",
    "    y_train_gd_tensor =  torch.from_numpy(Y_train_gd).to(device)\n",
    "    y_test_gd_tensor =  torch.from_numpy(unidentified_label).to(device)\n",
    "    \n",
    "    # Dataset wrapping tensors.Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "    deal_dataset = TensorDataset(x_train_tensor, y_train_gd_tensor) \n",
    "    test_dataset = TensorDataset(x_test_tensor,y_test_gd_tensor)\n",
    "    \n",
    "    config['window_size'] = X_train.shape[1]\n",
    "    config['nb_channels'] = X_train.shape[2]\n",
    "    config['nb_classes'] = 2\n",
    "    \n",
    "    # Sends network to the GPU and sets it to training mode\n",
    "    ResNet = Net_Teacher(block, [2, 2, 2, 2], 1, 2)\n",
    "    model_teacher = ResNet.to(device) \n",
    "    model_teacher.train()\n",
    "    \n",
    "    model_student = MobileNetV3_Small().to(device)\n",
    "    #model_student = Net_Student(config).to(device) \n",
    "    model_student.train()\n",
    "    \n",
    "    # DataLoader represents a Python iterable over a dataset\n",
    "    trainloader = DataLoader(deal_dataset, batch_size=config['batch_size'],shuffle=True) \n",
    "    testloader = DataLoader(test_dataset, batch_size=config['batch_size'],shuffle=False)\n",
    "    # Initialize the optimizer and loss\n",
    "    optimizer_teacher = torch.optim.Adam(model_teacher.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    optimizer_student = torch.optim.Adam(model_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Start training and testing Teacher and Student Model\n",
    "    training_teacher(trainloader,optimizer_teacher,criterion,model_teacher,testloader)\n",
    "    #testing_teacher(testloader,optimizer_teacher,criterion,model_teacher)\n",
    "  \n",
    "    training_student(trainloader,optimizer_student,criterion,model_student,testloader)    \n",
    "    #testing_student(testloader,optimizer_student,criterion,model_student)\n",
    "\n",
    "#print(teacher_train_preds)\n",
    "#print(teacher_train_gt)\n",
    "Teacher_eval_table = evaluation(teacher_train_preds, teacher_train_gt)\n",
    "\n",
    "Student_eval_table = evaluation(student_train_preds, student_train_gt)\n",
    "\n",
    "show_CM_teacher(teacher_train_preds, teacher_train_gt)\n",
    "show_CM_student(student_train_preds, student_train_gt)\n",
    "\n",
    "print(\"Teacher1(RestNet50)_Acc:\",Teacher_eval_table[0])\n",
    "print(\"Teacher1(RestNet50)_Rec:\",Teacher_eval_table[1])\n",
    "print(\"Teacher1(RestNet50)_Pre:\",Teacher_eval_table[2])\n",
    "print(\"Teacher1(RestNet50)_F1:\",Teacher_eval_table[3])\n",
    "\n",
    "print(\"Original_Student1(CNN_Small)_Acc:\",Student_eval_table[0])\n",
    "print(\"Original_Student1(CNN_Small)_Rec:\",Student_eval_table[1])\n",
    "print(\"Original_Student1(CNN_Small)_Pre:\",Student_eval_table[2])\n",
    "print(\"Original_Student1(CNN_Small)_F1:\",Student_eval_table[3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer 1 Training teacher\n",
    "teacher_student_train_preds = []\n",
    "teacher_student_train_gt = []\n",
    "\n",
    "\n",
    "for sub in lis:\n",
    "    print(\"\\nSubject\",sub)\n",
    "    test = (sliding_waist_subject == sub)\n",
    "    train = ~test\n",
    "\n",
    "    norm_sliding_waist_data = []\n",
    "    norm_sliding_waist_test = []\n",
    "    min_norm_value = []\n",
    "    min_norm_index = []\n",
    "    max_norm_value = []\n",
    "    max_norm_index = []\n",
    "    hori_sliding_waist_data = []\n",
    "    hori_sliding_waist_test = []\n",
    "    sliding_waist_data_hori = sliding_waist_data\n",
    "    sliding_waist_data_hori[:,:,0] = sliding_waist_data_hori[:,:,0]*0\n",
    "    min_hori_value = []\n",
    "    min_hori_index = []\n",
    "    max_hori_value = []\n",
    "    max_hori_index = []\n",
    "    \n",
    "    X_train = sliding_waist_data[train]\n",
    "    Y_train_gd = sliding_waist_label[train]\n",
    "    X_test = sliding_waist_data[test]\n",
    "    Y_test_gd = sliding_waist_label[test]\n",
    "\n",
    "    X_train_hori = sliding_waist_data_hori[train]\n",
    "    X_test_hori = sliding_waist_data_hori[test]\n",
    "\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        c1 = np.linalg.norm(X_train[i].astype('float32'), axis=1)\n",
    "        norm_sliding_waist_data.append(c1)\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        c2 = np.linalg.norm(X_test[i].astype('float32'), axis=1)\n",
    "        norm_sliding_waist_test.append(c2)\n",
    "\n",
    "    for i in range(len(X_train_hori)):\n",
    "        c1 = np.linalg.norm(X_train_hori[i].astype('float32'), axis=1)\n",
    "        hori_sliding_waist_data.append(c1)\n",
    "\n",
    "    for i in range(len(X_test_hori)):\n",
    "        c2 = np.linalg.norm(X_test_hori[i].astype('float32'), axis=1)\n",
    "        hori_sliding_waist_test.append(c2)\n",
    "\n",
    "    \n",
    "    for i in range(len(Y_train_gd)):\n",
    "        if Y_train_gd[i] ==1:\n",
    "            d1 = np.min(norm_sliding_waist_data[i])\n",
    "            min_norm_value.append(d1)\n",
    "            d2 = np.argmin(norm_sliding_waist_data[i])\n",
    "            min_norm_index.append(d2)\n",
    "\n",
    "            d3 = np.min(hori_sliding_waist_data[i])\n",
    "            min_hori_value.append(d3)\n",
    "            d4 = np.argmin(hori_sliding_waist_data[i])\n",
    "            min_hori_index.append(d4)\n",
    "            \n",
    "        else:\n",
    "            e1 = np.max(norm_sliding_waist_data[i])\n",
    "            max_norm_value.append(e1)\n",
    "            e2 = np.argmax(norm_sliding_waist_data[i])\n",
    "            max_norm_index.append(e2)\n",
    "\n",
    "            e3 = np.max(hori_sliding_waist_data[i])\n",
    "            max_hori_value.append(e3)\n",
    "            e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "            max_hori_index.append(e4)\n",
    "        \n",
    "\n",
    "    MIN_train = np.min(min_norm_value)\n",
    "    MAX_train = np.max(max_norm_value)\n",
    "\n",
    "    MIN_train_hori = np.min(min_hori_value)\n",
    "    MAX_train_hori = np.max(max_hori_value)\n",
    "\n",
    "    unidentified_data = []\n",
    "    unidentified_label = []\n",
    "\n",
    "    for i in range(len(norm_sliding_waist_test)):\n",
    "       if max(norm_sliding_waist_test[i]) > MIN_train and max(norm_sliding_waist_test[i]) < MAX_train\\\n",
    "          and max(hori_sliding_waist_test[i]) > MIN_train_hori and max(hori_sliding_waist_test[i]) < MAX_train_hori:\n",
    "            unidentified_data.append(X_test[i])\n",
    "            unidentified_label.append(Y_test_gd[i])\n",
    "\n",
    "    unidentified_data = np.array(unidentified_data)\n",
    "    unidentified_label = np.array(unidentified_label)\n",
    "\n",
    "    #breakpoint() # insert breakpoint   \n",
    "    \n",
    "    # Initializes the train and validation dataset in Torch format\n",
    "    x_train_tensor = torch.from_numpy(X_train.astype('float32')).to(device) \n",
    "    x_train_tensor = x_train_tensor.reshape([-1,1,14,3]) \n",
    "    x_test_tensor = torch.from_numpy(unidentified_data.astype('float32')).to(device)\n",
    "    x_test_tensor = x_test_tensor.reshape([-1,1,14,3])\n",
    "    \n",
    "    y_train_gd_tensor =  torch.from_numpy(Y_train_gd).to(device)\n",
    "    y_test_gd_tensor =  torch.from_numpy(unidentified_label).to(device)\n",
    "    \n",
    "    # Dataset wrapping tensors.Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "    deal_dataset = TensorDataset(x_train_tensor, y_train_gd_tensor) \n",
    "    test_dataset = TensorDataset(x_test_tensor,y_test_gd_tensor)\n",
    "    \n",
    "    config['window_size'] = X_train.shape[1]\n",
    "    config['nb_channels'] = X_train.shape[2]\n",
    "    config['nb_classes'] = 2\n",
    "    \n",
    "    # Sends network to the GPU and sets it to training mode\n",
    "\n",
    "    model_teacher_student = MobileNetV3_Small_KD().to(device) \n",
    "    model_teacher_student.train() \n",
    "    \n",
    "    # DataLoader represents a Python iterable over a dataset\n",
    "    trainloader = DataLoader(deal_dataset, batch_size=config['batch_size'],shuffle=True) \n",
    "    testloader = DataLoader(test_dataset, batch_size=config['batch_size'],shuffle=False)\n",
    "    # Initialize the optimizer and loss\n",
    "    optimizer_teacher_student = torch.optim.Adam(model_teacher_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Establishing Teacher-Student Model\n",
    "    alpha = 0.5\n",
    "    Layer = False\n",
    "    training_teacher_student(trainloader,optimizer_teacher_student,criterion,model_teacher,model_teacher_student,alpha,testloader,Layer,sub)\n",
    "\n",
    "#save(config['epochs'], teacher_student_model, teacher_student_optimizer, teacher_student_loss)\n",
    "show_CM_teacher_student(teacher_student_train_preds, teacher_student_train_gt)\n",
    "teacher_Student_eval_table = evaluation(teacher_student_train_preds, teacher_student_train_gt)    \n",
    "\n",
    "print(\"Distillation_teacher(Mobilnet)_Acc:\",teacher_Student_eval_table[0])\n",
    "print(\"Distillation_teacher(Mobilnet)_Rec:\",teacher_Student_eval_table[1])\n",
    "print(\"Distillation_teacher(Mobilnet)_Pre:\",teacher_Student_eval_table[2])\n",
    "print(\"Distillation_teacher(Mobilnet)_F1:\",teacher_Student_eval_table[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742fa944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer 2 Training Student\n",
    "# Distillation_teacher_model = None\n",
    "Distillation_teacher_model = load_checkpoint(model_teacher_student, PATH)\n",
    "\n",
    "Distillation_student_train_preds = []\n",
    "Distillation_student_train_gt = []\n",
    "\n",
    "for sub in lis:\n",
    "    print(\"\\nSubject\",sub)\n",
    "    test = (sliding_waist_subject == sub)\n",
    "    train = ~test\n",
    "\n",
    "    norm_sliding_waist_data = []\n",
    "    norm_sliding_waist_test = []\n",
    "    min_norm_value = []\n",
    "    min_norm_index = []\n",
    "    max_norm_value = []\n",
    "    max_norm_index = []\n",
    "    hori_sliding_waist_data = []\n",
    "    hori_sliding_waist_test = []\n",
    "    sliding_waist_data_hori = sliding_waist_data\n",
    "    sliding_waist_data_hori[:,:,0] = sliding_waist_data_hori[:,:,0]*0\n",
    "    min_hori_value = []\n",
    "    min_hori_index = []\n",
    "    max_hori_value = []\n",
    "    max_hori_index = []\n",
    "    \n",
    "    X_train = sliding_waist_data[train]\n",
    "    Y_train_gd = sliding_waist_label[train]\n",
    "    X_test = sliding_waist_data[test]\n",
    "    Y_test_gd = sliding_waist_label[test]\n",
    "\n",
    "    X_train_hori = sliding_waist_data_hori[train]\n",
    "    X_test_hori = sliding_waist_data_hori[test]\n",
    "\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        c1 = np.linalg.norm(X_train[i].astype('float32'), axis=1)\n",
    "        norm_sliding_waist_data.append(c1)\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        c2 = np.linalg.norm(X_test[i].astype('float32'), axis=1)\n",
    "        norm_sliding_waist_test.append(c2)\n",
    "\n",
    "    for i in range(len(X_train_hori)):\n",
    "        c1 = np.linalg.norm(X_train_hori[i].astype('float32'), axis=1)\n",
    "        hori_sliding_waist_data.append(c1)\n",
    "\n",
    "    for i in range(len(X_test_hori)):\n",
    "        c2 = np.linalg.norm(X_test_hori[i].astype('float32'), axis=1)\n",
    "        hori_sliding_waist_test.append(c2)\n",
    "\n",
    "    \n",
    "    for i in range(len(Y_train_gd)):\n",
    "        if Y_train_gd[i] ==1:\n",
    "            d1 = np.min(norm_sliding_waist_data[i])\n",
    "            min_norm_value.append(d1)\n",
    "            d2 = np.argmin(norm_sliding_waist_data[i])\n",
    "            min_norm_index.append(d2)\n",
    "\n",
    "            d3 = np.min(hori_sliding_waist_data[i])\n",
    "            min_hori_value.append(d3)\n",
    "            d4 = np.argmin(hori_sliding_waist_data[i])\n",
    "            min_hori_index.append(d4)\n",
    "            \n",
    "        else:\n",
    "            e1 = np.max(norm_sliding_waist_data[i])\n",
    "            max_norm_value.append(e1)\n",
    "            e2 = np.argmax(norm_sliding_waist_data[i])\n",
    "            max_norm_index.append(e2)\n",
    "\n",
    "            e3 = np.max(hori_sliding_waist_data[i])\n",
    "            max_hori_value.append(e3)\n",
    "            e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "            max_hori_index.append(e4)\n",
    "        \n",
    "\n",
    "    MIN_train = np.min(min_norm_value)\n",
    "    MAX_train = np.max(max_norm_value)\n",
    "\n",
    "    MIN_train_hori = np.min(min_hori_value)\n",
    "    MAX_train_hori = np.max(max_hori_value)\n",
    "\n",
    "    unidentified_data = []\n",
    "    unidentified_label = []\n",
    "\n",
    "    for i in range(len(norm_sliding_waist_test)):\n",
    "       if max(norm_sliding_waist_test[i]) > MIN_train and max(norm_sliding_waist_test[i]) < MAX_train\\\n",
    "          and max(hori_sliding_waist_test[i]) > MIN_train_hori and max(hori_sliding_waist_test[i]) < MAX_train_hori:\n",
    "            unidentified_data.append(X_test[i])\n",
    "            unidentified_label.append(Y_test_gd[i])\n",
    "\n",
    "    unidentified_data = np.array(unidentified_data)\n",
    "    unidentified_label = np.array(unidentified_label)\n",
    "\n",
    "    #breakpoint() # insert breakpoint   \n",
    "    \n",
    "    # Initializes the train and validation dataset in Torch format\n",
    "    x_train_tensor = torch.from_numpy(X_train.astype('float32')).to(device) \n",
    "    x_train_tensor = x_train_tensor.reshape([-1,1,14,3]) \n",
    "    x_test_tensor = torch.from_numpy(unidentified_data.astype('float32')).to(device)\n",
    "    x_test_tensor = x_test_tensor.reshape([-1,1,14,3])\n",
    "    \n",
    "    y_train_gd_tensor =  torch.from_numpy(Y_train_gd).to(device)\n",
    "    y_test_gd_tensor =  torch.from_numpy(unidentified_label).to(device)\n",
    "    \n",
    "    # Dataset wrapping tensors.Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "    deal_dataset = TensorDataset(x_train_tensor, y_train_gd_tensor) \n",
    "    test_dataset = TensorDataset(x_test_tensor,y_test_gd_tensor)\n",
    "    \n",
    "    config['window_size'] = X_train.shape[1]\n",
    "    config['nb_channels'] = X_train.shape[2]\n",
    "    config['nb_classes'] = 2\n",
    "    \n",
    "    # Sends network to the GPU and sets it to training mode\n",
    "    model_Distillation_student = MobileNetV3_Small_KD().to(device)\n",
    "    #model_Distillation_student = Net_Teacher_Student(config).to(device) \n",
    "    model_Distillation_student.train() \n",
    "    \n",
    "    # DataLoader represents a Python iterable over a dataset\n",
    "    trainloader = DataLoader(deal_dataset, batch_size=config['batch_size'],shuffle=True) \n",
    "    testloader = DataLoader(test_dataset, batch_size=config['batch_size'],shuffle=False)\n",
    "    # Initialize the optimizer and loss\n",
    "    optimizer_distillation_student = torch.optim.Adam(model_Distillation_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Establishing Teacher-Student Model\n",
    "    alpha = 0.5\n",
    "    Layer = True\n",
    "    training_distillation_student(trainloader,optimizer_distillation_student,criterion,Distillation_teacher_model,model_Distillation_student,alpha,testloader,Layer,sub)\n",
    "    #testing_teacher_student(testloader,optimizer_teacher_student,criterion,model_teacher,model_teacher_student)\n",
    "\n",
    "show_CM_teacher_student(Distillation_student_train_preds, Distillation_student_train_gt)\n",
    "teacher_Student_eval_table = evaluation(Distillation_student_train_preds, Distillation_student_train_gt)     \n",
    "\n",
    "print(\"Distillation_student(CNNSmall)_Acc:\",teacher_Student_eval_table[0])\n",
    "print(\"Distillation_student(CNNSmall)_Rec:\",teacher_Student_eval_table[1])\n",
    "print(\"Distillation_student(CNNSmall)_Pre:\",teacher_Student_eval_table[2])\n",
    "print(\"Distillation_student(CNNSmall)_F1:\",teacher_Student_eval_table[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Testing\n",
    "#for i in range(Average_times):\n",
    "Distillation_student_train_preds=[]\n",
    "Distillation_student_train_gt=[]\n",
    "teacher_student_train_preds=[]\n",
    "teacher_student_train_gt=[]\n",
    "teacher_train_preds=[]\n",
    "teacher_train_gt=[]\n",
    "lis = [1,2,3,4,5,7,8,9,10,11,12,13,14,15,16,18,19,21,22,23]\n",
    "TH=0.92\n",
    "op=True\n",
    "# while op:\n",
    "for 1 in 10:\n",
    "    for sub in lis:\n",
    "        if(sub==15):\n",
    "            print(\"\\nSubject\",sub)\n",
    "        test = (sliding_waist_subject == sub)\n",
    "        train = ~test\n",
    "\n",
    "        norm_sliding_waist_data = []\n",
    "        norm_sliding_waist_test = []\n",
    "        min_norm_value = []\n",
    "        min_norm_index = []\n",
    "        max_norm_value = []\n",
    "        max_norm_index = []\n",
    "        hori_sliding_waist_data = []\n",
    "        hori_sliding_waist_test = []\n",
    "        sliding_waist_data_hori = sliding_waist_data\n",
    "        sliding_waist_data_hori[:,:,0] = sliding_waist_data_hori[:,:,0]*0\n",
    "        min_hori_value = []\n",
    "        min_hori_index = []\n",
    "        max_hori_value = []\n",
    "        max_hori_index = []\n",
    "\n",
    "\n",
    "        X_train = sliding_waist_data[train]\n",
    "        Y_train_gd = sliding_waist_label[train]\n",
    "        X_test = sliding_waist_data[test]\n",
    "        Y_test_gd = sliding_waist_label[test]\n",
    "\n",
    "        X_train_hori = sliding_waist_data_hori[train]\n",
    "        X_test_hori = sliding_waist_data_hori[test]\n",
    "\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            c1 = np.linalg.norm(X_train[i].astype('float32'), axis=1)\n",
    "            norm_sliding_waist_data.append(c1)\n",
    "\n",
    "        for i in range(len(X_test)):\n",
    "            c2 = np.linalg.norm(X_test[i].astype('float32'), axis=1)\n",
    "            norm_sliding_waist_test.append(c2)\n",
    "\n",
    "        for i in range(len(X_train_hori)):\n",
    "            c1 = np.linalg.norm(X_train_hori[i].astype('float32'), axis=1)\n",
    "            hori_sliding_waist_data.append(c1)\n",
    "\n",
    "        for i in range(len(X_test_hori)):\n",
    "            c2 = np.linalg.norm(X_test_hori[i].astype('float32'), axis=1)\n",
    "            hori_sliding_waist_test.append(c2)\n",
    "\n",
    "\n",
    "        for i in range(len(Y_train_gd)):\n",
    "            if Y_train_gd[i] ==1:\n",
    "                d1 = np.min(norm_sliding_waist_data[i])\n",
    "                min_norm_value.append(d1)\n",
    "                d2 = np.argmin(norm_sliding_waist_data[i])\n",
    "                min_norm_index.append(d2)\n",
    "\n",
    "                d3 = np.min(hori_sliding_waist_data[i])\n",
    "                min_hori_value.append(d3)\n",
    "                d4 = np.argmin(hori_sliding_waist_data[i])\n",
    "                min_hori_index.append(d4)\n",
    "\n",
    "            else:\n",
    "                e1 = np.max(norm_sliding_waist_data[i])\n",
    "                max_norm_value.append(e1)\n",
    "                e2 = np.argmax(norm_sliding_waist_data[i])\n",
    "                max_norm_index.append(e2)\n",
    "\n",
    "                e3 = np.max(hori_sliding_waist_data[i])\n",
    "                max_hori_value.append(e3)\n",
    "                e4 = np.argmax(hori_sliding_waist_data[i])\n",
    "                max_hori_index.append(e4)\n",
    "\n",
    "\n",
    "\n",
    "        MIN_train = np.min(min_norm_value)\n",
    "        MAX_train = np.max(max_norm_value)\n",
    "\n",
    "        MIN_train_hori = np.min(min_hori_value)\n",
    "        MAX_train_hori = np.max(max_hori_value)\n",
    "\n",
    "        unidentified_data = []\n",
    "        unidentified_label = []\n",
    "\n",
    "        for i in range(len(norm_sliding_waist_test)):\n",
    "           if max(norm_sliding_waist_test[i]) > MIN_train and max(norm_sliding_waist_test[i]) < MAX_train\\\n",
    "              and max(hori_sliding_waist_test[i]) > MIN_train_hori and max(hori_sliding_waist_test[i]) < MAX_train_hori:\n",
    "                unidentified_data.append(X_test[i])\n",
    "                unidentified_label.append(Y_test_gd[i])\n",
    "\n",
    "        unidentified_data = np.array(unidentified_data)\n",
    "        unidentified_label = np.array(unidentified_label)\n",
    "\n",
    "        #breakpoint() # insert breakpoint   \n",
    "\n",
    "        # Initializes the train and validation dataset in Torch format\n",
    "        x_train_tensor = torch.from_numpy(X_train.astype('float32')).to(device) \n",
    "        x_train_tensor = x_train_tensor.reshape([-1,1,14,3]) \n",
    "        x_test_tensor = torch.from_numpy(unidentified_data.astype('float32')).to(device)\n",
    "        x_test_tensor = x_test_tensor.reshape([-1,1,14,3])\n",
    "\n",
    "        y_train_gd_tensor =  torch.from_numpy(Y_train_gd).to(device)\n",
    "        y_test_gd_tensor =  torch.from_numpy(unidentified_label).to(device)\n",
    "\n",
    "        # Dataset wrapping tensors.Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "        deal_dataset = TensorDataset(x_train_tensor, y_train_gd_tensor) \n",
    "        test_dataset = TensorDataset(x_test_tensor,y_test_gd_tensor)\n",
    "\n",
    "        config['window_size'] = X_train.shape[1]\n",
    "        config['nb_channels'] = X_train.shape[2]\n",
    "        config['nb_classes'] = 2\n",
    "\n",
    "        # Sends network to the GPU and sets it to training mode\n",
    "    #     ResNet = Net_Teacher(block, [2, 2, 2, 2], 1, 2)\n",
    "    #     model_teacher = ResNet.to(device) \n",
    "    #     model_teacher.train()\n",
    "\n",
    "    #     model_student = Net_Student(config).to(device) \n",
    "    #     model_student.train()\n",
    "\n",
    "        # DataLoader represents a Python iterable over a dataset\n",
    "        #trainloader = DataLoader(deal_dataset, batch_size=config['batch_size'],shuffle=True) \n",
    "        testloader = DataLoader(test_dataset, batch_size=config['batch_size'],shuffle=False)\n",
    "    #     # Initialize the optimizer and loss\n",
    "        optimizer_teacher = torch.optim.Adam(model_teacher.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    #     optimizer_student = torch.optim.Adam(model_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "        # Start training and testing Teacher and Student Model\n",
    "        #training_teacher(trainloader,optimizer_teacher,criterion,model_teacher,testloader)    \n",
    "        #testing_teacher(testloader,optimizer_teacher,criterion,model_teacher)\n",
    "\n",
    "        #training_student(trainloader,optimizer_student,criterion,model_student,testloader)\n",
    "\n",
    "        alpha=0.5\n",
    "\n",
    "        optimizer_distillation_student = torch.optim.Adam(model_Distillation_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "        testing_Distillation_student(testloader,optimizer_distillation_student,criterion,Distillation_teacher_model,model_Distillation_student,alpha,TH)\n",
    "\n",
    "        if(sub==23):\n",
    "            teacher_Student_eval_table = evaluation(Distillation_student_train_preds, Distillation_student_train_gt)             \n",
    "            print('Distillation_student_Acc:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[0],TH))\n",
    "            print('Distillation_student_Rec:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[1],TH))\n",
    "            print('Distillation_student_Pre:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[2],TH))\n",
    "            print('Distillation_student_F1:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[3],TH))\n",
    "            show_CM_teacher_student(Distillation_student_train_preds, Distillation_student_train_gt) \n",
    "            Distillation_student_train_preds=[]\n",
    "            Distillation_student_train_gt=[]\n",
    "\n",
    "        if(teacher_Student_eval_table[0]<0.9):\n",
    "            optimizer_teacher_student = torch.optim.Adam(model_teacher_student.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "            testing_teacher_student(testloader,optimizer_teacher_student,criterion,model_teacher,model_teacher_student,alpha,TH)\n",
    "\n",
    "            if(sub==23):\n",
    "                teacher_Student_eval_table = evaluation(teacher_student_train_preds, teacher_student_train_gt)\n",
    "                print('Teacher_Student_Acc:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[0], TH))\n",
    "                print('Teacher_Student_Rec:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[1], TH))\n",
    "                print('Teacher_Student_Pre:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[2], TH))\n",
    "                print('Teacher_Student_F1:{:5.4f} | TH:{:5.4f}'.format(teacher_Student_eval_table[3], TH))\n",
    "                show_CM_teacher_student(teacher_student_train_preds, teacher_student_train_gt)   \n",
    "                teacher_student_train_preds=[]\n",
    "                teacher_student_train_gt=[]\n",
    "\n",
    "                if(teacher_Student_eval_table[0]<0.9):\n",
    "                    testing_teacher(testloader,optimizer_teacher,criterion,model_teacher,TH)\n",
    "\n",
    "                    if(sub==23):\n",
    "                        Teacher_eval_table = evaluation(teacher_train_preds, teacher_train_gt)\n",
    "                        print('Teacher_Acc:{:5.4f} | TH:{:5.4f}'.format(Teacher_eval_table[0], TH))\n",
    "                        print('Teacher_Rec:{:5.4f} | TH:{:5.4f}'.format(Teacher_eval_table[1], TH))\n",
    "                        print('Teacher_Pre:{:5.4f} | TH:{:5.4f}'.format(Teacher_eval_table[2], TH))\n",
    "                        print('Teacher_F1:{:5.4f} | TH:{:5.4f}'.format(Teacher_eval_table[3], TH))\n",
    "                        show_CM_teacher(teacher_train_preds, teacher_train_gt)\n",
    "                        teacher_train_preds=[]\n",
    "                        teacher_train_gt=[]\n",
    "#     TH+=0.01            \n",
    "#     if(TH==1.01):\n",
    "#         op=False\n",
    "            \n",
    "#print(teacher_train_preds)\n",
    "#print(teacher_train_gt)\n",
    "#Teacher_eval_table = evaluation(teacher_train_preds, teacher_train_gt)\n",
    "#Student_eval_table = evaluation(student_train_preds, student_train_gt)\n",
    "\n",
    "# show_CM_teacher(teacher_train_preds, teacher_train_gt)\n",
    "# show_CM_student(student_train_preds, student_train_gt)\n",
    "\n",
    "# print(\"Teacher(RestNet50)_Acc:\",Teacher_eval_table[0])\n",
    "# print(\"Teacher(RestNet50)_Rec:\",Teacher_eval_table[1])\n",
    "# print(\"Teacher(RestNet50)_Pre:\",Teacher_eval_table[2])\n",
    "# print(\"Teacher(RestNet50)_F1:\",Teacher_eval_table[3])\n",
    "\n",
    "# print(\"Original_Student(CNN_Small)_Acc:\",Student_eval_table[0])\n",
    "# print(\"Original_Student(CNN_Small)_Rec:\",Student_eval_table[1])\n",
    "# print(\"Original_Student(CNN_Small)_Pre:\",Student_eval_table[2])\n",
    "# print(\"Original_Student(CNN_Small)_F1:\",Student_eval_table[3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Teacher(RestNet50)_Acc:\",sum(Accuracy_teacher)/14)\n",
    "# print(\"Teacher(RestNet50)_Rec:\",sum(Recall_teacher)/11)\n",
    "# print(\"Teacher(RestNet50)_Pre:\",sum(Precision_teacher)/11)\n",
    "# print(\"Teacher(RestNet50)_F1:\",sum(F1score_teacher)/11)\n",
    "\n",
    "# print(\"Original_Student(CNN)_Acc:\",sum(Accuracy_student)/14)\n",
    "# print(\"Original_Student(CNN)_Rec:\",sum(Recall_student)/11)\n",
    "# print(\"Original_Student(CNN)_Pre:\",sum(Precision_student)/11)\n",
    "# print(\"Original_Student(CNN)_F1:\",sum(F1score_student)/11)\n",
    "\n",
    "# print(\"Distillation_Student(CNN)_Acc:\",sum(Accuracy_teacher_student)/14)\n",
    "# print(\"Distillation_Student(CNN)_Rec:\",sum(Recall_teacher_student)/11)\n",
    "# print(\"Distillation_Student(CNN)_Pre:\",sum(Precision_teacher_student)/11)\n",
    "# print(\"Distillation_Student(CNN)_F1:\",sum(F1score_teacher_student)/11)\n",
    "\n",
    "\n",
    "\n",
    "dummy_input = torch.randn(64,1,14,3,device=device)\n",
    "flops_teacher, params_teachar = profile(model_teacher,(dummy_input,))\n",
    "print('\\n\\nflops_teacher: %.3f M, params_teachar: %.3f M' % (flops_teacher / 1000000.0, params_teachar / 1000000.0))\n",
    "dummy_input = torch.randn(64,1,14,3,device=device)\n",
    "flops_student, params_student = profile(model_student,(dummy_input,))\n",
    "print('flops:',flops_student, 'params', params_student)\n",
    "print('\\nflops_student: %.3f K, params_student: %.3f K' % (flops_student / 1000.0, params_student / 1000.0))\n",
    "dummy_input = torch.randn(64,1,14,3,device=device)\n",
    "flops_teacher_student, params_teacher_student = profile(model_teacher_student,(dummy_input,))\n",
    "print('flops:',flops_teacher_student, 'params', params_teacher_student)\n",
    "print('\\nflops_teacher_student: %.3f K, params_teacher_student: %.3f K' % (flops_teacher_student / 1000.0, params_teacher_student / 1000.0))\n",
    "\n",
    "flops_Distillation_student, params_Distillation_student = profile(Distillation_teacher_model,(dummy_input,))\n",
    "print('flops:',flops_Distillation_student, 'params', params_Distillation_student)\n",
    "print('\\nflops_Distillation_student: %.3f K, params_Distillation_student: %.3f K' % (flops_Distillation_student / 1000.0, params_Distillation_student / 1000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50617f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
